{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "e8a4a759-259b-4257-8b8c-bbe7ca28a87e",
   "metadata": {
    "language": "python",
    "name": "pip_install",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "!pip install easyga",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bd112ea-7bfa-4847-b018-95bddbeba536",
   "metadata": {
    "language": "python",
    "name": "make_folder",
    "collapsed": false
   },
   "outputs": [],
   "source": "!mkdir files/",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f3a2e42d-63e1-4cfc-9366-919a6a945e1e",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "!nvidia-smi",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "117466c8-0609-4310-828a-ff425abc7d07",
   "metadata": {
    "language": "python",
    "name": "download_llm",
    "collapsed": false
   },
   "outputs": [],
   "source": "import warnings\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Suppress the FutureWarning for clean_up_tokenization_spaces\nwarnings.filterwarnings('ignore', category=FutureWarning, module='transformers.tokenization_utils_base')\n\n# Define the model name (you can change this to any other model available on Hugging Face)\nmodel_name = \"gpt2\"  # Example model, replace with your preferred model\n\n# Define the directory to save the model and tokenizer\nsave_directory = \"files/\"\n\n# Download the tokenizer and model and save them to the specified directory\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\n\n# Save the tokenizer and model locally\ntokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ab92d823-1f7a-415c-a22c-015f858617f8",
   "metadata": {
    "language": "python",
    "name": "show_files",
    "collapsed": false
   },
   "outputs": [],
   "source": "!ls files/",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e037398f-15fb-4e3f-a46d-8c68a03be1d6",
   "metadata": {
    "language": "python",
    "name": "load_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Load the tokenizer and model from the saved directory\ntokenizer = AutoTokenizer.from_pretrained(save_directory)\nmodel = AutoModelForCausalLM.from_pretrained(save_directory)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c2681fc-e0d5-4abe-a989-0602db395121",
   "metadata": {
    "language": "python",
    "name": "use_model",
    "collapsed": false
   },
   "outputs": [],
   "source": "import streamlit as st\n\n# Example text input for the model\ninput_text = \"Once upon a time in heaven \"\n\n# Tokenize the input text\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Generate text with the model\noutput = model.generate(input_ids, max_length=50, num_return_sequences=1)\n\n# Decode the output to human-readable text\noutput_text = tokenizer.decode(output[0], skip_special_tokens=True)\n\n# Print the generated text\nst.title(\"Generated text:\")\nst.text(output_text)",
   "execution_count": null
  }
 ]
}