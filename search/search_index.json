{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Daniel Wilczak","text":"<p>Welcome to My Docs.</p>"},{"location":"account/backup/replication/","title":"Replication","text":""},{"location":"account/backup/replication/#replication","title":"Replication","text":"<p>This tutorial will show how to replicate your data from one Snowflake account to anther. </p>"},{"location":"account/backup/replication/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"account/backup/replication/#requirement","title":"Requirement","text":"<ul> <li>You will need to be an account admin and orgadmin role.</li> </ul> You will need to enable orgadmin role to see the account page under the admin page. <pre><code>use role accountadmin;\n\n-- (option 1) grant the orgadmin role to a user.\ngrant role orgadmin to user &lt;username&gt;;\n\n-- (option 2) grant orgadmin to a role.\ngrant role orgadmin to role &lt;role&gt;;\n</code></pre>"},{"location":"account/backup/replication/#walk-through","title":"Walk Through","text":"<p>First we'll want to enable the other account to be able to replicate to. Please put in your orginization name and account name.</p>  Code Code <pre><code>-- If you do not have this role please read warning above.\nuse role orgadmin;\n\n/* \nView the list of the accounts in your organization. NOTE the\norganization name and account name for each account for which\nyou are enabling replication. You will use it in the next command.\n*/\n\nshow accounts;\n\n--  Enable replication for the source account.\nselect system$global_account_set_parameter(\n    '&lt;orginization&gt;.&lt;account name&gt;',\n    'enable_account_database_replication',\n    'true'\n);\n\n--  Enable replication for the target account.\nselect system$global_account_set_parameter(\n    '&lt;orginization&gt;.&lt;account name&gt;',\n    'enable_account_database_replication',\n    'true'\n);\n</code></pre> <pre><code>Result.\n</code></pre> <p>Next navigate to the account page and select replication and finally \"+ group\". </p> <p>Select the account you enabled. </p> <p>Once selected you will want to login to confirm the connection of the other account. </p> <p>Once logged in you will see it being succesful. </p> <p>Next we'll want to name the replication, select the objects to replicate and a replication frequency. </p> <p>Once you click create you will begin the replication process. Once it's replicated you will recieve metrics about how much has been replicated. </p>"},{"location":"account/configurations/add/","title":"Add account to Org","text":""},{"location":"account/configurations/add/#add-new-account-to-orginization","title":"Add new account to orginization","text":"<p>This tutorial will show how you can add a new Snowflake account under your organization.</p>"},{"location":"account/configurations/add/#video","title":"Video","text":""},{"location":"account/configurations/add/#requirement","title":"Requirement","text":"<ul> <li>Must be on a on-demand (Paying) or contracted account, you CAN NOT be on free trial. We also assume no complex security needs.</li> </ul>"},{"location":"account/configurations/add/#walk-through","title":"Walk Through","text":"<p>We first want to start by enabling the <code>orgadmin</code> role and go to the accounts page. Start by running the code below to enable <code>orgadmin</code>.</p>  Code Example Result <pre><code>-- Assume the ACCOUNTADMIN role\nuse role accountadmin;\n\n-- Grant the ORGADMIN role to a user\ngrant role orgadmin to user &lt;username&gt;;\n</code></pre> <pre><code>-- Assume the ACCOUNTADMIN role\nuse role accountadmin;\n\n-- Grant the ORGADMIN role to a user\ngrant role orgadmin to user danielwilczak;\n</code></pre> status Statement executed successfully. <p>Once you have <code>orgadmin</code> role enabled for your user, we'll switch to the role. </p> <p>Now that we switch to the <code>orgadmin</code> role we will see a new tab under the admin section called <code>accounts</code>. We'll click on that and then click <code>+ account</code> in the top right hand corner. </p> <p>From here you can select your preferred preferences for the account and click next, followed by create. </p>"},{"location":"account/configurations/name/","title":"Org / Account name","text":""},{"location":"account/configurations/name/#change-name-account-and-organization","title":"Change Name - Account and Organization","text":"<p>In this tutorial we will show how you can update your orginization and account name.</p>"},{"location":"account/configurations/name/#video","title":"Video","text":""},{"location":"account/configurations/name/#requirement","title":"Requirement","text":"<ul> <li>Must be on a on-demand (Paying) or contracted account, you CAN NOT be on free trial. We also assume no complex security needs.</li> </ul>"},{"location":"account/configurations/name/#walk-through","title":"Walk Through","text":"<p>When it comes to updating organization names there is only one way and that is via a support ticket where as account name updates can happen one of two way. One being via a support ticket where as the other is via the UI. We will show both.</p>"},{"location":"account/configurations/name/#organization-account-name","title":"Organization / account name","text":"<p>Lets start a support ticket to update our organization name. To start we will click on our profile name in the bottom left corner and click on support. </p> If you have not submitted a ticket in the past or added your email to their user profile <p>You will have to setup an email in your profile before you are allowed to submit a support ticket. To add a email we will navigate to the your profile. </p> <p>Update the email and click save. </p> <p>Click on \"+ support ticket\" to add a new ticket </p> <p>We will fill in the support ticket with the settings showen and click \"create case\". </p> <p>We are done! Typically your account will be updated the same day but could take 24-48 hours.</p>"},{"location":"account/configurations/name/#account-name-via-ui-option-2","title":"Account name via UI (Option 2)","text":"<p>Warning</p> <p>You can not edit your account name if it's the only account via this option. You will not be given the option and will have to submit a support ticket.</p> <p>The section option is via the UI where we enable the <code>orgadmin</code> role and go to the accounts page to edit our accounts name. Start by running the code below to enable <code>orgadmin</code>.</p>  Code Example Result <pre><code>-- Assume the ACCOUNTADMIN role\nUSE ROLE accountadmin;\n\n-- Grant the ORGADMIN role to a user\nGRANT ROLE orgadmin TO USER &lt;username&gt;;\n</code></pre> <pre><code>-- Assume the ACCOUNTADMIN role\nUSE ROLE accountadmin;\n\n-- Grant the ORGADMIN role to a user\nGRANT ROLE orgadmin TO USER danielwilczak;\n</code></pre> status Statement executed successfully. <p>Once you have <code>orgadmin</code> role enable for your user, we'll switch to the role. </p> <p>Now that we switch to the <code>orgadmin</code> role we will see a new tab under the admin section called <code>accounts</code>. We'll click on that and then click the three dots next to the account we want to change the name of. </p> <p>Now we can change the name of the account and click save. </p>"},{"location":"account/configurations/service_level/","title":"Change Edition Level","text":""},{"location":"account/configurations/service_level/#change-service-level","title":"Change Service Level","text":"<p>To change your service level in snowflake to Standard, Enterprise, or Busniess Critical we will need to submit a support ticket and this tutorial will show how.</p>"},{"location":"account/configurations/service_level/#video","title":"Video","text":""},{"location":"account/configurations/service_level/#requirement","title":"Requirement","text":"<p>You will need to be an account admin to submit the support ticket request.</p>"},{"location":"account/configurations/service_level/#walk-through","title":"Walk Through","text":"<p>To start we will click on our profile name in the bottom left corner and click on support. </p> If you have not submitted a ticket in the past or added your email to their user profile <p>You will have to setup an email in your profile before you are allowed to submit a support ticket. To add a email we will navigate to the your profile. </p> <p>Update the email and click save. </p> <p>Click on \"+ support ticket\" to add a new ticket </p> <p>We will fill in the support ticket with the settings showen and click \"create case\". </p> <p>We are done! Typically your account will be updated the same day but could take 24-48 hours. If this is not the case (Longer then 48 hours), please reachout to your account team.</p>"},{"location":"account/support/","title":"Ticket","text":""},{"location":"account/support/#support-ticket","title":"Support Ticket","text":"<p>In this tutorial we will show how to submit a support ticket in Snowflake. Support tickets are submitted for a varity of reasons. For example - configuration settings, query error or optimization etc. Support tickets are typically answer withing 12 hours based on the ticket complexity.</p>"},{"location":"account/support/#video","title":"Video","text":"<p>Video in development.</p>"},{"location":"account/support/#requirement","title":"Requirement","text":"<p>You will need to be an account admin to submit the support ticket request.</p>"},{"location":"account/support/#walk-through","title":"Walk Through","text":"<p>To start we will click on our profile name in the bottom left corner and click on support. </p> If you have not submitted a ticket in the past or added your email to their user profile <p>You will have to setup an email in your profile before you are allowed to submit a support ticket. To add a email we will navigate to the your profile. </p> <p>Update the email and click save. </p> <p>Click on \"+ support ticket\" to add a new ticket </p> <p>We will fill in the support ticket with the settings showen and click \"create case\". </p> <p>We are done! Now all communication will happen in the ticket messaging system. </p>"},{"location":"apps/container/flask/","title":"Flask","text":""},{"location":"apps/container/flask/#container-services-introduction","title":"Container Services - Introduction","text":"<p>Goal of this tutorial is to get a introduction to Snowflake Container services by creating a flask website and then uploading / running it in Snowflake.</p>"},{"location":"apps/container/flask/#video","title":"Video","text":""},{"location":"apps/container/flask/#requirements","title":"Requirements","text":"<ul> <li>Be in a container services enabled region.(Link)</li> <li>You can NOT be on a trial account. (Link)</li> </ul>"},{"location":"apps/container/flask/#download","title":"Download","text":"<ul> <li>Files (Link)</li> </ul>"},{"location":"apps/container/flask/#setup","title":"Setup","text":"<p>Lets go through some setup before to go into our application.</p>"},{"location":"apps/container/flask/#snowflake","title":"Snowflake","text":"<p>Let's start by setting up Snowflake before we jump to docker. Create a worksheet in snowflake and add / run the code below.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema raw.website;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema website;\nuse warehouse developer;\n</code></pre>  Setup Result <pre><code>-- Use account admin for the integration.\nuse role accountadmin;\n\n-- Allow sysadmin to create our website url.\ngrant bind service endpoint on account to role sysadmin;\n\n-- Use sysadmin for everything else.\nuse role sysadmin;\n\n-- Compute pool to run containers on.\ncreate compute pool cpu_x64_xs\n    min_nodes = 1\n    max_nodes = 1\n    instance_family = cpu_x64_xs;\n\n-- Image registry to upload our docker image to.\ncreate or replace image repository images;\n\n-- Give us the url to upload our docker container to.\nshow image repositories;\nselect \"repository_url\" from table(result_scan(last_query_id()));\n</code></pre> repository_url sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images"},{"location":"apps/container/flask/#docker","title":"Docker","text":"<p>Our goal is to run the application locally and check if it works and then upload the built docker image to our snowflake image repository so it can be hosted on Snowflake container services.</p> <p>Note</p> <p>Please install docker desktop - https://www.docker.com/products/docker-desktop/</p> <p>Using terminal, navigate to the folder that has the docker file you downloaded.  </p>  Build and Run Result <pre><code>docker build --rm -t flask:website .\ndocker run --rm -p 8080:8080 flask:website\n</code></pre> <pre><code>WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n* Serving Flask app 'app'\n* Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n* Running on all addresses (0.0.0.0)\n* Running on http://127.0.0.1:8080\n* Running on http://172.17.0.2:8080\n</code></pre> <p>Now you can go to Localhost or the direct local url http://127.0.0.1:8080/. To see what the website will look like before we upload it.</p>"},{"location":"apps/container/flask/#upload","title":"Upload","text":"<p>Now that we have our image created. Lets upload it to Snowflake. We will need our Snowflake image url (1) that we got from our Snowflake setup.</p> <ol> <li> repository_url sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images </li> </ol>  Code Example <pre><code>docker tag flask:website &lt;URL GOES HERE&gt;/flask:website\n</code></pre> <pre><code>docker tag flask:website \\\nsfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images/flask:website\n</code></pre> <p>Next docker login to our snowflake image repo and upload the image. We will use the login name that has access to sysadmin role.</p>  Code Example <pre><code>docker login &lt;FIRST PART OF THE URL&gt; -u &lt;LOGIN USERNAME&gt;\n</code></pre> <pre><code>docker login sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/ -u danielwilczak\n</code></pre> If your user is using MFA - Please enable token caching before uploading <p>Docker will upload the image in a multi threaded way, if you have MFA it will ask you a hundred times. To prevent this we suggest to enable token caching.</p> <pre><code>alter account set allow_client_mfa_caching = TRUE;\n</code></pre> <p>Finally push the image to your image repository living on Snowflake.</p>  Code Example Result <pre><code>docker push &lt;URL GOES HERE&gt;/flask:website\n</code></pre> <pre><code>docker push sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images/flask:website\n</code></pre> <pre><code>The push refers to repository [sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images/flask]\n5c496785191b: Pushed \naf426fba40ce: Pushed \n89732b928ca7: Pushed \nc3fcfdebbdeb: Pushed \n5930c727f5d3: Pushed \nfe4e91e03123: Pushed \n1ee7a83db2c7: Pushed \n57bbd9d1460f: Pushed \n2920aec4783e: Pushed \n61255a2b6a94: Pushed \n4c519b360763: Pushed \nabf3d7b50651: Pushed \na0b7220ad76b: Pushed \nwebsite: digest: sha256:f0ef0b8e2a6e9fa218fd7cd771bce6befeecd1d62a26c740df8fb8b45ed6831c size: 3045\n</code></pre>"},{"location":"apps/container/flask/#run","title":"Run","text":"<p>Lets switch back to snowflake to start our container. </p>"},{"location":"apps/container/flask/#start-service","title":"Start Service","text":"<p>Create the service to host the container with our inline service specification..</p>  SQL Result <pre><code>use role sysadmin;\n\n-- Create the service that will host our containerized application.\ncreate service website\n    in compute pool cpu_x64_xs\n    from specification $$\n    spec:\n        container:  \n        - name: website\n          image: /raw/website/images/flask:website\n\n        endpoint:\n        - name: app\n          port: 8080\n          public: true\n    $$;\n\n-- Give us a URL to see our application.\nshow endpoints in service website;\nselect \"ingress_url\" from table(result_scan(last_query_id()));\n</code></pre> ingress_url br2sbye-sfsenorthamerica-wilczak-videos2.snowflakecomputing.app"},{"location":"apps/container/flask/#set-default-role","title":"Set default role","text":"<p>Before we go to our URL. Please make sure your default role is set to Sysadmin </p> <p>Now go to the webiste url, login and see your amazing website! </p>"},{"location":"apps/container/flask/#clean-up-script","title":"Clean up script","text":"<p>If you don't plan to keep this running. Which I don't reccomend considering it's using .11 credits per hour. Here is a clean up script.</p>  SQL Result <pre><code>use role sysadmin;\n\ndrop service website;\ndrop compute pool cpu_x64_xs;\ndrop warehouse developer;\ndrop database raw;\n</code></pre> status RAW successfully dropped."},{"location":"apps/container/streamlit/","title":"Streamlit","text":""},{"location":"apps/container/streamlit/#container-services-introduction","title":"Container Services - Introduction","text":"<p>Goal of this tutorial is to move our Streamlit in Snowflake \"SiS\" application into Snowflake container services. This is a typical workflow to remove the Snowflake headers around the application.</p>"},{"location":"apps/container/streamlit/#video","title":"Video","text":""},{"location":"apps/container/streamlit/#requirements","title":"Requirements","text":"<ul> <li>Be in a container services enabled region.(Link)</li> <li>You can NOT be on a trial account. (Link)</li> </ul>"},{"location":"apps/container/streamlit/#download","title":"Download","text":"<ul> <li>Files (Link)</li> </ul>"},{"location":"apps/container/streamlit/#setup","title":"Setup","text":"<p>Lets go through some setup before to go into our application.</p>"},{"location":"apps/container/streamlit/#snowflake","title":"Snowflake","text":"<p>Let's start by setting up Snowflake before we jump to docker. Create a worksheet in snowflake and add / run the code below.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema raw.website;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema website;\nuse warehouse developer;\n</code></pre>  Setup Result <pre><code>use role sysadmin;\n\n-- Compute pool to run containers on.\ncreate compute pool cpu_x64_xs\n    min_nodes = 1\n    max_nodes = 1\n    instance_family = cpu_x64_xs;\n\n-- Image registry to upload our docker image to.\ncreate or replace image repository raw.website.docker;\n\n-- Give us the url to upload our docker container to.\nshow image repositories;\nselect \"repository_url\" from table(result_scan(last_query_id()));\n</code></pre> repository_url sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images"},{"location":"apps/container/streamlit/#docker","title":"Docker","text":"<p>Our goal is to run the application locally and check if it works and then upload the built docker image to our snowflake image repository so it can be hosted on Snowflake container services.</p> <p>Note</p> <p>Please install docker desktop - https://www.docker.com/products/docker-desktop/</p> <p>Using terminal, navigate to the folder that has the docker file you downloaded.  </p>  Build and Run Result <pre><code>docker build --platform linux/amd64 -t website .\ndocker run --rm -p 8501:8501 website\n</code></pre> <pre><code>You can now view your Streamlit app in your browser.\n\nLocal URL: http://localhost:8501\nNetwork URL: http://172.17.0.2:8501\n</code></pre> <p>Now you can go to Localhost or the direct local url http://127.0.0.1:8501/. To see what the website will look like before we upload it.</p>"},{"location":"apps/container/streamlit/#upload","title":"Upload","text":"<p>Now that we have our image created. Lets upload it to Snowflake. We will need our Snowflake image url (1) that we got from our Snowflake setup.</p> <ol> <li> repository_url sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/docker/images </li> </ol>  Code Example <pre><code>docker tag website &lt;URL GOES HERE&gt;/flask:website\n</code></pre> <pre><code>docker tag website \\\nsfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/docker/website\n</code></pre> <p>Next docker login to our snowflake image repo and upload the image. We will use the login name that has access to sysadmin role.</p>  Code Example <pre><code>docker login &lt;FIRST PART OF THE URL&gt; -u &lt;LOGIN USERNAME&gt;\n</code></pre> <pre><code>docker login sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/ -u danielwilczak\n</code></pre> If your user is using MFA - Please enable token caching before uploading <p>Docker will upload the image in a multi threaded way, if you have MFA it will ask you a hundred times. To prevent this we suggest to enable token caching.</p> <pre><code>alter account set allow_client_mfa_caching = TRUE;\n</code></pre> <p>Finally push the image to your image repository living on Snowflake.</p>  Code Example Result <pre><code>docker push &lt;URL GOES HERE&gt;/website\n</code></pre> <pre><code>docker push sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/docker/website\n</code></pre> <pre><code>The push refers to repository [sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/docker/website]\n5c496785191b: Pushed \naf426fba40ce: Pushed \n89732b928ca7: Pushed \nc3fcfdebbdeb: Pushed \n5930c727f5d3: Pushed \nfe4e91e03123: Pushed \n1ee7a83db2c7: Pushed \n57bbd9d1460f: Pushed \n2920aec4783e: Pushed \n61255a2b6a94: Pushed \n4c519b360763: Pushed \nabf3d7b50651: Pushed \na0b7220ad76b: Pushed \nwebsite: digest: sha256:f0ef0b8e2a6e9fa218fd7cd771bce6befeecd1d62a26c740df8fb8b45ed6831c size: 3045\n</code></pre>"},{"location":"apps/container/streamlit/#run","title":"Run","text":"<p>Lets switch back to snowflake to start our container. </p>"},{"location":"apps/container/streamlit/#start-service","title":"Start Service","text":"<p>Create the service to host the container with our inline service specification..</p>  SQL Result <pre><code>use role sysadmin;\nuse database raw;\nuse schema website;\n\ncreate service streamlit_application\n    in compute pool cpu_x64_xs\n    from specification $$\n    spec:\n    containers:\n        - name: website\n        image: /raw/website/docker/website\n\n    endpoints:\n        - name: main\n        port: 8501\n        public: true\n    $$;\n\n-- Give us a URL to see our application.\nshow endpoints in service streamlit_application;\nselect \"ingress_url\" from table(result_scan(last_query_id()));\n</code></pre> ingress_url br2sbye-sfsenorthamerica-wilczak-videos2.snowflakecomputing.app"},{"location":"apps/container/streamlit/#set-default-role","title":"Set default role","text":"<p>Before we go to our URL. Please make sure your default role is set to Sysadmin </p> <p>Now go to the webiste url, login and see your amazing website! </p>"},{"location":"apps/container/streamlit/#clean-up-script","title":"Clean up script","text":"<p>If you don't plan to keep this running. Which I don't reccomend considering it's using .11 credits per hour. Here is a clean up script.</p>  SQL Result <pre><code>use role sysadmin;\n\ndrop service streamlit_application;\ndrop compute pool cpu_x64_xs;\ndrop warehouse developer;\ndrop database raw;\n</code></pre> status RAW successfully dropped."},{"location":"apps/native/containers/","title":"Container Services","text":""},{"location":"apps/native/containers/#native-app-container-services-intro","title":"Native App + Container Services (Intro)","text":"<p>Goal of this tutorial is to get a introduction to Native App + Container Services. In this tutorial you will build a containerized website and then deploy it via a Native app + Container Services. This is the same container we used in our container services introduction tutorial.</p>"},{"location":"apps/native/containers/#video","title":"Video","text":""},{"location":"apps/native/containers/#requirements","title":"Requirements","text":"<ul> <li>Be in a container services enabled region.(Link)</li> <li>You can NOT be on a trial account. (Link)</li> <li>Please install docker desktop - https://www.docker.com/products/docker-desktop/</li> </ul>"},{"location":"apps/native/containers/#download-choose-1","title":"Download (Choose 1)","text":"<ul> <li>Files for simple website - (Image) (Link)   </li> <li>Files for fancy website - (Image) (Link)   </li> </ul>"},{"location":"apps/native/containers/#setup","title":"Setup","text":"<p>Lets go through some setup before going to docker / building our native application.</p>"},{"location":"apps/native/containers/#snowflake","title":"Snowflake","text":"<p>Let's start by setting up Snowflake before we jump to docker. Create a worksheet in Snowflake and add / run the code below.</p> <p>Note</p> <p>Copy the resulting URL, we will use it in Docker later.</p>  Setup Result <pre><code>use role accountadmin;\n\n-- Database\ncreate database provider_db;\n\n-- Schema\ncreate schema provider_db.provider_schema;\n\n-- Stage\ncreate stage provider_db.provider_schema.code\n    directory = (enable = true)\n    encryption = (type='snowflake_sse')\n    file_format = (type = 'csv' field_delimiter = '|' skip_header = 1);\n\n-- Docker image repo.\ncreate image repository provider_db.provider_schema.docker;\n\n-- Tell us where to upload our docker image to. This will be used later.\nshow image repositories;\nselect \"repository_url\" from table(result_scan(last_query_id()));\n</code></pre> repository_url sfsenorthamerica-wilczak-videos2.registry.snowflakecomputing.com/raw/website/images"},{"location":"apps/native/containers/#upload-native-app-files","title":"Upload Native App Files","text":"<p>In this section we will go back to our \"provider_db/provider_schema\" database/schema and upload our native app code we got earlier into the \"code\" stage.</p> <p></p> <p>Next we'll take the files from the \"stage\" folder and upload them.  </p>"},{"location":"apps/native/containers/#docker","title":"Docker","text":"<p>Our goal is to run the application locally and check if it works and then upload the built docker image to our snowflake image repository so it can be hosted on Snowflake container services followed by a Native App.</p> <p>Note</p> <p>Please install docker desktop and have it running - https://www.docker.com/products/docker-desktop/</p> <p>Using terminal, navigate into the \"website\" folder. And run the code below to build the application locally. </p>  Build and Run Result <pre><code>docker build --platform linux/amd64 -t website .\ndocker run --rm -p 8080:8080 website\n</code></pre> <pre><code>WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n* Serving Flask app 'app'\n* Debug mode: off\nWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n* Running on all addresses (0.0.0.0)\n* Running on http://127.0.0.1:8080\n* Running on http://172.17.0.2:8080\n</code></pre> <p>Now you can go to Localhost or the direct local url http://127.0.0.1:8080/. To see what the website will look like before we upload it. Press Ctrl+C to stop the application.</p>"},{"location":"apps/native/containers/#upload","title":"Upload","text":"<p>Now that we have our image created. Lets upload it to Snowflake. We will need our Snowflake image url (1) that we got from our Snowflake setup.</p> <ol> <li> repository_url sfsenorthamerica-demo-dwilczak.registry.snowflakecomputing.com/provider_db/provider_schema/docker </li> </ol>  Code Example <pre><code>docker tag website &lt;URL GOES HERE&gt;/website\n</code></pre> <pre><code>docker tag website sfsenorthamerica-demo-dwilczak.registry.snowflakecomputing.com/provider_db/provider_schema/docker/website\n</code></pre> <p>Next docker login to our snowflake image repo using only the part of the url prior to the first forward slash. We will use the user with accountadmin access.</p>  Code Example <pre><code>docker login &lt;FIRST PART OF THE URL&gt; -u &lt;USERNAME&gt;\n</code></pre> <pre><code>docker login sfsenorthamerica-demo-dwilczak.registry.snowflakecomputing.com/ -u danielwilczak\n</code></pre> <p>Finally push the image to your image repository living on Snowflake.</p>  Code Example Result <pre><code>docker push &lt;URL GOES HERE&gt;/website\n</code></pre> <pre><code>docker push sfsenorthamerica-demo-dwilczak.registry.snowflakecomputing.com/provider_db/provider_schema/docker/website\n</code></pre> <pre><code>Using default tag: latest\nThe push refers to repository [sfsenorthamerica-demo-dwilczak.registry.snowflakecomputing.com/provider_db/provider_schema/docker/website]\ndf7fb4f01ad1: Pushed \n620025ace117: Pushed \n3e2bded7d904: Pushed \n4ad6620b3816: Pushed \n6828fc9c1ffb: Pushed \n4a40ad7ca2cf: Pushed \n064ffeaf7830: Pushed \n28f8d0721800: Pushed \ncbe4fb5e267b: Pushed \n734c0f0b65c2: Pushed \n8845ab872c1c: Pushed \nd7d4c2f9d26b: Pushed \nbbe1a212f7e9: Pushed \nlatest: digest: sha256:1d6431ee1c749261d35129ee07ebc34ea2868fe2473e0b226078cf774951dc99 size: 3045\n</code></pre>"},{"location":"apps/native/containers/#build-application","title":"Build Application","text":"<p>Now were ready to build our application package and test it by building/testing it in our Snowflake account. Lets go back to our worksheet and copy the code below.</p>  Application Result <pre><code>use role accountadmin;\n\n-- Create the package.\ncreate application package provider_package;\n\n-- Copy our native app code and docker image over to the package.\nalter application package provider_package add version v1 using @provider_db.provider_schema.code;\n\n-- Create the application from the package.\ncreate application consumer_app from application package provider_package using version v1;\n</code></pre> status Application 'CONSUMER_APP' created successfully."},{"location":"apps/native/containers/#using-the-app","title":"Using the App","text":"<p>Lets walk through the prcoess of installing the application. Lets click on our new application called \"consumer_app\". </p> <p>Grant the application the privligies to create a warehouse, compute pool, and endpoint to acccess the website. </p> <p>Click activate, this will start the build process. </p> <p>Warning</p> <p>I recommend to refresh the screen once in a while, the UI might just hung up.</p> <p>The build will go into a loading screen. This can load for upward of a few minutes for a simple application and longer for more complex apps. </p> <p>Once the application is installed we can launch the app. </p> <p>We'll login with a user who has the accountadmin role. </p> <p>Now we can see the amazing website we have made in Native apps and Container Services on Snowflake. </p>"},{"location":"apps/native/containers/#extra-sharing","title":"(Extra) Sharing","text":"<p>Warning</p> <p>Your second account will also have to be enabled.</p> <p>If you want to share your application, you will have to set a default version.</p>  Application Result <pre><code>-- if you want to publish it.\nalter application package provider_package set default release directive version=v1 patch=0;\n</code></pre> status Default release directive set to version 'V1', patch 0."},{"location":"apps/native/traditional/","title":"Traditional","text":""},{"location":"apps/native/traditional/#native-application-traditional-streamlit","title":"Native Application - Traditional Streamlit","text":"<p>Goal of this tutorial is to build a traditional native application that has sql function, data that is shared, and a streamlit application that uses that shared data.</p>"},{"location":"apps/native/traditional/#video","title":"Video","text":""},{"location":"apps/native/traditional/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"apps/native/traditional/#download","title":"Download","text":"<ul> <li>Files (Link)</li> </ul>"},{"location":"apps/native/traditional/#setup","title":"Setup","text":"<p>This tutorial is very UI focused so we suggest to follow along with the video and use the files / code provided. </p>"},{"location":"apps/native/traditional/#example-v1-basic-funtions","title":"Example V1 - Basic funtions","text":"<p>The goals in this first step is to introduce you to a native application framework and how to upload files to the stage. In this example we'll run the first block of code and then upload V1 files to the stage. Then run the second to create the application from the package.</p>  Code Result <pre><code>use role accountadmin;\ncreate application package snowflake_application_package;\nuse application package snowflake_application_package;\ncreate schema core;\ncreate or replace stage files;\n\n-- Now go upload v1 zip contents to the package's files stage.\n\ncreate application snowflake_application\nfrom application package snowflake_application_package\nusing '@snowflake_application_package.core.files';\n\ncall snowflake_application.resources.hello();\n</code></pre> <pre><code>| HELLO            |\n|------------------|\n| hello snowflake! |\n</code></pre>"},{"location":"apps/native/traditional/#example-v2-share-data","title":"Example V2 - Share data","text":"<p>The of this exmaple is to show how you can add data that will be shared with the application to the end consumer. In this example we'll run the first block of code and then upload V2 files to the stage. Then run the second to create the application from the package.</p>  Code Result <pre><code>use application package snowflake_application_package;\n\n-- Create and add data to package.\ncreate schema if not exists shared_data;\n\ncreate table if not exists accounts (id int, name varchar, value varchar);\n\ninsert into accounts values\n(1, 'Nihar', 'Snowflake'),\n(2, 'Frank', 'Snowflake'),\n(3, 'Benoit', 'Snowflake'),\n(4, 'Steven', 'Acme');\n\nselect * from accounts;\n\ngrant usage on schema shared_data to share in application package snowflake_application_package;\ngrant select on table accounts to share in application package snowflake_application_package;\n\ndrop application snowflake_application;\n\n-- Upload V2 files to the stage before running.\n\ncreate application snowflake_application\nfrom application package snowflake_application_package\nusing '@snowflake_application_package.core.files';\n\nuse application snowflake_application;\nselect * from data.accounts_view;\n</code></pre> <pre><code>| ID | NAME   | VALUE     |\n|----|--------|-----------|\n| 1  | Nihar  | Snowflake |\n| 2  | Frank  | Snowflake |\n| 3  | Benoit | Snowflake |\n| 4  | Steven | Acme      |\n</code></pre>"},{"location":"apps/native/traditional/#example-v3-streamlit","title":"Example V3 - Streamlit","text":"<p>For this example we'll add a streamlit application to our application so users can visulize the data we have shared with them. In this we'll upload the V3 files and then run the code.</p>  Code Result <pre><code>-- Upload V3 files to the stage.\ndrop application snowflake_application;\n\nuse application package snowflake_application_package;\n\ncreate application snowflake_application\nfrom application package snowflake_application_package\nusing '@snowflake_application_package.core.files';\n</code></pre> <pre><code>| status                                                    |\n|-----------------------------------------------------------|\n| Application 'SNOWFLAKE_APPLICATION' created successfully. |\n</code></pre> <p>Now we can click on the application and go to the app. </p> <p>Now we can see our streamlit application that is referencing our shared data. </p>"},{"location":"apps/native/traditional/#versioning-and-deploying-the-application","title":"Versioning and deploying the application","text":"<p>To version your application and deploy the application so it can be share to other consumers please watch the video, it is all done in the UI.</p>"},{"location":"clouds/aws/S3_key_and_secret/","title":"S3 with Key/Secret","text":""},{"location":"clouds/aws/S3_key_and_secret/#connect-snowflake-to-s3-storage-via-keysecret","title":"Connect Snowflake to S3 Storage via Key/Secret","text":"<p>In this tutorial we will show how to generate a key and secret for an S3 bucket and then use that in Snowflake to create a stage.</p>"},{"location":"clouds/aws/S3_key_and_secret/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>AWS account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/aws/S3_key_and_secret/#video","title":"Video","text":"<p>Video in development.</p>"},{"location":"clouds/aws/S3_key_and_secret/#download","title":"Download","text":"<ul> <li>Sample data (Link)</li> </ul>"},{"location":"clouds/aws/S3_key_and_secret/#aws","title":"AWS","text":"<p>Sign into your aws account.</p> If you don't have a bucket yet follow here <p>Search S3 in the navigation bar. </p> <p>Click Create bucket. </p> <p>Select general purpose and give your bucket a name. </p> <p>Keep all the default settings and click \"Create bucket\". \"\"</p> <p>Lets start by selecting the bucket we want Snowflake to access. </p> <p>We'll first copy our ARN by going to properties. </p> <p>If we have a new bucket, we can take the time here to upload the sample data. </p>"},{"location":"clouds/aws/S3_key_and_secret/#access-policy","title":"Access Policy","text":"<p>Lets setup a read policy that we later apply to a user that will then be used by Snowflake to access the bucket. Search and click on \"IAM\". </p> <p>Click Policies. </p> <p>Click create policy. </p> <p>Next we'll want to click json and add the JSON below and updating our ARN.</p>  Template Example <p></p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n            \"s3:GetObject\",\n            \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"&lt;COPY ARN HERE&gt;/*\" /* (1)! */\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"&lt;COPY ARN HERE&gt;\", /* (2)! */\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n            \"s3:GetObject\",\n            \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::danielwilczak/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::danielwilczak\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>This is what it will look like. </p> <p>Give the policy a name, make sure to remember this name we will need it later. Click \"Create Policy\". </p>"},{"location":"clouds/aws/S3_key_and_secret/#create-user","title":"Create User","text":"<p>Lets create the user and apply the policy to it. Start by going to users. </p> <p>Click create user. </p> <p>Give the user a name. </p> <p>Now will select \"Attach policies directly\", search and select our policy, and click next. </p> <p>Click \"Create user\". </p>"},{"location":"clouds/aws/S3_key_and_secret/#keysecret-generation","title":"Key/Secret Generation","text":"<p>Now that we have our user lets generate our credientals. Select the user. </p> <p>Go to \"Security credentials\" and then click \"create access key\". </p> <p>Select other and then click next. </p> <p>Click \"Create access key\". </p> <p>Copy your access key and secret we will use this in Snowflake in the next step. </p>"},{"location":"clouds/aws/S3_key_and_secret/#snowflake","title":"Snowflake","text":"<p>Lets now head into Snowflake and create a sql sheet in workspaces.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>```sql linenums=\"1\"\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.aws;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema aws;\nuse warehouse development;\n```\n</code></pre> <p></p> <p>Create a new sheet. </p> <p>From here we'll add our stage code and paste in our bucket name, key and secret. Click run.</p>  Code Example Result <pre><code>create or alter stage s3\n    url='s3://&lt;BUCKET NAME&gt;/'\n    credentials=(\n        aws_key_id='&lt;KEY&gt;'\n        aws_secret_key='&lt;Secret&gt;'\n    )\n    directory=(enable=true);\n</code></pre> <pre><code>create or alter stage s3\n    url='s3://danielwilczak/'\n    credentials=(\n        aws_key_id='AKIARH....BWUZ7Q'\n        aws_secret_key='Vh597QKZqMX....sdoAAK4GD90M'\n    )\n    directory=(enable=true);\n</code></pre> <pre><code>Stage area S3 successfully created.\n</code></pre> <p>This is what it will look like once ran. </p> <p>Now that we have created our stage we can go view the files in it. If you don't see the file right away, hit refresh. From here we can start loading data. But the process is the same as our S3 tutorial and not need to repeat here. </p>"},{"location":"clouds/aws/kinesis/","title":"Kinesis","text":""},{"location":"clouds/aws/kinesis/#aws-kinesis-to-snowflake","title":"AWS Kinesis to Snowflake","text":"<p>In this tutorial we will show how you can setup Kinesis stream and firehose to load data into Snowflake. We will use a local python script of taxi data to act as our source.</p> <p>Credit for the orginial tutorial goes to Umesh Patel!</p>"},{"location":"clouds/aws/kinesis/#video","title":"Video","text":""},{"location":"clouds/aws/kinesis/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p>"},{"location":"clouds/aws/kinesis/#download-needed-files","title":"Download needed files:","text":"<ul> <li>Data generator code (Link)</li> </ul>"},{"location":"clouds/aws/kinesis/#snowflake-setup","title":"Snowflake Setup","text":"<p>For the Snowflake setup we'll want to create the service user with an ssh key and the table to load data into.</p>"},{"location":"clouds/aws/kinesis/#generate-an-ssh-key","title":"Generate an ssh key","text":"<p>Lets create the private and public key so that we can apply the public key to our user.</p>  Setup Result <pre><code>openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out private_key.p8 -nocrypt\nsed '/-----/d' private_key.p8 | tr -d '\\n' &gt; private_key_kinesis.txt\nopenssl rsa -in private_key.p8 -pubout -out public_key.pub\n</code></pre> <pre><code>Writing RSA key.\n</code></pre> <p>This will create three files in the folder we are currently located. </p>"},{"location":"clouds/aws/kinesis/#user","title":"User","text":"<p>Lets create the user and apply the public key to our user in Snowflake a Worksheet (1). The public key file will end with <code>.pub</code>.</p>  Setup Example Result <p></p><pre><code>use role accountadmin;\n\ncreate user danielwilczak type = 'service';\n\nalter user danielwilczak set \n    rsa_public_key='&lt;Public Key&gt;';  /* (1)! */\n\ngrant role sysadmin to user danielwilczak;\n</code></pre><p></p> <ol> <li>The public key file will end with <code>.pub</code>. We got this from our \"Generate an ssh key step\".</li> </ol> <pre><code>use role accountadmin;\n\ncreate user danielwilczak type = 'service';\n\nalter user danielwilczak set \n    rsa_public_key='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsLiIQpJ0SkB0KgyN/Cj5\n        O+3W3zIN5HvjBwsQnVbXAGpu920fohXRQAFc5hZpMNZOGNsLvl1YY1HtQ15j4K7o\n        Ip3Eo2.............................................EUnH8sGWDvH+U\n        g5ha+Sa6KD5864ajlkylKFiu9T++GQaItyLNsOVx8AGi8J4oDtv02a6MlG7oDyOo\n        ArBubofdmM+8exWL7NfYNfV04Wjnpz5itGNq9CM718Fx910mom4sIUPBGQC0Dnio\n        Wr9cvDxXmfWdRUjgeKDGAwrvXP9+PtCMoLlo+eYjWhz9Gii2lxdHqfLgY67ZCa1t\n        ZQIDAQAB';\n\ngrant role sysadmin to user danielwilczak;\n</code></pre> <pre><code>Statement executed successfully.\n</code></pre>"},{"location":"clouds/aws/kinesis/#table","title":"Table","text":"<p>Lets create the table in Snowflake to load data into.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.kinesis;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema kinesis;\nuse warehouse development;\n</code></pre>  Table Result <pre><code>use role sysadmin; \n\ncreate or replace table taxi_data (\n    vendor_id NUMBER, \n    tpep_pickup_datetime VARCHAR, \n    tpep_dropoff_datetime VARCHAR, \n    passenger_count NUMBER, \n    trip_distance FLOAT, \n    ratecode_id NUMBER, \n    store_and_fwd_flag TEXT, \n    pu_location_id NUMBER, \n    do_location_id NUMBER, \n    payment_type NUMBER, \n    fare_amount FLOAT, \n    extra FLOAT, \n    mta_tax FLOAT, \n    tip_amount FLOAT, \n    tolls_amount FLOAT, \n    improvement_surcharge FLOAT, \n    total_amount FLOAT,\n    congestion_surcharge FLOAT\n);\n</code></pre> <pre><code>Table TAXI_DATA successfully created.\n</code></pre>"},{"location":"clouds/aws/kinesis/#aws-setup","title":"AWS Setup","text":"<p>Lets setup AWS to recieve data via a stream and the move it to Snowflake via firehose. In the process we will create a user and credentials.</p>"},{"location":"clouds/aws/kinesis/#stream","title":"Stream","text":"<p>Lets start by heading into aws and going to Kinesis. </p> <p>Create a data stream. </p> <p>Lets give it a name and select on-demand as we only want to be changed as we use it. </p> <p>Click create stream. </p> <p>Once it's created we'll want to copy the region. In this case us-east-1. We will use this later in our data generator. </p>"},{"location":"clouds/aws/kinesis/#firehouse","title":"Firehouse","text":"<p>Next we'll want to search for kinesis firehose. Firehose will be the tool that moves our stream to Snowflake. </p> <p>Click create firehouse stream.  </p> <p>We'll wanted to select kinesis data stream as a source and destination being Snowflake. Next give it a name and click \"browse\" to find our stream we setup earlier. </p> <p>Select our stream. </p> <p>Next we'll want to copy our Account url from Snowflake. Lets start by going to account details. </p> <p>Click the copy button with the URL. </p> <p>First, paste your Snowflake URL. Next, select \"Private Key\" and enter your Snowflake username along with the kinesis private key we generated earlier. For the role, choose \"Customer Snowflake Role\" and specify <code>SYSADMIN</code>. </p> <p>For the database configuration we'll enter <code>RAW</code> for the database, <code>KINESIS</code> for the schema name and <code>TAXI_DATA</code> for the table. We'll also want it to use the keys as column names when it's dropped into Snowflake. </p> <p>For the backup settings we'll create a new bucket. </p> <p>We'll give the bucket a name and then scroll down and click create. </p> <p>We'll go back and select our bucket we just created. If it does not show up right away, try clicking the refresh button in the top right. </p> <p>Finish by clicking create firehose. </p> <p>Finished our firehouse setup and ready to have data moved to Snowflake. </p>"},{"location":"clouds/aws/kinesis/#aws-user-access-key","title":"AWS User / Access Key","text":"<p>Now for our data generator to be able to connect we'll need a AWS user, credentials to work with kinesis and an access key/secret. In this setup we will set that up.</p>"},{"location":"clouds/aws/kinesis/#user_1","title":"User","text":"<p>Lets start by searching <code>IAM</code> and the going to users. </p> <p>Click create a new user. </p> <p>Give your user a name and click next. </p> <p>We need to attach a policy directly. Search for \"Kinesis\" and select <code>AmazonKinesisFullAccess</code>. This policy grants the permissions needed to push data to the stream. Once selected, click Next to proceed. </p> <p>Click create user. </p>"},{"location":"clouds/aws/kinesis/#access-key","title":"Access Key","text":"<p>After the user is created we'll select it by clicking on it's name. </p> <p>We'll want to create a access key by clicking \"Create access key\". </p> <p>The key will be used for the CLI. </p> <p>Click create access key. </p> <p>Now we'll want to copy our access key and secret access key, so we can put it into our CLI configuration in the next step. </p>"},{"location":"clouds/aws/kinesis/#aws-cli-setup","title":"AWS CLI Setup","text":"<p>In this section we'll setup the AWS SLI to be able to connect to our stream via python. I suggest using the GUI installer for this example.</p> <p>https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</p> <p>Click, download the cli and install it. It's pretty straight forward. </p> <p>Once installed well want to call and fill in the questions:</p>  Code Result <pre><code>aws configure\n</code></pre> <pre><code>dwilczak /snowflake/aws/kinesis/files:aws configure\n    AWS Access Key ID [None]: AKIAUZES76LQB6ZQHYEM\n    AWS Secret Access Key [None]: MbxsSKg4voO+c773.....cxwlYHWxB/Vsjdc\n    Default region name [None]: us-east-1\n    Default output format [None]: \n</code></pre> <p></p>"},{"location":"clouds/aws/kinesis/#python-data-generator","title":"Python data generator","text":"<p>Now we are ready to start generating the data that will be passed to kinesis and then loaded into our Snowflake table. We'll want to open the folder of the files we downloaded at the start of the tutorial.</p> <p>Lets start by updating our code to use the stream we setup at the beginning. Inside our <code>main.py</code> we'll update:</p>  Code Result <pre><code>StreamName=\"&lt;YOUR KINESIS STREAM NAME&gt;\",  # Name of the Kinesis stream\n</code></pre> <pre><code>StreamName=\"danielwilczak_stream\",  # Name of the Kinesis stream\n</code></pre> <p>Next we'll want to run that code to start generating the data which will be moved to Snowflake.</p>  Code Result <pre><code>python -m venv venv \nsource venv/bin/activate\npip install -r requirements.txt\npython main.py\n</code></pre> <pre><code>Message sent #1\nMessage sent #2\nMessage sent #3\nMessage sent #4\n...\n</code></pre> <p>We will be able to see the messages being sent to kinesis. Now we should start seeing our data being move to Snowflake. If you don't check your firehose logs. You might have a connection issue. </p>"},{"location":"clouds/aws/kinesis/#result","title":"Result","text":"<p>Now we should see our data being loaded into Snowflake. </p> <p>If we refresh our page we'll see new records being streamed into our table. </p>"},{"location":"clouds/aws/s3/","title":"S3","text":""},{"location":"clouds/aws/s3/#connect-snowflake-to-s3-storage","title":"Connect Snowflake to S3 Storage","text":"<p>Goal of this tutorial is to load JSON and CSV data from a S3 bucket using the Copy into sql command and Snowpipe to automate the ingestion process.</p>"},{"location":"clouds/aws/s3/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>AWS account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/aws/s3/#video","title":"Video","text":""},{"location":"clouds/aws/s3/#download","title":"Download","text":"<ul> <li>Sample data (Link)</li> </ul>"},{"location":"clouds/aws/s3/#manual-loading","title":"Manual Loading","text":"<p>Lets start by setting up a snowflake connection to AWS s3 storage and load json data. After that use snowpipe to automate the ingestion of CSV files.</p>"},{"location":"clouds/aws/s3/#aws","title":"AWS","text":"<p>Sign into your aws account.</p>"},{"location":"clouds/aws/s3/#create-s3-bucket","title":"Create S3 bucket","text":"<p>Create the bucket you intend to use. In our case we'll call the bucket danielwilczak. </p>"},{"location":"clouds/aws/s3/#upload-sample-data","title":"Upload sample data","text":"<p>Upload the sample data to your s3 bucket (json/csv) provided in the data folder. </p>"},{"location":"clouds/aws/s3/#policy-and-role","title":"Policy and role","text":"<p>Copy your ARN name. This wil be used in the policy step. </p> <p>Go to IAM: </p> <p>Create a policy: </p> <p>Copy the template policy json code below and add your buckets arn.</p>  Template Example <p></p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n            \"s3:GetObject\",\n            \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"&lt;COPY ARN HERE&gt;/*\" /* (1)! */\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"&lt;COPY ARN HERE&gt;\", /* (2)! */\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n            \"s3:GetObject\",\n            \"s3:GetObjectVersion\"\n            ],\n            \"Resource\": \"arn:aws:s3:::danielwilczak/*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\",\n                \"s3:GetBucketLocation\"\n            ],\n            \"Resource\": \"arn:aws:s3:::danielwilczak\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"s3:prefix\": [\n                        \"*\"\n                    ]\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Enter your policy using the template / example above and click next. </p> <p>Give the policy a name and then click \"create policy\". </p> <p>Next lets create a role! Navigate back to IAM: </p> <p>Select Roles on the navbar and then click create role: </p> <p>Select AWS account, This account(#), Require external ID and enter 0000 for now. We will update this later. </p> <p>Add the policy to the role: </p> <p>Add the role name and click \"create role\": </p> <p>Once created click your role: </p> <p>Copy your role ARN, this will be used in the next step: </p>"},{"location":"clouds/aws/s3/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a worksheet (1) and creating our database and schema. Followed by creating the integration to AWS by running the code below with your copied role arn and bucket name:</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code> use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.aws;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema aws;\nuse warehouse development;\n</code></pre>  Template Example Result <p></p><pre><code>use role accountadmin;\n\n/*\nIntegrations are on of those important features that\naccount admins should do because it's allowing outside \nsnowflake connections to your data.\n*/\ncreate storage integration s3_integration\n    type = external_stage\n    storage_provider = 's3'\n    enabled = true\n    storage_aws_role_arn = '&lt;ROLE ARN HERE&gt;' /* (1)! */\n    storage_allowed_locations = ('s3://&lt;BUCKET NAME&gt;/'); /* (2)! */\n\n-- Give the sysadmin access to use the integration.\ngrant usage on integration s3_integration to role sysadmin;\n\ndesc integration s3_integration;\nselect \"property\", \"property_value\" from TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nwhere \"property\" = 'STORAGE_AWS_IAM_USER_ARN' or \"property\" = 'STORAGE_AWS_EXTERNAL_ID';\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objectss.\ncreate schema if not exists raw.aws;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n/*\n    Integrations are on of those important features that\n    account admins should do because it's allowing outside \n    snowflake connections to your data.\n*/\nuse role accountadmin;\n\ncreate storage integration s3_integration\n    type = external_stage\n    storage_provider = 's3'\n    enabled = true\n    storage_aws_role_arn = 'arn:aws:iam::484577546576:role/danielwilczak-role'\n    storage_allowed_locations = ('s3://danielwilczak/');\n\n-- Give the sysadmin access to use the integration.\ngrant usage on integration s3_integration to role sysadmin;\n\ndesc integration s3_integration;\nselect \"property\", \"property_value\" from TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nwhere \"property\" = 'STORAGE_AWS_IAM_USER_ARN' or \"property\" = 'STORAGE_AWS_EXTERNAL_ID';\n</code></pre> property property_value STORAGE_AWS_IAM_USER_ARN arn:aws:iam::001782626159:user/8pbb0000-s STORAGE_AWS_EXTERNAL_ID GGB82720_SFCRole=2_vcN2MIiC7PW0OMOyA82W5BLJrqY= <p>Note the result, it will be used in the next step.</p>"},{"location":"clouds/aws/s3/#grant-access-in-s3","title":"Grant Access in S3","text":"<p>Navigate back to the role: </p> <p>Click trusted relationship: </p> <p>Click edit trust policy: </p> <p>Copy the policy json template code below and add your \"STORAGE_AWS_IAM_USER_ARN\" and \"STORAGE_AWS_EXTERNAL_ID\" from prior Snowflake step.</p>  Template Example <p></p><pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n    {\n        \"Sid\": \"\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n        \"AWS\": \"&lt;STORAGE_AWS_IAM_USER_ARN&gt;\" /* (1)! */\n        },\n        \"Action\": \"sts:AssumeRole\",\n        \"Condition\": {\n        \"StringEquals\": {\n            \"sts:ExternalId\": \"&lt;STORAGE_AWS_EXTERNAL_ID&gt;\" /* (2)! */\n        }\n        }\n    }\n    ]\n}\n</code></pre><p></p> <ol> <li> <p>From snowflake results we got earlier:</p> property property_value STORAGE_AWS_IAM_USER_ARN arn:aws:iam::001782626159:user/8pbb0000-s STORAGE_AWS_EXTERNAL_ID GGB82720_SFCRole=2_vcN2MIiC7PW0OMOyA82W5BLJrqY= </li> <li> <p>From snowflake results we got earlier:</p> property property_value STORAGE_AWS_IAM_USER_ARN arn:aws:iam::001782626159:user/8pbb0000-s STORAGE_AWS_EXTERNAL_ID GGB82720_SFCRole=2_vcN2MIiC7PW0OMOyA82W5BLJrqY= </li> </ol> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n    {\n        \"Sid\": \"\",\n        \"Effect\": \"Allow\",\n        \"Principal\": {\n        \"AWS\": \"arn:aws:iam::001782626159:user/8pbb0000-s\"\n        },\n        \"Action\": \"sts:AssumeRole\",\n        \"Condition\": {\n        \"StringEquals\": {\n            \"sts:ExternalId\": \"GGB82720_SFCRole=2_vcN2MIiC7PW0OMOyA82W5BLJrqY=\"\n        }\n        }\n    }\n    ]\n}\n</code></pre> <p>Enter your policy using the template / example above. </p>"},{"location":"clouds/aws/s3/#load-the-data","title":"Load the data","text":"<p>Lets setup the stage, file format, warehouse and finally load some json data.</p>  Template Example Result <p></p><pre><code>use role sysadmin;\nuse database raw;\nuse schema aws;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage s3\n    storage_integration = s3_integration\n    url = 's3://&lt;BUCKET NAME&gt;/' /* (1)! */\n    directory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format json\n    type = 'json';\n\n-- Create the table.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load json data.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @s3/json\n        (file_format =&gt; json)\n);\n</code></pre><p></p> <pre><code>use role sysadmin;\nuse database raw;\nuse schema aws;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage s3\n    storage_integration = s3_integration\n    url = 's3://danielwilczak/'\n    directory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format json\n    type = 'json';\n\n-- Create the table.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load json data.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @s3/json\n        (file_format =&gt; json)\n);\n</code></pre> file status s3://danielwilczak/json/sample.json LOADED <p>Look at the data you just loaded.</p>  Code <pre><code>select * from raw.aws.json; \n</code></pre> <p></p>"},{"location":"clouds/aws/s3/#automatic-loading","title":"Automatic Loading","text":"<p>Warning</p> <p>If you have not manually loaded data yet from S3. Please go back and complete that section first.</p> <p>Lets create a pipe to automate copying data into a table. Create the file format, table and pipe in snowflake. This approach automates the process so you don't have to manually name all the columns. This code will also give you your SQS queue string to be entered into AWS later.</p>"},{"location":"clouds/aws/s3/#snowflake_1","title":"Snowflake","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how AWS is feeling today.</p> <p>In this case we'll load a csv file by automating the creation of the table and infering the names in the csv pipe.</p>  Code Result <pre><code>use role sysadmin;\nuse database raw;\nuse schema aws;\nuse warehouse development;\n\n/*\n    Copy CSV data using a pipe without having\n    to write out the column names.\n*/\ncreate or replace file format infer\n    type = csv\n    parse_header = true\n    skip_blank_lines = true\n    field_optionally_enclosed_by ='\"'\n    trim_space = true\n    error_on_column_count_mismatch = false;\n\n/*\n    Creat the table with the column names\n    generated for us.\n*/\ncreate or replace table csv\n    using template (\n        select array_agg(object_construct(*))\n        within group (order by order_id)\n        from table(\n            infer_schema(        \n            LOCATION=&gt;'@s3/csv'\n        , file_format =&gt; 'infer')\n        )\n    );\n\n-- Create the pipe to load any new data.\ncreate pipe csv auto_ingest=true as\n    COPY into\n        csv\n    from\n        @s3/csv\n\n    file_format = (format_name= 'infer')\n    match_by_column_name=case_insensitive;\n\n/*\n    Get the arn for the pipe. We will add this\n    to aws in step the next AWS step.\n*/\nshow pipes;\nselect \"name\", \"notification_channel\" as sqs_queue\nfrom TABLE(RESULT_SCAN(LAST_QUERY_ID()));\n</code></pre> name sqs_queue CSV arn:aws:sqs:us-west-2:001782626159:sf-snowpipe-AIDAQ..."},{"location":"clouds/aws/s3/#grant-access-in-s3_1","title":"Grant Access in S3","text":"<p>Navigate to your bucket and click properties: </p> <p>Scroll down to \"Create event notification\": </p> <p>Add a name to the notification and select all object create notification: </p> <p>Scroll down and enter your sqs queue we got from our snowflake step and click \"save changes\": </p> <p>Almost done, in snowflake lets refresh the pipe so that we ingest all the current files.</p>"},{"location":"clouds/aws/s3/#load-the-data_1","title":"Load the data","text":"Code Result <pre><code>alter pipe raw.aws.csv refresh;\n</code></pre> File Status /sample_1.csv SENT"},{"location":"clouds/aws/s3/#result","title":"Result","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how AWS is feeling today.</p> <p>Let's add more sample data into the S3 bucket CSV folder and see it added in snowflake ~30 seconds later. We can see this by doing a count on our table and seeing 20 records, whereas the original CSV only has 10 records.</p>"},{"location":"clouds/azure/storage/","title":"Blob / Storage","text":""},{"location":"clouds/azure/storage/#connect-snowflake-to-azure-storage","title":"Connect Snowflake to Azure Storage","text":"<p>Goal of this tutorial is to load JSON and CSV data from a Azure Storage using the Copy into sql command and Snowpipe to automate the ingestion process.</p>"},{"location":"clouds/azure/storage/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Azure account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/azure/storage/#video","title":"Video","text":""},{"location":"clouds/azure/storage/#download","title":"Download","text":"<ul> <li>Sample data (Link)</li> </ul>"},{"location":"clouds/azure/storage/#manual-loading","title":"Manual Loading","text":"<p>Lets start by setting up a Snowflake connection to Azure storage and load json data. After that use snowpipe to automate the ingestion of CSV files.</p>"},{"location":"clouds/azure/storage/#azure","title":"Azure","text":"<p>Sign into your azure account.</p>"},{"location":"clouds/azure/storage/#create-storage-account","title":"Create Storage account","text":"<p>Create the stoage account you intend to use.  </p> <p>In our case we'll call the stoage account danielwilczak and make it's region the same as the one our Snowflake account. It's okay if it's not with the same provider or region, it's just the quickest. We will also use the default for everything else. If creating a resource group just enter a name and take the defaults. </p> <p>We will then navigate into the storage account. </p>"},{"location":"clouds/azure/storage/#create-container","title":"Create Container","text":"<p>Create a storage container.  </p> <p>In our case we will call it data. </p> <p>Navigate into the data container. </p>"},{"location":"clouds/azure/storage/#upload-sample-data","title":"Upload sample data","text":"<p>Lets upload some sample data </p> <p>Upload the sample data to your azure storage bucket (json/csv) provided in the data folder.</p> <p></p>"},{"location":"clouds/azure/storage/#get-teanent-id","title":"Get teanent ID","text":"<p>Using the search bar, look up tenant properties. </p> <p>Copy your Tenant ID. We will use this in two places. </p>"},{"location":"clouds/azure/storage/#snowflake","title":"Snowflake","text":"<p>Let's transition to Snowflake by creating a worksheet (1) and adding the code below with your information:</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code> /*\nWe switch to \"sysadmin\" to create an object\nbecause it will be owned by that role.\n*/\nuse role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw ;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.azure;\n\n/*\nWarehouses are synonymous with the idea of\ncompute resources in other systems.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema azure;\nuse warehouse development;\n</code></pre>  Template Example Result <p></p><pre><code>/*\nIntegrations are on of those important features that account admins\nshould do because it's allowing outside snowflake connections to your data.\n*/\nuse role accountadmin;\n\ncreate or replace storage integration azure_integration\n    type = external_stage\n    storage_provider = 'azure'\n    enabled = true \n    azure_tenant_id = '&lt;TENANT_ID&gt;' /* (1)! */\n        storage_allowed_locations = (\n'azure://&lt;STORAGE ACCOUNT NAME&gt;.blob.core.windows.net/&lt;CONTAINER NAME&gt;' /* (2)! */ /* (3)! */\n        );\n\n-- Give the sysadmin access to use the integration later.\ngrant usage on integration azure_integration to role sysadmin;\n\n-- Get the URL to authenticate with azure and the app name to use later.\ndescribe storage integration azure_integration;\nselect \"property\", \ncase  when \"property\" = 'AZURE_MULTI_TENANT_APP_NAME' then split_part(\"property_value\", '_', 1) else \"property_value\"end as \"property_value\"\nfrom table(result_scan(last_query_id()))\nwhere \"property\" in ('AZURE_CONSENT_URL', 'AZURE_MULTI_TENANT_APP_NAME');\n</code></pre><p></p> <ol> <li> <p>Using the search bar, look up tenant properties.     </p> <p>Copy your Tenant ID. </p> </li> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>use role accountadmin;\n\ncreate or replace storage integration azure_integration\n    type = external_stage\n    storage_provider = 'azure'\n    enabled = true \n    azure_tenant_id = '9a2dkd8cb-73e9-40ee-a558-fcdnj5ef57a7'\n        storage_allowed_locations = (\n            'azure://danielwilczak.blob.core.windows.net/data'\n        );\n\n-- Give the sysadmin access to use the integration later.\ngrant usage on integration azure_integration to role sysadmin;\n\n-- Get the URL to authenticate with azure and the app name to use later.\ndescribe storage integration azure_integration;\nselect \"property\", \ncase  when \"property\" = 'AZURE_MULTI_TENANT_APP_NAME' then split_part(\"property_value\", '_', 1) else \"property_value\"end as \"property_value\"\nfrom table(result_scan(last_query_id()))\nwhere \"property\" in ('AZURE_CONSENT_URL', 'AZURE_MULTI_TENANT_APP_NAME');\n</code></pre> property property_value AZURE_CONSENT_URL https://login.microsoftonline.com/9a2d... AZURE_MULTI_TENANT_APP_NAME c9pnugsnowflakepacint_1700096201187 <p>Please Ctrl+Click the URL or copy and paste it into your browser. </p>"},{"location":"clouds/azure/storage/#grant-access-in-azure","title":"Grant Access in Azure","text":"<p>Lets navigate to IAM so that we can give snowflake access to our storage account. </p> <p>Search for Storage Blob Data Contributor, select the role and click next. </p> <p>Lets add the Snowflake member by selecting \"select member\" and search for our AZURE_MULTI_TENANT_APP_NAME that we got earlier from snowflake. </p> <p></p> <p>Once selected, click Review and Assign. </p>"},{"location":"clouds/azure/storage/#load-the-data","title":"Load the data","text":"If you get an error when creating the stage. <p>Just wait 5-10 minutes and try again. Sometimes it takes Azure a bit to update security.</p> <p>Lets setup the stage, file format, warehouse and finally load some json data.</p>  Template Example Result <p></p><pre><code>use database raw;\nuse schema azure;\nuse role sysadmin;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage azure\nstorage_integration = azure_integration\nurl = 'azure://&lt;STORAGE NAME&gt;.blob.core.windows.net/&lt;CONTAINER NAME&gt;' /* (1)! */ /* (2)! */\ndirectory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format raw.azure.json\n    type = 'json';\n\n-- Create the table to load into.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load the json file from the json folder.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @azure/json\n        (file_format =&gt; json)\n);\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>use database raw;\nuse schema azure;\nuse role sysadmin;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage azure\n    storage_integration = azure_integration\n    url = 'azure://danielwilczak.blob.core.windows.net/data'\n    directory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format raw.azure.json\n    type = 'json';\n\n-- Create the table to load into.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load the json file from the json folder.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @azure/json\n        (file_format =&gt; json)\n);\n</code></pre> file status azure://danielwilczak.blob.core.windows.net/data/json/sample.json LOADED <p>Look at the data you just loaded.</p>  Code <pre><code>select * from raw.azure.json; \n</code></pre> <p></p>"},{"location":"clouds/azure/storage/#automatic-loading","title":"Automatic Loading","text":"<p>Warning</p> <p>If you have not manually loaded data yet from azure storage. Please go back and complete that section first.</p>"},{"location":"clouds/azure/storage/#azure_1","title":"Azure","text":"<p>Lets start with azure by creating a queue. All you need is a name. I've named mine <code>danielwilczak-snowflake-queue</code>.  </p> <p>Store the URL. It will be used later. </p> <p>Next lets create an event subscription.  </p> If you get an error with the topic follow this prosess to enable it <p>Search for subscription. </p> <p>Select your subscription. </p> <p>Search \"resource provider\" and select it. </p> <p>Search \"EventGrid\", select it, and the select register. This will enable it in your account. </p> <p>Wait for it to say registered. </p> <p>Enter in the highlighted fields. I've named mine <code>danielwilczak-snowflake-event</code>. Once entered click \"configure endpoint\". </p> <p>Enter the storage account and selct your queue that we setup prior. </p> <p>To finish the event, click \"create\". </p> <p>Result </p>"},{"location":"clouds/azure/storage/#snowflake_1","title":"Snowflake","text":"<p>Open a worksheet and enter in your <code>queue url</code> and <code>tenant_id</code>.</p>  Template Example Result <p></p><pre><code>use role accountadmin;\n\n-- Setup the storage notification to know when storage events happen.\ncreate or replace notification integration azure_snowpipe_integration\n    enabled = true\n    type = queue\n    notification_provider = azure_storage_queue\n    azure_storage_queue_primary_uri = '&lt;QUEUE URL&gt;' /* (1)! */\n    azure_tenant_id = '&lt;TENANT ID&gt;'; /* (2)! */\n\n-- Give the sysadmin access to use the integration.\ngrant usage on integration azure_snowpipe_integration to role sysadmin;\n\ndescribe storage integration azure_integration;\nselect \"property\", \ncase  when \"property\" = 'AZURE_MULTI_TENANT_APP_NAME' then split_part(\"property_value\", '_', 1) else \"property_value\"end as \"property_value\"\nfrom table(result_scan(last_query_id()))\nwhere \"property\" in ('AZURE_CONSENT_URL', 'AZURE_MULTI_TENANT_APP_NAME');\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p>Using the search bar, look up tenant properties.     </p> <p>Copy your Tenant ID. </p> </li> </ol> <pre><code>use role accountadmin;\n\n-- Setup the storage notification to know when storage events happen.\ncreate or replace notification integration azure_snowpipe_integration\n    enabled = true\n    type = queue\n    notification_provider = azure_storage_queue\n    azure_storage_queue_primary_uri = 'https://danielwilczak.queue.core.windows.net/danielwilczak-snowflake-queue'\n    azure_tenant_id = '9a2d78cb-73e9-40ee-a558-fc1adfff57a7';\n\n-- Give the sysadmin access to use the integration.\ngrant usage on integration azure_snowpipe_integration to role sysadmin;\n\ndescribe notification integration azure_snowpipe_integration;\nselect \"property\", \ncase  when \"property\" = 'AZURE_MULTI_TENANT_APP_NAME' then split_part(\"property_value\", '_', 1) else \"property_value\"end as \"property_value\"\nfrom table(result_scan(last_query_id()))\nwhere \"property\" in ('AZURE_CONSENT_URL', 'AZURE_MULTI_TENANT_APP_NAME');\n</code></pre> property PROPERTY_VALUE AZURE_CONSENT_URL https://login.microsoftonline.com/9a2d7... AZURE_MULTI_TENANT_APP_NAME tzpbnzsnowflakepacint_1700112906001 <p>Please ctrl+click the url or enter it into your browser you will have to accept the integration on azure.  </p>"},{"location":"clouds/azure/storage/#grant-access-in-azure_1","title":"Grant Access in Azure","text":"<p>Using the left side search \"iam\" and select Access Control and \"add role assignment\". </p> <p>Search for \"storage queue data contributor\" and select it and click next. </p> <p>Click \"Select members\" and add your <code>AZURE_MULTI_TENANT_APP_NAME</code> that we recieved from Snowflake in the prior snowflake step.  </p> <p>Final Azure step, select review and assign and your done with azure. </p>"},{"location":"clouds/azure/storage/#load-the-data_1","title":"Load the data","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how Azure is feeling today.</p> If you get an error when creating the pipe. <p>Just wait 5-10 minutes. Sometimes it takes Azure a bit to update security.</p> <p>We'll load a csv file by automating the creation of the table and infering the names in the csv pipe.</p>  SQL Result <pre><code>use role sysadmin;\nuse database raw;\nuse schema azure;\nuse warehouse development;\n\n/*\n    Copy CSV data using a pipe without having\n    to write out the column names.\n*/\ncreate or replace file format infer\n    type = csv\n    parse_header = true\n    skip_blank_lines = true\n    field_optionally_enclosed_by ='\"'\n    trim_space = true\n    error_on_column_count_mismatch = false;\n\n/*\n    Creat the table with the column names\n    generated for us.\n*/\ncreate or replace table csv\n    using template (\n        select array_agg(object_construct(*))\n        within group (order by order_id)\n        from table(\n            infer_schema(        \n            LOCATION=&gt;'@azure/csv'\n        , file_format =&gt; 'infer')\n        )\n    );\n\n/*\n    Load the data and assign the pipe notification\n    to know when a file is added.\n*/\ncreate or replace pipe csv \n    auto_ingest = true \n    integration = 'AZURE_SNOWPIPE_INTEGRATION' \n    as\n\n    COPY into\n        csv\n    from\n        @azure/csv\n\n    file_format = (format_name= 'infer')\n    match_by_column_name=case_insensitive;\n\n/* \n    Refresh the state of the pipe to make\n    sure it's updated with all files.\n*/\nalter pipe csv refresh;\n</code></pre> File Status /sample_1.csv SENT"},{"location":"clouds/azure/storage/#result","title":"Result","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how Azure is feeling today.</p> <p>Lets add more sample data into the azure csv folder and see it added in snowflake ~30 seconds later. We can see this by doing a count on our table and see 20 records where th original csv only has 10 records.</p>"},{"location":"clouds/google/external/","title":"External Tables","text":""},{"location":"clouds/google/external/#external-tables-google-cloud-storage","title":"External Tables - Google Cloud Storage","text":"<p>Goal of this tutorial is to setup a Snowflake external table on files that are stored in an external google cloud storage bucket.</p>"},{"location":"clouds/google/external/#video","title":"Video","text":"<p>Video in development.</p>"},{"location":"clouds/google/external/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Google cloud account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/google/external/#download","title":"Download","text":"<ul> <li>Sample Data</li> </ul>"},{"location":"clouds/google/external/#setup","title":"Setup","text":"<p>Warning</p> <p>This tutorial assumes you have already setup a stage with read/write privligies. If you have not please follow this tutorial.</p> <p>In this section we will upload the new sample files to a GCP folder and then setup Snowflake.</p> <p></p>"},{"location":"clouds/google/external/#external-table","title":"External Table","text":"<p>Let's not start to create our table and see the options we can have with regard to our table.</p>"},{"location":"clouds/google/external/#simple","title":"Simple","text":"<p>Lets create the table on top of the sample files we have stored in the bucket / stage.</p>  Code Result <pre><code>create or replace file format customers_csv_format\n    type = 'csv'\n    field_optionally_enclosed_by = '\"'\n    skip_header = 1;\n\ncreate or replace external table ext_customers (\n    customer_id number as (value:c1::number),\n    first_name string as (value:c2::string),\n    last_name string as (value:c3::string),\n    email string as (value:c4::string)\n)\n    location = @gcp/customers\n    file_format = customers_csv_format\n    auto_refresh = false;\n</code></pre> status Table EXT_CUSTOMERS successfully created. <p>Now you'll be able to query the data in the table as if it were a native Snowflake table. The data will not be refreshed automaticlly in this setup, it has to be triggered manually via:</p>  Code Result <pre><code>alter external table external_customers refresh;\n</code></pre> file status description customers/2025-04-21T17-20-44-970197.csv REGISTERED_NEW File registered successfully. customers/2025-04-21T17-20-44-932488.csv REGISTERED_NEW File registered successfully. customers/2025-04-21T17-20-44-919172.csv REGISTERED_NEW File registered successfully. customers/2025-04-21T17-20-44-944873.csv REGISTERED_NEW File registered successfully. customers/2025-04-21T17-20-44-957653.csv REGISTERED_NEW File registered successfully."},{"location":"clouds/google/external/#partition-by","title":"Partition by","text":"<p>After setting up our file format and querying larger, more complex datasets, we can improve performance by partitioning the external table. Partitioning by a column like file_created_at helps Snowflake prune files during queries, reducing scan time.</p>  Code Result <pre><code>create or replace external table external_customers (\n    customer_id number as (value:c1::number),\n    first_name string as (value:c2::string),\n    last_name string as (value:c3::string),\n    email string as (value:c4::string),\n    row_created_at timestamp_ntz as (value:c5::timestamp),\n    file_created_at date as cast(split_part(split_part(metadata$filename, '/', -1), 'T', 1) as date)\n)\n    partition by (file_created_at)\n    location = @gcp/customers\n    file_format = customers_csv_format\n    auto_refresh = false;\n</code></pre> status Table EXTERNAL_CUSTOMERS successfully created."},{"location":"clouds/google/external/#auto-refreshing","title":"Auto-Refreshing","text":"<p>Warning</p> <p>This sections assumes you have already setup event notification system via pub/sub on the stage/bucket. If you have not please follow this tutorial.</p> <p>Now that we have the data partitioned correctly we probably want to be able to see new data as it is loaded into the folder. To do se we will change the autorefresh to true. This assumes we already have event notifications setup similar to Snowpipe.</p>  Code Result <pre><code>create or replace external table external_customers (\n    customer_id number as (value:c1::number),\n    first_name string as (value:c2::string),\n    last_name string as (value:c3::string),\n    email string as (value:c4::string),\n    row_created_at timestamp_ntz as (value:c5::timestamp),\n    file_created_at date as cast(split_part(split_part(metadata$filename, '/', -1), 'T', 1) as date)\n)\n    partition by (file_created_at)\n    location = @gcp/customers\n    file_format = customers_csv_format\n    auto_refresh = true;\n</code></pre> status Table EXTERNAL_CUSTOMERS successfully created."},{"location":"clouds/google/fivetran/","title":"Fivetran Data Lake","text":""},{"location":"clouds/google/fivetran/#gcp-fivetran-lata-lake","title":"GCP / Fivetran - Lata Lake","text":"<p>Goal of this tutorial is to setup and integration with Fivetran, GCP and Snowflake to allow fivetran to load iceberg data into a GCP datalake and then integrate Snowflake to be the query engine on that data.</p>"},{"location":"clouds/google/fivetran/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"clouds/google/fivetran/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Google cloud account, you can setup a free account to get started.</li> <li>Fivetran account by signing up for a free trial.</li> </ul>"},{"location":"clouds/google/fivetran/#setup","title":"Setup","text":"<p>Warning</p> <p>Your GCP bucket and Snowflake acount have to be in the same region to be able to manage iceberg tables.</p> <p>Lets start by setting up our Google Cloud Storage bucket. After that we'll connect fivetran to it and finish it off with the Snowflake integration to the Fivtran hosted Polaris Catalog.</p>"},{"location":"clouds/google/fivetran/#google-cloud","title":"Google Cloud","text":"<p>Sign into your google account. </p>"},{"location":"clouds/google/fivetran/#create-project","title":"Create project","text":"<p>If you don't have a project, start by selecting/creating a project. </p> <p>Click create project. </p> <p>In our case we'll call the project <code>danielwilczak</code> and select the default <code>no organization</code> for the location. </p>"},{"location":"clouds/google/fivetran/#create-storage-bucket","title":"Create storage bucket","text":"<p>We will select our new project and click <code>cloud storage</code> to create a storage bucket. </p> <p>Click <code>create</code> or <code>create bucket</code>. </p> <p>Warning</p> <p>Your GCP bucket and Snowflake acount have to be in the same region to be able to manage iceberg tables.</p> <p>I'm going to name the bucket <code>danielwilczak</code> as well. Copy this name, we will use it later. We'll also want to make sure the region is the same as our Snowflake account. </p> <p>Lets now select our new bucket and add a folder. </p> <p>Click create folder. </p> <p>Give it a name, I've named mine fivetran because that is who is loading the data. </p> <p>You can now see we have our folder. Let's move to setting up Fivetran to load data into our folder. </p>"},{"location":"clouds/google/fivetran/#fivetran","title":"Fivetran","text":"<p>Let's now move to connecting fivetran to our bucket. To start let's add our bucket as a destination. Click destination in fivetran. </p> <p>Select GCP cloud storage as the location. </p> <p>Give the destination a name. </p> <p>We'll first want to copy our service account URL to give Fivetran permission to use the bucket. </p>"},{"location":"clouds/google/fivetran/#authorize-fivetran-in-gcp","title":"Authorize Fivetran in GCP","text":"<p>Let's head back to our bucket and click the check box followed by the permissions button. </p> <p>Click add principle. </p> <p>Paste our service account URL given by Fivetran and then click select role. </p> <p>We'll want to search for \"Storage Object Admin\" and then click on it in the dropdown. </p> <p>Once both are added click \"Save\". </p> <p>Now we can added our bucket name, folder and click save and test. </p> <p>Once tested your destination is ready to use. Let's view the destination. </p>"},{"location":"clouds/google/fivetran/#snowflake","title":"Snowflake","text":"<p>Warning</p> <p>Currently you must grab the catalog code when the destination is created otherwise the key will be hidden '*'. Fivetran is working on fixing this issue.</p> <p>Now that our data is loaded into our GCP bucket let's connect Snowflake to it using Fivetrans hosted Polaris catalog. Let's start by going back into our GCP destination in Fivetran to copy some Snowflake code. </p> <p>Once in the destination, on the top navigation bar, select \"Catalog Integration\", select Snowflake and then copy the sql code. We will use this in Snowflake next. </p> <p>Let's head into Snowflake and add the code from fivetran and the line below. This will allow us to see the tables that are managed by the catalog.</p>  Template Example Template <pre><code>-- To see the tables in the catalog\nselect system$list_iceberg_tables_from_catalog('&lt;Catalog Name&gt;', '', 0);\n</code></pre> <pre><code>-- To see the tables in the catalog\nselect system$list_iceberg_tables_from_catalog('fivetran_catalog_throwback_refinery', '', 0);\n</code></pre> <pre><code>[\n    {\"namespace\":\"google_analytics\",\"name\":\"conversion_events\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"events_report\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"accounts\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"content_group_report\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"audiences_report\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"properties\"},\n    {\"namespace\":\"google_analytics\",\"name\":\"conversions_report\"}\n]\n</code></pre>"},{"location":"clouds/google/fivetran/#grant-access-in-gcp","title":"Grant access in GCP","text":"<p>We'll now need to allow Snowflake to interact with with bucket via the external volume. To do so we'll </p>  Template Example Result <pre><code>use role accountadmin;\n\n-- Get our principal url to be used in GCP so we can connect to the bucket.\ndescribe external volume &lt;VOLUME NAME &gt;;\nselect \n    \"property\",\n    REPLACE(GET_PATH(PARSE_JSON(\"property_value\"), 'STORAGE_GCP_SERVICE_ACCOUNT')::STRING, '\"', '') AS url\nfrom\n    table(result_scan(last_query_id()))\nwhere\n    \"property\" = 'STORAGE_LOCATION_1';\n</code></pre> <pre><code>use role accountadmin;\n\n-- Get our principal url to be used in GCP so we can connect to the bucket.\ndescribe external volume fivetran_volume_throwback_refinery;\nselect \n    \"property\",\n    REPLACE(GET_PATH(PARSE_JSON(\"property_value\"), 'STORAGE_GCP_SERVICE_ACCOUNT')::STRING, '\"', '') AS url\nfrom\n    table(result_scan(last_query_id()))\nwhere\n    \"property\" = 'STORAGE_LOCATION_1';\n</code></pre> property URL STORAGE_LOCATION_1 kuyp12345@gcpuscentral1-1dfa.iam.gserviceaccount.com <p>Let's navigate to IAM so that we can give snowflake access to our storage account. </p> <p>Create a new role. </p> <p>Fill in the role information. We will call it <code>snowflake</code>. After that click <code>Add Permissions</code>. </p> <p>The permissions to select can be found on Snowflake's documentation. In this tutorial I have chosen <code>Data loading and unloading</code>. I have also provided a gif to show how to select the permissions because the user interface is terrible. </p> <p>Navigate back to our bucket. Click <code>permissions</code>, followed by <code>add principle</code>. </p> <p>In the new principles section, add your URL given by Snowflake earlier. </p> <p>Now add your role by clicking <code>select role</code> -&gt; <code>custom</code> -&gt; <code>snowflake</code>. The last one will be your role name. </p> If you get a 'Domain restricted sharing' error when you click 'Save'.  <p>If you run into this error it's because google cloud has updated their policy as of March 2024. We'll have to update them. First select your organization (not your project), then go to IAM in the search, followed by clicking \"grant access\". </p> <p>Next we'll add our user email into the new principals area. We'll search and click on \"Organization Policy Administrator\". </p> <p>Click save. </p> <p>Next we'll want to update the policy. By searching IAM, selecting organization policies, searching domain and clicking on \"Domain restricted sharing\". </p> <p>Click Manage policy. </p> <p>Note</p> <p>\"Allow All\" is the simple approach but feel free to use more fine grain approach via Snowflake documentation.</p> <p>We'll want to override the parent policy with a new rule. Select replace the policy and then select \"Allow All\". Click done and \"Set Policy.\" and your good to go.  </p> <p>The policy has been updated and you can retry adding the role to the new principal. </p> <p>Click <code>Save</code> and you're finished GCP setup. </p>"},{"location":"clouds/google/fivetran/#load-source-data","title":"Load source data","text":"<p>This section will be vary depending on what source you plan to use. In my example I would load some data from google analytics by adding it as a connector. </p> <p>Now if we check our bucket you can see fivetran has loaded the google analytics data into our data lake. </p>"},{"location":"clouds/google/fivetran/#create-table","title":"Create - Table","text":"<p>Let's create a database, schema and our first table in Snowflake using everything we have created.</p>  Template Example Result <pre><code>-- Create the database and schema to store the tables.\ncreate or replace database &lt;Database name&gt;;\ncreate or replace schema &lt;Schema&gt;;\n\n-- Create the table from  the data and the catalog.\ncreate or replace iceberg table &lt;Table name&gt;\n    external_volume = '&lt;volume&gt;'\n    catalog = '&lt;catalog&gt;'\n    catalog_namespace = '&lt;source name&gt;'\n    catalog_table_name = '&lt;table name&gt;'\n    auto_refresh = true;\n</code></pre> <pre><code>create or replace database fivetran;\ncreate or replace schema google_analytics_4;\n\n-- Create the table from  the data and the catalog.\ncreate or replace iceberg table accounts\n    external_volume = 'fivetran_volume_throwback_refinery'\n    catalog = 'fivetran_catalog_throwback_refinery'\n    catalog_namespace = 'google_analytics_4'\n    catalog_table_name = 'accounts'\n    auto_refresh = true;\n</code></pre> <p>UPDATE</p> <p>Now that we have our first table created let's query from it.</p>  Example Result <pre><code>select * from accounts;\n</code></pre> NAME DISPLAY_NAME REGION_CODE _FIVETRAN_SYNCED accounts/302439834 danielwilczak US 2025-04-24 19:48:34.101 -0700"},{"location":"clouds/google/fivetran/#automate-table-creation","title":"Automate - Table Creation","text":"<p>Now let's automate the creation of all the tables using a Py. We'll add this python code into a \"thon scriptPython Worksheet\". We'll want to fill in our \"catalog integration name\" and \"external volume name\" that we got from fivetran.</p>  Example Example Result <pre><code>import snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col, lit\nimport json\nimport datetime\nimport pandas as pd\nfrom functools import reduce\n\nCATALOG_INTEGRATION_NAME = '&lt;Catalog name&gt;'\nEXTERNAL_VOLUME_NAME     = '&lt;External volume name&gt;'\n\ndef main(session: snowpark.Session):\n    # Fetch a list of tables from the external catalog\n    external_catalog_tables = [\n        row\n        for rs in session.sql(f\"select SYSTEM$LIST_ICEBERG_TABLES_FROM_CATALOG('{CATALOG_INTEGRATION_NAME}','',0) as TABLE_LIST\").collect()\n        for row in json.loads(rs.as_dict()[\"TABLE_LIST\"])\n    ]\n\n    # Convert the tables to the appropriate CREATE SCHEMA and CREATE ICEBERG TABLE statements\n    statements = [\n        sql\n        for table in external_catalog_tables\n        for sql in [create_schema(table), create_table(table)]\n    ]\n\n    # Execute each of the statements and merge the resulting dataframes into one combined dataframe to show the user\n    results = reduce(lambda left, right: left.union_all(right), [\n        exec(session, statement)\n        for statement in statements\n    ])\n\n    # Identify any tables that exist in CURRENT_DATABASE() that are not tracked in the external catalog and optionally\n    sync_dropped_tables(session, external_catalog_tables, drop=False)\n\n    return results.sort(col('statement_timestamp'))\n\ndef create_schema(table):\n    return f\"\"\"\nCREATE SCHEMA if not exists {table['namespace']}\nEXTERNAL_VOLUME = '{EXTERNAL_VOLUME_NAME}'\nCATALOG='{CATALOG_INTEGRATION_NAME}'\n\"\"\"\n\ndef create_table(table):\n    return f\"\"\"\nCREATE OR REPLACE ICEBERG TABLE {table['namespace']}.{table['name']}\nEXTERNAL_VOLUME = '{EXTERNAL_VOLUME_NAME}'\nCATALOG='{CATALOG_INTEGRATION_NAME}'\nCATALOG_NAMESPACE= '{table['namespace']}'\nCATALOG_TABLE_NAME = '{table['name']}'\nAUTO_REFRESH=TRUE;\n\"\"\"\n\ndef exec_and_aggregate_results(session: snowpark.Session, dataframe: snowpark.DataFrame, sql: str) -&gt; snowpark.DataFrame:\n    results = session.sql(sql)\n    results = results.with_column('statement_timestamp', lit(datetime.datetime.now()))\n    results = results.with_column('statement', lit(sql))\n\n    return dataframe.union_all(results) if dataframe else results\n\ndef exec(session: snowpark.Session, sql: str) -&gt; snowpark.DataFrame:\n    results = session.sql(sql)\n    results = results.with_column('statement_timestamp', lit(datetime.datetime.now()))\n    results = results.with_column('statement', lit(sql))\n    return results\n\ndef sync_dropped_tables(session: snowpark.Session, external_catalog_tables: list, drop=False):\n    all_tables_df = session.sql(\"\"\"\nSELECT CONCAT(table_schema, '.', table_name) AS FQTN, table_schema, table_name  FROM INFORMATION_SCHEMA.TABLES WHERE table_catalog = CURRENT_DATABASE() and table_schema NOT IN ('INFORMATION_SCHEMA', 'PUBLIC')\n\"\"\").toPandas()\n\n    external_catalog_tables_df = pd.DataFrame.from_dict(external_catalog_tables) \n    external_catalog_tables_df['FQTN'] = external_catalog_tables_df[\"namespace\"].str.upper() + \".\" + external_catalog_tables_df[\"name\"].str.upper()\n\n    tables_to_drop = all_tables_df.merge(external_catalog_tables_df, on='FQTN', how='left', indicator=True)\n    tables_to_drop = tables_to_drop[tables_to_drop['_merge'] == 'left_only'].drop(columns=['_merge'])\n    drop_statements = tables_to_drop[\"FQTN\"].map(lambda fqtn: f\"\"\"DROP TABLE {fqtn}\"\"\")\n\n    for sql in drop_statements:\n        if drop:\n            session.sql(sql)\n            print(f\"Dropped orphan table: {sql}\")\n        else:\n            print(\"Orphan table detected. Run the following to drop it:\")\n            print(sql)\n</code></pre> <pre><code>import snowflake.snowpark as snowpark\nfrom snowflake.snowpark.functions import col, lit\nimport json\nimport datetime\nimport pandas as pd\nfrom functools import reduce\n\nCATALOG_INTEGRATION_NAME = 'fivetran_catalog_throwback_refinery'\nEXTERNAL_VOLUME_NAME     = 'fivetran_volume_throwback_refinery'\n\ndef main(session: snowpark.Session):\n    # Fetch a list of tables from the external catalog\n    external_catalog_tables = [\n        row\n        for rs in session.sql(f\"select SYSTEM$LIST_ICEBERG_TABLES_FROM_CATALOG('{CATALOG_INTEGRATION_NAME}','',0) as TABLE_LIST\").collect()\n        for row in json.loads(rs.as_dict()[\"TABLE_LIST\"])\n    ]\n\n    # Convert the tables to the appropriate CREATE SCHEMA and CREATE ICEBERG TABLE statements\n    statements = [\n        sql\n        for table in external_catalog_tables\n        for sql in [create_schema(table), create_table(table)]\n    ]\n\n    # Execute each of the statements and merge the resulting dataframes into one combined dataframe to show the user\n    results = reduce(lambda left, right: left.union_all(right), [\n        exec(session, statement)\n        for statement in statements\n    ])\n\n    # Identify any tables that exist in CURRENT_DATABASE() that are not tracked in the external catalog and optionally\n    sync_dropped_tables(session, external_catalog_tables, drop=False)\n\n    return results.sort(col('statement_timestamp'))\n\ndef create_schema(table):\n    return f\"\"\"\nCREATE SCHEMA if not exists {table['namespace']}\nEXTERNAL_VOLUME = '{EXTERNAL_VOLUME_NAME}'\nCATALOG='{CATALOG_INTEGRATION_NAME}'\n\"\"\"\n\ndef create_table(table):\n    return f\"\"\"\nCREATE OR REPLACE ICEBERG TABLE {table['namespace']}.{table['name']}\nEXTERNAL_VOLUME = '{EXTERNAL_VOLUME_NAME}'\nCATALOG='{CATALOG_INTEGRATION_NAME}'\nCATALOG_NAMESPACE= '{table['namespace']}'\nCATALOG_TABLE_NAME = '{table['name']}'\nAUTO_REFRESH=TRUE;\n\"\"\"\n\ndef exec_and_aggregate_results(session: snowpark.Session, dataframe: snowpark.DataFrame, sql: str) -&gt; snowpark.DataFrame:\n    results = session.sql(sql)\n    results = results.with_column('statement_timestamp', lit(datetime.datetime.now()))\n    results = results.with_column('statement', lit(sql))\n\n    return dataframe.union_all(results) if dataframe else results\n\ndef exec(session: snowpark.Session, sql: str) -&gt; snowpark.DataFrame:\n    results = session.sql(sql)\n    results = results.with_column('statement_timestamp', lit(datetime.datetime.now()))\n    results = results.with_column('statement', lit(sql))\n    return results\n\ndef sync_dropped_tables(session: snowpark.Session, external_catalog_tables: list, drop=False):\n    all_tables_df = session.sql(\"\"\"\nSELECT CONCAT(table_schema, '.', table_name) AS FQTN, table_schema, table_name  FROM INFORMATION_SCHEMA.TABLES WHERE table_catalog = CURRENT_DATABASE() and table_schema NOT IN ('INFORMATION_SCHEMA', 'PUBLIC')\n\"\"\").toPandas()\n\n    external_catalog_tables_df = pd.DataFrame.from_dict(external_catalog_tables) \n    external_catalog_tables_df['FQTN'] = external_catalog_tables_df[\"namespace\"].str.upper() + \".\" + external_catalog_tables_df[\"name\"].str.upper()\n\n    tables_to_drop = all_tables_df.merge(external_catalog_tables_df, on='FQTN', how='left', indicator=True)\n    tables_to_drop = tables_to_drop[tables_to_drop['_merge'] == 'left_only'].drop(columns=['_merge'])\n    drop_statements = tables_to_drop[\"FQTN\"].map(lambda fqtn: f\"\"\"DROP TABLE {fqtn}\"\"\")\n\n    for sql in drop_statements:\n        if drop:\n            session.sql(sql)\n            print(f\"Dropped orphan table: {sql}\")\n        else:\n            print(\"Orphan table detected. Run the following to drop it:\")\n            print(sql)\n</code></pre> status STATEMENT_TIMESTAMP STATEMENT Schema GOOGLE_ANALYTICS successfully created. 2025-05-23 14:28:14.514 CREATE SCHEMA if not exists google_analytics EXTERNAL_VOLUME = 'fivetran_volume_buffer_concierge' CATALOG='fivetran_catalog_buffer_concierge' Table CONVERSION_EVENTS successfully created. 2025-05-23 14:28:14.517 CREATE OR REPLACE ICEBERG TABLE google_analytics.conversion_events EXTERNAL_VOLUME = 'fivetran_volume_buffer_concierge' CATALOG='fivetran_catalog_buffer_concierge' CATALOG_NAMESPACE= 'google_analytics' CATALOG_TABLE_NAME = 'conversion_events' AUTO_REFRESH=TRUE; GOOGLE_ANALYTICS already exists, statement succeeded. 2025-05-23 14:28:14.520 CREATE SCHEMA if not exists google_analytics EXTERNAL_VOLUME = 'fivetran_volume_buffer_concierge' CATALOG='fivetran_catalog_buffer_concierge'"},{"location":"clouds/google/iceberg/","title":"Iceberg","text":""},{"location":"clouds/google/iceberg/#iceberg-google-cloud-storage","title":"Iceberg - Google Cloud Storage","text":"<p>Goal of this tutorial is to setup the an external volume in Snowflake to be able to create and manage iceberg table.</p>"},{"location":"clouds/google/iceberg/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"clouds/google/iceberg/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Google cloud account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/google/iceberg/#setup","title":"Setup","text":"<p>Warning</p> <p>Your GCP bucket and Snowflake acount have to be in the same region to be able to create iceberg tables.</p> <p>Lets start by setting up a Snowflake connection to Google Cloud Storage. After that we'll create and load data into some Iceberg tables.</p>"},{"location":"clouds/google/iceberg/#google-cloud","title":"Google Cloud","text":"<p>Sign into your google account. </p>"},{"location":"clouds/google/iceberg/#create-project","title":"Create project","text":"<p>If you don't have a project, start by selecting/creating a project. </p> <p>Click create project. </p> <p>In our case we'll call the project <code>danielwilczak</code> and select the default <code>no orginization</code> for the locaition. </p>"},{"location":"clouds/google/iceberg/#create-cloud-storage","title":"Create cloud storage","text":"<p>We will select our new project and click <code>cloud storage</code> to create a storage bucket. </p> <p>Click <code>create</code> or <code>create bucket</code>. </p> <p>I'm going to name the bucket <code>danielwilczak</code> as well. Copy this name, we will use it later. </p>"},{"location":"clouds/google/iceberg/#snowflake","title":"Snowflake","text":"<p>Let's setup snowflake by creating a worksheet in snowflake and add the code below with your bucket name from earlier and hit run.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.gcp;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema gcp;\nuse warehouse development;\n</code></pre>  Template Example Result <p></p><pre><code>use role accountadmin;\n\n-- Create a volume which will act as our connection to GCP.\ncreate or replace external volume external_volume\n    storage_locations =\n    (\n        (\n        name = 'external_volume'\n        storage_provider = 'GCS'\n        storage_base_url = 'gcs://&lt;storage bucket name&gt;/' /* (1)! */\n        )\n    );\n\n-- Grant sysadmin access to the volume.\ngrant all on external volume external_volume to role sysadmin with grant option;\n\n-- Get our principal url to be used in GCP.\ndescribe external volume external_volume;\nselect \n    \"property\",\n    REPLACE(GET_PATH(PARSE_JSON(\"property_value\"), 'STORAGE_GCP_SERVICE_ACCOUNT')::STRING, '\"', '') AS url\nfrom\n    table(result_scan(last_query_id()))\nwhere\n    \"property\" = 'STORAGE_LOCATION_1';\n</code></pre><p></p> <pre><code>use role accountadmin;\n\n-- Create a volume which will act as our connection to GCP.\ncreate or replace external volume external_volume\n    storage_locations =\n    (\n        (\n        name = 'external_volume'\n        storage_provider = 'GCS'\n        storage_base_url = 'gcs://danielwilczak/'\n        )\n    );\n\n-- Grant sysadmin access to the volume.\ngrant all on external volume external_volume to role sysadmin with grant option;\n\n-- Get our principal url to be used in GCP.\ndescribe external volume external_volume;\nselect \n    \"property\",\n    REPLACE(GET_PATH(PARSE_JSON(\"property_value\"), 'STORAGE_GCP_SERVICE_ACCOUNT')::STRING, '\"', '') AS url\nfrom\n    table(result_scan(last_query_id()))\nwhere\n    \"property\" = 'STORAGE_LOCATION_1';\n</code></pre> property URL STORAGE_LOCATION_1 jtongh...k@prod3-f617.iam.gserviceaccount.com"},{"location":"clouds/google/iceberg/#grant-access-in-google-cloud","title":"Grant Access in Google Cloud","text":"<p>Lets navigate to IAM so that we can give snowflake access to our storage account. </p> <p>Create a new role. </p> <p>Fill in the role information. We will call it <code>snowflake</code>. After that click <code>Add Permissions</code>. </p> <p>The permissions to select can be found on Snowflake's documentation. In this tutorial I have choosen <code>Data loading and unloading</code>. I have also provided a gif to show how to select the permissions because the user interface is terrible. </p> <p>Navigate back to our bucket. Click <code>permissions</code>, followed by <code>add principle</code>. </p> <p>In the new principles section, add your STORAGE_GCP_SERVICE_ACCOUNT given by Snowflake earlier. </p> <p>Now add your role by clicking <code>select role</code> -&gt; <code>custom</code> -&gt; <code>snowflake</code>. The last one will be your role name. </p> If you get a 'Domain restricted sharing' error when you click 'Save'.  <p>If you run into this error it's because google cloud has updated thier policy as of March 2024. We'll have to update them. First select your orginization (not your project), then go to IAM in the search, followed by clicking \"grant access\". </p> <p>Next we'll add our user email into the new principals area. We'll search and click on \"Organization Policy Administrator\". </p> <p>Click save. </p> <p>Next we'll want to update the policy. By searching IAM, selecting orgianization policies, searching domain and clicking on \"Domain restricted sharing\". </p> <p>Click Manage polcy. </p> <p>Note</p> <p>\"Allow All\" is the simple approach but feel free to use more fine grain approach via Snowflake documentation.</p> <p>We'll want to overide the parent policy with a new rule. Select replace the policy and then select \"Allow All\". Click done and \"Set Polcy.\" and your good to go.  </p> <p>The policy has been updated and you can retry adding the role to the new principal. </p> <p>Click <code>Save</code> and your finished with Google Cloud for manual loading. </p>"},{"location":"clouds/google/iceberg/#iceberg-table","title":"Iceberg Table","text":"<p>Lets create a table and add data to it.</p>  Code Result <pre><code>use role sysadmin;\n\ncreate or replace iceberg table csv (x integer, y integer)  \n    catalog='SNOWFLAKE'\n    external_volume='external_volume'\n    base_location='iceberg';\n\ninsert into csv (x, y)\n    values (1, 2),\n        (3, 4),\n        (5, 6);\n</code></pre> number of rows inserted 3 <p>Once the table is created we can query data living in the table that really lives in our GCP bucekt.</p>  Code Result <pre><code>select * from csv;\n</code></pre> X Y 1 2 3 4 5 6 <p>We can also look in our bucket under the folder \"iceberg\" to see our metadata and data stored in parquet files. </p>"},{"location":"clouds/google/storage/","title":"Cloud Storage","text":""},{"location":"clouds/google/storage/#connect-snowflake-to-google-cloud-storage","title":"Connect Snowflake to Google Cloud Storage","text":"<p>Goal of this tutorial is to load JSON and CSV data from a Google Cloud Storage using the Copy into sql command and Snowpipe to automate the ingestion process.</p>"},{"location":"clouds/google/storage/#video","title":"Video","text":""},{"location":"clouds/google/storage/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Google cloud account, you can setup a free account to get started.</li> </ul>"},{"location":"clouds/google/storage/#download","title":"Download","text":"<ul> <li>Sample data (Link)</li> </ul>"},{"location":"clouds/google/storage/#manual-loading","title":"Manual Loading","text":"<p>Lets start by setting up a Snowflake connection to Google Cloud Storage and load json data. After that use snowpipe to automate the ingestion of CSV files.</p>"},{"location":"clouds/google/storage/#google-cloud","title":"Google Cloud","text":"<p>Sign into your google account. </p>"},{"location":"clouds/google/storage/#create-project","title":"Create project","text":"<p>If you don't have a project, start by selecting/creating a project. </p> <p>Click create project. </p> <p>In our case we'll call the project <code>danielwilczak</code> and select the default <code>no orginization</code> for the locaition. </p>"},{"location":"clouds/google/storage/#create-cloud-storage","title":"Create cloud storage","text":"<p>We will select our new project and click <code>cloud storage</code> to create a storage bucket. </p> <p>Click <code>create</code> or <code>create bucket</code>. </p> <p>I'm going to name the bucket <code>danielwilczak</code> as well. Copy this name, we will use it later. </p>"},{"location":"clouds/google/storage/#upload-sample-data","title":"Upload sample data","text":"<p>Upload the sample data to your google cloud storage bucket (json/csv) provided in the data folder. </p>"},{"location":"clouds/google/storage/#snowflake","title":"Snowflake","text":"<p>Let's setup snowflake by creating a worksheet in snowflake and add the code below with your bucket name from earlier and hit run:</p>  Template Example Result <p></p><pre><code>/*\n    We switch to \"sysadmin\" to create an object\n    because it will be owned by that role.\n*/\nuse role sysadmin;\n\n--- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.gcp;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n/*\n    Integrations are on of those important features that\n    account admins should do because it's allowing outside \n    snowflake connections to your data.\n*/\nuse role accountadmin;\n\ncreate storage integration gcp_integration\n    type = external_stage\n    storage_provider = 'gcs'\n    enabled = true\n    storage_allowed_locations = ('gcs://&lt;storage bucket name&gt;'); /* (1)! */\n\n-- Give the sysadmin access to use the integration.\ngrant usage on integration gcp_integration to role sysadmin;\n\ndesc storage integration gcp_integration;\nselect \"property\", \"property_value\" as principal from table(result_scan(last_query_id()))\nwhere \"property\" = 'STORAGE_GCP_SERVICE_ACCOUNT';\n</code></pre><p></p> <pre><code>/*\n    We switch to \"sysadmin\" to create an object\n    because it will be owned by that role.\n*/\nuse role sysadmin;\n\n--- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objectss.\ncreate schema if not exists raw.gcp;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n/*\n    Integrations are on of those important features that\n    account admins should do because it's allowing outside \n    snowflake connections to your data.\n*/\nuse role accountadmin;\n\ncreate storage integration gcp_integration\n    type = external_stage\n    storage_provider = 'gcs'\n    enabled = true\n    storage_allowed_locations = ('gcs://danielwilczak');\n\n-- give the sysadmin access to use the integration.\ngrant usage on integration gcp_integration to role sysadmin;\n\ndesc storage integration gcp_integration;\nselect \"property\", \"property_value\" as principal from table(result_scan(last_query_id()))\nwhere \"property\" = 'STORAGE_GCP_SERVICE_ACCOUNT';\n</code></pre> property principal STORAGE_GCP_SERVICE_ACCOUNT rbnxkdkujw@prod3-f617.iam.gserviceaccount.com"},{"location":"clouds/google/storage/#grant-access-in-google-cloud","title":"Grant Access in Google Cloud","text":"<p>Lets navigate to IAM so that we can give snowflake access to our storage account. </p> <p>Create a new role. </p> <p>Fill in the role information. We will call it <code>snowflake</code>. After that click <code>Add Permissions</code>. </p> <p>The permissions to select can be found on Snowflake's documentation. In this tutorial I have choosen <code>Data loading and unloading</code>. I have also provided a gif to show how to select the permissions because the user interface is terrible. </p> <p>Navigate back to our bucket. Click <code>permissions</code>, followed by <code>add principle</code>. </p> <p>In the new principles section, add your STORAGE_GCP_SERVICE_ACCOUNT given by Snowflake earlier. </p> <p>Now add your role by clicking <code>select role</code> -&gt; <code>custom</code> -&gt; <code>snowflake</code>. The last one will be your role name. </p> If you get a 'Domain restricted sharing' error when you click 'Save'.  <p>If you run into this error it's because google cloud has updated thier policy as of March 2024. We'll have to update them. First select your orginization (not your project), then go to IAM in the search, followed by clicking \"grant access\". </p> <p>Next we'll add our user email into the new principals area. We'll search and click on \"Organization Policy Administrator\". </p> <p>Click save. </p> <p>Next we'll want to update the policy. By searching IAM, selecting orgianization policies, searching domain and clicking on \"Domain restricted sharing\". </p> <p>Click Manage polcy. </p> <p>Note</p> <p>\"Allow All\" is the simple approach but feel free to use more fine grain approach via Snowflake documentation.</p> <p>We'll want to overide the parent policy with a new rule. Select replace the policy and then select \"Allow All\". Click done and \"Set Polcy.\" and your good to go.  </p> <p>The policy has been updated and you can retry adding the role to the new principal. </p> <p>Click <code>Save</code> and your finished with Google Cloud for manual loading. </p>"},{"location":"clouds/google/storage/#load-the-data","title":"Load the data","text":"<p>Lets setup the stage, file format and finally load some json data.</p>  Template Example Result <p></p><pre><code>use database raw;\nuse schema gcp;\nuse role sysadmin;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage raw.gcp.gcp\n    storage_integration = gcp_integration\n    url = 'gcs://&lt;BUCKET NAME&gt;/' /* (1)! */\n    directory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format json\n    type = 'json';\n\n\n-- Create the table to load into.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load the json file from the json folder.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @gcp/json\n        (file_format =&gt; json)\n);\n</code></pre><p></p> <pre><code>use database raw;\nuse schema gcp;\nuse role sysadmin;\nuse warehouse development;\n\n/*\n   Stages are synonymous with the idea of folders\n   that can be either internal or external.\n*/\ncreate or replace stage raw.gcp.gcp\n    storage_integration = gcp_integration\n    url = 'gcs://danielwilczak/'\n    directory = ( enable = true);\n\n/* \n    Create a file format so the \"copy into\"\n    command knows how to copy the data.\n*/\ncreate or replace file format raw.gcp.json\n    type = 'json';\n\n-- Create the table to load into.\ncreate or replace table json (\n    file_name varchar,\n    data variant\n);\n\n-- Load the json file from the json folder.\ncopy into json(file_name,data)\nfrom (\n    select \n        metadata$filename,\n        $1\n    from\n        @gcp/json\n        (file_format =&gt; json)\n);\n</code></pre> file status gcs://danielwilczak/json/sample.json LOADED <p>Look at the data you just loaded.</p>  Code <pre><code>select * from raw.gcp.json; \n</code></pre> <p></p>"},{"location":"clouds/google/storage/#automatic-loading","title":"Automatic Loading","text":"<p>Warning</p> <p>If you have not manually loaded data yet from Google Cloud storage. Please go back and complete that section first.</p>"},{"location":"clouds/google/storage/#google-cloud_1","title":"Google Cloud","text":"<p>First we'll start by clicking on the project selector. </p> <p>We'll copy our project id. We will use this later. </p> <p>Next we'll open the google cloud command line interface (CLI). </p> <p>You will have to authorize the CLI but once opened a window below will show. </p> <p>First we'll set the current project</p>  Template Example <pre><code>gcloud config set project &lt;Project ID&gt;\n</code></pre> <pre><code>gcloud config set project danielwilczak\n</code></pre> <p></p> <p>Next we'll create the notification topic for our cloud storage.</p>  Template Example <p></p><pre><code>gsutil notification create -t snowpipe -f json gs://&lt;Storage Bucket Name&gt;/ /* (1)! */\n</code></pre><p></p> <pre><code>gsutil notification create -t snowpipe -f json gs://danielwilczak/\n</code></pre> <p></p> <p>After that we're done with the CLI. We can close it. </p> <p>Next we'll navigate to pub/sub using the search bar. </p> <p>We'll click on the topic we created. </p> <p>We'll click <code>create subscription</code>. </p> <p>We'll give it a name. I used <code>snowpipe_subscription</code> and make sure it's set to <code>Pull</code>. </p> <p>We'll click <code>create</code>. </p> <p>Once created we'll copy the full <code>subscription name</code>. We will use this in the next step. </p>"},{"location":"clouds/google/storage/#snowflake_1","title":"Snowflake","text":"<p>Lets create the notification integration in a Snowflake worksheet.</p>  Template Example Result <p></p><pre><code>use role accountadmin;\n\ncreate or replace notification integration gcp_notification_integration\n    type = queue\n    notification_provider = gcp_pubsub\n    enabled = true\n    gcp_pubsub_subscription_name = '&lt;SUBSCRIPTION NAME&gt;'; /* (1)! */\n\ngrant usage on integration gcp_notification_integration to role sysadmin;\n\ndesc notification integration gcp_notification_integration;\nselect \"property\", \"property_value\" as principal from table(result_scan(last_query_id()))\nwhere \"property\" = 'GCP_PUBSUB_SERVICE_ACCOUNT';\n</code></pre><p></p> <pre><code>use role accountadmin;\n\ncreate or replace notification integration gcp_notification_integration\n    type = queue\n    notification_provider = gcp_pubsub\n    enabled = true\n    gcp_pubsub_subscription_name = 'projects/danielwilczak/subscriptions/snowpipe_subscription';\n\ngrant usage on integration gcp_notification_integration to role sysadmin;\n\ndesc notification integration gcp_notification_integration;\nselect \"property\", \"property_value\" as principal from table(result_scan(last_query_id()))\nwhere \"property\" = 'GCP_PUBSUB_SERVICE_ACCOUNT';\n</code></pre> property principal GCP_PUBSUB_SERVICE_ACCOUNT geimkrazlq@prod3-f617.iam.gserviceaccount.com"},{"location":"clouds/google/storage/#grant-access-in-google-cloud_1","title":"Grant Access in Google Cloud","text":"<p>Lets go back into google cloud and click on our subscription. </p> <p>Click show panel if not open already. </p> <p>Click <code>Add Principle</code>. </p> <p>Add your principle login user we got from snowflake in the prior step. </p> <p>Click select role and select <code>Pub / Sub</code> -&gt; <code>Pub / Sub Subscriber</code>. </p> <p>Click <code>Save</code>. </p> <p>Next we'll want to go back to IAM. </p> <p>Click <code>Grant Access</code>. </p> <p>Add your principle login user we got from snowflake in the prior step. </p> <p>Click select role and search <code>Monitoring Viewer</code> and click <code>Monitoring Viewer</code>. </p> <p>Final Google Cloud step - Click <code>Save</code>. </p>"},{"location":"clouds/google/storage/#load-the-data_1","title":"Load the data","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how Google Cloud is feeling today.</p> <p>In this case we'll load a csv file by automating the creation of the table and infering the names in the csv pipe.</p>  SQL Result <pre><code>use role sysadmin;\nuse database raw;\nuse schema gcp;\nuse warehouse development;\n\n/*\n    Copy CSV data using a pipe without having\n    to write out the column names.\n*/\ncreate or replace file format infer\n    type = csv\n    parse_header = true\n    skip_blank_lines = true\n    field_optionally_enclosed_by ='\"'\n    trim_space = true\n    error_on_column_count_mismatch = false;\n\n/*\n    Creat the table with the column names\n    generated for us.\n*/\ncreate or replace table csv\n    using template (\n        select array_agg(object_construct(*))\n        within group (order by order_id)\n        from table(\n            infer_schema(        \n            LOCATION=&gt;'@gcp/csv'\n        , file_format =&gt; 'infer')\n        )\n    );\n\n/*\n    Load the data and assign the pipe notification\n    to know when a file is added.\n*/\ncreate or replace pipe csv \n    auto_ingest = true \n    integration = 'GCP_NOTIFICATION_INTEGRATION' \n    as\n\n    COPY into\n        csv\n    from\n        @gcp/csv\n\n    file_format = (format_name= 'infer')\n    match_by_column_name=case_insensitive;\n\n/* \n    Refresh the state of the pipe to make\n    sure it's updated with all files.\n*/\nalter pipe csv refresh;\n</code></pre> File Status /sample_1.csv SENT"},{"location":"clouds/google/storage/#result","title":"Result","text":"<p>Note</p> <p>Sometimes it may take 1-2 minutes before you see data in the table. This depends on how Google Cloud is feeling today.</p> <p>Lets add more sample data into the google cloud storage bucket csv folder and see it added in snowflake ~30 seconds later. We can see this by doing a count on our table and see 20 records where the original csv only has 10 records.</p>"},{"location":"engineering/functions/chatgpt/","title":"Chatgpt","text":""},{"location":"engineering/functions/chatgpt/#chatgpt-in-snowflake","title":"Chatgpt in Snowflake","text":"<p>In this tutorial we will show you how to integrate chatgpt Open.ai into a user defined function in Snowflake, so it can be used anywhere in Snowflake.</p>"},{"location":"engineering/functions/chatgpt/#video","title":"Video","text":""},{"location":"engineering/functions/chatgpt/#requirement","title":"Requirement","text":"<p>This tutorial assume you have an account with openai.com. You will need to get an api token from it.</p>"},{"location":"engineering/functions/chatgpt/#setup","title":"Setup","text":"<p>In this section we will do the setup to support our user-defined function by:</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists \n    api comment='This is only api data from our sources.';\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists api.functions;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database api;\nuse schema functions;\nuse warehouse developer;\n</code></pre>"},{"location":"engineering/functions/chatgpt/#external-access","title":"External Access","text":"<p>Lets setup the external access so our Snowflake can talk with Open.ai.</p>  Setup Result <p></p><pre><code>use role accountadmin;\n\ncreate or replace network rule chatgpt_network_rule\n    mode = egress\n    type = host_port\n    value_list = ('api.openai.com');\n\ncreate or replace secret chatgpt_api_key\n    type = generic_string\n    secret_string='OPENAI API KEY HERE';  /* (1)! */\n\ncreate or replace external access integration openai_integration\n    allowed_network_rules = (chatgpt_network_rule)\n    allowed_authentication_secrets = (chatgpt_api_key)\n    enabled=true;\n</code></pre><p></p> <ol> <li> <p>Go to openai.com and log in.</p> <p></p> <p>Click on Api Keys. </p> <p>Click create new secret key and then copy it into your code. </p> </li> </ol> <pre><code>Integration OPENAI_INTEGRATION successfully created.\n</code></pre>"},{"location":"engineering/functions/chatgpt/#sql-function","title":"SQL Function","text":"<p>Let's create the python user defined function to make it easy to access later.</p>  Function <pre><code>create or replace function api.functions.chatgpt(\"question\" varchar)\n    returns varchar\n    language python\n    runtime_version = '3.8'\n    packages = ('requests','openai')\n    handler = 'chatgpt'\n    external_access_integrations = (openai_integration)\n    secrets = ('cred'=chatgpt_api_key)\nAS '\nimport _snowflake\n\nfrom openai import OpenAI\n\ndef chatgpt(question):\n    openai_api_key = _snowflake.get_generic_secret_string(''cred'')\n\n    client = OpenAI(api_key=openai_api_key)\n    completion = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": question}]\n    )    \n    return completion.choices[0].message.content\n';\n</code></pre>"},{"location":"engineering/functions/chatgpt/#use","title":"Use","text":"<p>Lets use the function. Chatgpt's comments are so nice.</p>  Use Result <pre><code>select api.functions.chatgpt('Can you tell me how amazing daniels tutorials are?'); \n</code></pre> <p>Yes, Daniels tutorials are incredibly informative, easy to follow, and thorough. He breaks down complex concepts into simple, easy-to-understand steps and provides clear explanations and examples. Whether you are a beginner or an advanced learner, Daniels tutorials are a valuable resource for learning and mastering new skills. Overall, they are truly amazing and highly recommended.</p>"},{"location":"engineering/orchestration/task/","title":"Lineage","text":""},{"location":"engineering/orchestration/task/#task-lineage","title":"Task Lineage","text":"<p>In this tutorial we will show how to child and partent tasks so that once the parent run the child will follow. We call this task lineage.</p> <p>For the official Snowflake documentation this tutorial was based on: https://docs.snowflake.com/en/user-guide/tasks-graphs#create-a-task-graph</p>"},{"location":"engineering/orchestration/task/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"engineering/orchestration/task/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p>"},{"location":"engineering/orchestration/task/#setup","title":"Setup","text":"<p>Lets start the setup prcoess in Snowflake. </p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema raw.data;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema data;\nuse warehouse developer;\n</code></pre> <p>First lets start by setting up the table we'll enter data into.</p>  Setup Result <pre><code>create or replace table your_table (\n    column1 string,\n    column2 string\n);\n\ncreate or replace task parent_task\n    warehouse = 'developer'\n    schedule = 'USING CRON 0 0 * * * UTC' -- schedule to run daily at midnight\nas\n    insert into raw.data.your_table (column1, column2)\n    values ('task', 'task');\n</code></pre> <pre><code>Task PARENT_TASK successfully created.\n</code></pre>"},{"location":"engineering/orchestration/task/#parent-task","title":"Parent Task","text":"<p>Next lets create a Snowflake notebook named \"my_notebook\" and enter the sql below. </p>  Notebook <pre><code>insert into raw.data.your_table (column1, column2)\n    values ('notebook', 'notebook')\n</code></pre>"},{"location":"engineering/orchestration/task/#child-task","title":"Child Task","text":"<p>Lets create the child task and resume the task so it runs after our parent task.</p>  Setup Result <pre><code>create or replace task child_task\n    warehouse = 'developer'\n    after parent_task\nas\n    execute notebook raw.data.my_notebook();\n\nalter task child_task resume;\n</code></pre> <pre><code>Statement executed successfully.\n</code></pre> <p>You can also see the task linage by going to the parent task in the UI. </p>"},{"location":"engineering/orchestration/task/#run-task","title":"Run Task","text":"<p>Lets run the notebook and see that once the task row is added, the notebook row follows.</p>  Execute Result <pre><code>-- Run the task tree.\nexecute task parent_task;\n\n-- Look at the results.\nselect * from raw.aws.your_table;\n</code></pre> # Column Column 1 task task 2 notebook notebook"},{"location":"engineering/parse/unstructured/","title":"Document AI","text":""},{"location":"engineering/parse/unstructured/#document-ai-introduction","title":"Document AI - Introduction","text":"<p>In this tutorial we will show how setup, train and use document AI in Snowflake.</p>"},{"location":"engineering/parse/unstructured/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"engineering/parse/unstructured/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p>"},{"location":"engineering/parse/unstructured/#downloads","title":"Downloads","text":"<ul> <li>Training data</li> <li>Testing data</li> </ul> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.documents;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    initially_suspended = true;\n</code></pre>"},{"location":"engineering/parse/unstructured/#snowflake","title":"Snowflake","text":"<p>Lets start in the AI &amp; ML under Document AI and we'll build our first model.  </p>"},{"location":"engineering/parse/unstructured/#training","title":"Training","text":"<p>Lets give the model a name and location to be stored. Once done, click \"create\".  </p> <p>Now that it's created lets upload our training resumes. Click upload documents.  </p> <p>Browse and upload the training pdf files.  </p> <p>Once all have been uploaded click done.  </p> <p>Now they are uploaded to the dataset lets define our question / values we want to pull.  </p> <p>Click \"+ value\" to start asking questions.  </p> <p>On the left will be it's title/key and right will be the question we want to ask to retrieve from the resumes. Once you click enter on it, it will pull the value and give you an accuracy number.  </p> <p>I asked it a four questions and one that pulls multiple responses/items. When I'm happy with the responses I can click the check box or correct the model. </p> <p>After we accept all and review next we'll get a new example which we'll want to validate or correct. We'll follow this process until we have reviewed all training resumes.  </p> <p>Once all resumes are reviewed we'll be able to see the status of all training resumes.  </p> <p>If we are not happy with the accuracy we can train the model to make our accuracy better.  </p> <p>Once we are happy with the accuracy we can publish the model so it can be used on our testing resumes.  </p> <p>Click publish.  </p>"},{"location":"engineering/parse/unstructured/#parsing-new-documents","title":"Parsing new documents","text":"<p>Now we'll notice that two examples are provided for parsing new documents using our published model. We can copy either the folder or single files example, we will use this later.  </p>"},{"location":"engineering/parse/unstructured/#upload-testing-data","title":"Upload Testing Data","text":"<p>Lets create a new stage in our schema for our testing pdf documents.  </p> <p>Lets call it resumes, encrypt it using \"Server-side encreption\", and click create.  </p> <p>Lets upload our testing resumes.  </p> <p>Browse, upload all three pdf's, and click \"upload\".  </p>"},{"location":"engineering/parse/unstructured/#parse-new-documents","title":"Parse new documents","text":"<p>Once upload lets open a worksheet and run one of our two example codes pointed at our stage.</p>  Single Directory <pre><code>use schema raw.documents;\n\nselect resumes!predict(get_presigned_url(@resumes,'Ex11.pdf', 1);\n</code></pre> <pre><code>use schema raw.documents;\n\nselect \n    resumes!predict(get_presigned_url(@resumes,relative_path, 1)\nfrom\n    directory(@resumes);\n</code></pre> <p>Now we can see our JSON response, if you do an entire directory you can also put it in a CTE and flatten it after be parsed. </p>"},{"location":"engineering/rbac/masking/","title":"Masking","text":""},{"location":"engineering/rbac/masking/#rbac-data-masking","title":"RBAC - Data Masking","text":"<p>Goal of this tutorial is to show examples of both column level masking and row level masking in Snowflake.</p>"},{"location":"engineering/rbac/masking/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"engineering/rbac/masking/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Snowflake account needs to Enterprise Edition or higher.</li> </ul>"},{"location":"engineering/rbac/masking/#sample-data","title":"Sample data","text":"<p>Please update the \"YOUR_USER_EMAIL\" section for row level masking to work.</p>  Code Result <pre><code>-- Create and use a dedicated schema\ncreate or replace schema security_demo;\nuse schema security_demo;\n\n-- 1. Create the unified table\ncreate or replace table sales_performance (\norder_id number,\nemployee_name string,\nemployee_email string, -- Column to be masked\nsales_region string,    -- Column for row filtering\nsale_amount float\n);\n\n-- 2. Create the mapping table for Row Access\ncreate or replace table user_region_map (\nuser_email string, \nallowed_region string\n);\n\n-- 3. Insert sample data\ninsert into sales_performance (order_id, employee_name, employee_email, sales_region, sale_amount) values\n(1001, 'Jane Doe', 'jane.doe@example.com', 'West', 1500.00),\n(1002, 'Mike Smith', 'mike.smith@example.com', 'East', 2200.50),\n(1003, 'Sara Lee', 'sara.lee@example.com', 'Central', 950.00),\n(1004, 'Admin User', 'admin@example.com', 'West', 3100.25);\n\n-- 4. Insert mapping data (Update '&lt;YOUR_USER_EMAIL&gt;' with your actual Snowflake login)\ninsert into user_region_map (user_email, allowed_region) values\n('&lt;YOUR_USER_EMAIL&gt;', 'West'), \n('mike.smith@example.com', 'East'),    \n('sara.lee@example.com', 'Central');\n</code></pre> number of rows inserted 3"},{"location":"engineering/rbac/masking/#column-level-masking","title":"Column Level Masking","text":"<p>We want only the ENGINEER or ACCOUNTADMIN role to see the full email addresses. Everyone else will see asterisks.</p>  Code Result <pre><code>-- Create the masking policy\ncreate or replace masking policy email_mask\nas (val string)\nreturns string -&gt;\ncase\n    when current_role() in ('ENGINEER', 'ACCOUNTADMIN') then val\n    else '*********'\nend;\n\n-- Apply the policy to the email column\nalter table sales_performance \nmodify column employee_email \nset masking policy email_mask;\n</code></pre> status Statement executed successfully. <p>Lets see the results. Try changing your role in the sheet by using \"use role sysadmin\".</p>  Code Result - Masked <pre><code>-- Test your results\nuse role sysadmin;\nselect * from sales_performance;\n\nuse role accountadmin;\nselect * from sales_performance;\n</code></pre> ORDER_ID EMPLOYEE_NAME EMPLOYEE_EMAIL SALES_REGION SALE_AMOUNT 1001 Jane Doe *** West 1500 1002 Mike Smith *** East 2200.5 1003 Sara Lee *** Central 950 1004 Admin User *** West 3100.25"},{"location":"engineering/rbac/masking/#row-level-masking","title":"Row Level Masking","text":"<p>We want users to only see rows belonging to their assigned region based on the user_region_map.</p>  Code <pre><code>-- Create the row access policy\ncreate or replace row access policy region_access_policy\nas (sales_region varchar) returns boolean -&gt;\n    current_role() = 'ACCOUNTADMIN'\n    or exists (\n        select 1 from user_region_map\n        where user_email = current_user()\n        and allowed_region = sales_region\n    );\n\n-- Apply the policy to the table\nalter table sales_performance \nadd row access policy region_access_policy on (sales_region);\n</code></pre> <p>h</p>  Result status Statement executed successfully. <p>Lets see the results. Try changing your role in the sheet by using \"use role sysadmin\"</p>  Code Result - Masked <pre><code>-- Test your results \nuse role sysadmin;\nselect * from sales_performance;\n\nuse role accountadmin;\nselect * from sales_performance;\n</code></pre> ORDER_ID EMPLOYEE_NAME EMPLOYEE_EMAIL SALES_REGION SALE_AMOUNT 1001 Jane Doe *** West 1500 1004 Admin User *** West 3100.25"},{"location":"engineering/rbac/roles/","title":"Roles","text":""},{"location":"engineering/rbac/roles/#rbac-database-roles-and-orginization","title":"RBAC - Database Roles and Orginization","text":"<p>Goal of this tutorial is to explain how I would setup Snowflake from a governance perspective for commercial customers. The scope of this tutorial is basic and I do not want to go too deep into any one topic but show how best to orginize everthing to start successfully.</p>"},{"location":"engineering/rbac/roles/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"engineering/rbac/roles/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"engineering/rbac/roles/#files","title":"Files","text":"<ul> <li>Files (Link)</li> </ul>"},{"location":"engineering/rbac/roles/#overview","title":"Overview","text":"<p>Most of this tutorial is an explanation and overview that is better explained in video format. Please watch the video aboe with the tutorial files provided above.</p>"},{"location":"engineering/sharing/direct/","title":"Direct","text":""},{"location":"engineering/sharing/direct/#sharing-direct","title":"Sharing - Direct","text":"<p>Warning</p> <p>Direct data sharing can only be done to accounts on the same cloud provider and in the same region as your Snowflake account. To share to other providers/regions use a data share listing.</p> <p>In this tutorial we will cover how to create sercure views on our data and then direct data share with anther Snowflake account.</p>"},{"location":"engineering/sharing/direct/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"engineering/sharing/direct/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"engineering/sharing/direct/#second-account","title":"Second Account","text":"<p>Lets start by setting up our second account in our Snowflake orginization so that we can go through the process of sharing with a real life account. To create anther account please follow this tutorial (3 minutes) but make sure it's in the same region as the first account.</p> <p>Now that we have our second account we'll need that account's locator. Lets start by going to our account details. </p> <p>Copy the account locator, we'll need this for later. </p>"},{"location":"engineering/sharing/direct/#setup","title":"Setup","text":"<p>Lets create some data and share it to our second account.</p>"},{"location":"engineering/sharing/direct/#data","title":"Data","text":"<p>Lets now create some data to be shared with our second account. Lets start a sql worksheet and add the code below.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists sharing;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists sharing.data;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database sharing;\nuse schema data;\nuse warehouse development;\n</code></pre>  Table Result <pre><code>create or replace table customers (\n    id number autoincrement primary key,\n    name string,\n    email string\n);\n\ninsert into customers (name, email)\n    values\n    ('john doe', 'john@example.com'),\n    ('jane smith', 'jane@example.com'),\n    ('carlos mendez', 'carlos@example.com');\n</code></pre> number of rows inserted 3"},{"location":"engineering/sharing/direct/#share","title":"Share","text":"<p>Note</p> <p>You must be using accountadmin or the button/dropdown won't show up. </p> <p>Now that we have our table, lets create a data share by going to private sharing and then in the top right clicking the dropdown to direct data sharing. </p> <p>Lets click select data. </p> <p>Navigate to the sharing database, data schema and finally lets select the customer data table. </p> <p>Now that we have our data selected lets give the share a name and description. The customer will see both when they add the share. Finally lets add the second accounts locator. If it's correct you will get the dropdown to select the account. </p> <p>Once everything is to our liking we can click \"create share\". </p> <p>Once the share is created you'll see it's an object we can change if needed. </p>"},{"location":"engineering/sharing/direct/#add-data-share","title":"Add data share","text":"<p>Lets now add the data share in the second account by going to private sharing and clicking the download button next to our share. </p> <p>Lets update the database name and click \"get data\". </p> <p>The data has now been added to the second account. Lets go view that data. </p> <p>We can see it has been added with the database name we wantsed and we can go to the customers table to see the data. </p>"},{"location":"engineering/sharing/direct/#bonus-sharing-filtered-data","title":"Bonus - Sharing filtered data","text":"<p>We might want to only share specific data to anther account without that account knowing how we filtered the data. To do this we will create a secure view on top of our table to filter only to rows with the name \"john doe\".</p>  View Result <pre><code>create or replace secure view filtered_customers as\n    select\n        *\n    from\n        customers\n    where\n        name = 'john doe';\n</code></pre> status View FILTERED_CUSTOMERS successfully created. <p>Now this secure view can be selected instead of our table in the data share. </p> <p>Lets edit our data share. </p> <p>Lets uncheck our original table and select our secure view. </p> <p>Click save. </p> <p>Now we can go to our second account and see that now the secure view object has been added and only the one row is now avaliable. </p>"},{"location":"engineering/sharing/listing/","title":"Listings","text":""},{"location":"engineering/sharing/listing/#sharing-listing","title":"Sharing - Listing","text":"<p>In this tutorial, we\u2019ll walk through how to share data with another Snowflake account using a listing, even if the account is on a different cloud provider than your own.</p>"},{"location":"engineering/sharing/listing/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"engineering/sharing/listing/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"engineering/sharing/listing/#second-account","title":"Second Account","text":"<p>Lets start by setting up our second account in our Snowflake orginization so that we can go through the process of sharing with a real life account. To create anther account please follow this tutorial (3 minutes). In this tutorial we will setup the second account in Azure while our primary account is in AWS.</p> <p>Now that we have our second account we'll need that account's <code>Data Sharing Account Identifier</code>. Lets start by going to our account details. </p> <p>Copy the <code>Data Sharing Account Identifier</code>, we'll need this for later. </p>"},{"location":"engineering/sharing/listing/#setup","title":"Setup","text":"<p>Lets create some data and share it to our second account.</p>"},{"location":"engineering/sharing/listing/#data","title":"Data","text":"<p>Lets now create some data to be shared with our second account. Lets start a sql worksheet and add the code below.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists sharing;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists sharing.data;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database sharing;\nuse schema data;\nuse warehouse development;\n</code></pre>  Table Result <pre><code>create or replace table customers (\n    id number autoincrement primary key,\n    name string,\n    email string\n);\n\ninsert into customers (name, email)\n    values\n    ('john doe', 'john@example.com'),\n    ('jane smith', 'jane@example.com'),\n    ('carlos mendez', 'carlos@example.com');\n</code></pre> number of rows inserted 3"},{"location":"engineering/sharing/listing/#share","title":"Share","text":"<p>Note</p> <p>You must be using accountadmin or the button/dropdown won't show up. </p> <p>Now that we have our table, lets create a data share listing by going to private sharing and then in the top right click the dropdown to \"Publish to Specified Consumer\". </p> <p>We'll give the listing a name, select \"Only specified consumers\" and click next. </p> <p>We'll start with selecting our data. </p> <p>Select our table. </p> If you get error: This product can be shared only in the local region <p>If you see this error: </p> <p>You will need to use the main orginizational account to enable the primary account your working in to share with other providers/region.</p>  Code Example Result <pre><code>CALL SYSTEM$ENABLE_GLOBAL_DATA_SHARING_FOR_ACCOUNT('&lt;account_name&gt;');\n</code></pre> <pre><code>CALL SYSTEM$ENABLE_GLOBAL_DATA_SHARING_FOR_ACCOUNT('tutorials_aws');\n</code></pre> SYSTEM$ENABLE_GLOBAL_DATA_SHARING_FOR_ACCOUNT Statement executed successfully. <p>We'll give our share object a name, description and then we'll enter the <code>Data Sharing Account Identifier</code>. </p> <p>Once we enter our <code>Data Sharing Account Identifier</code> we'll see new options show up to allow us to set the data replication refresh internal. This will be how often the dat will be refreshed. </p> <p>Now our listing is live and shared with our second account. Once it's added to the second account it will replicate the data and the second account will have access to the shared data. </p>"},{"location":"engineering/sharing/listing/#add-data-share","title":"Add data share","text":"<p>Now that we have our data shared, lets add it to the second account. Lets go into our Azure account and add the data. You must use the accountadmin role. </p> <p>Note</p> <p>You might have to validate your email before you can add the data share. Also keep in mind your must use the <code>accountadmin</code> role to add the data share.</p> <p>Now that you have added it, it will start to replicate the data from AWS to Azure, this may take a fiew minutes depending on the volume of data. You'll recieve an email once the data is avaliable. </p> <p>Now that the data is avaliable lets add it again. We'll give it a name and add it to the account. </p> <p>Your data is now added. </p> <p>We can see the new shared database in our accont with our table. </p>"},{"location":"engineering/sharing/listing/#bonus-sharing-filtered-data","title":"Bonus - Sharing filtered data","text":"<p>We might want to only share specific data to anther account without that account knowing how we filtered the data. To do this we will create a secure view on top of our table to filter only to rows with the name \"john doe\".</p>  View Result <pre><code>create or replace secure view filtered_customers as\n    select\n        *\n    from\n        customers\n    where\n        name = 'john doe';\n</code></pre> status View FILTERED_CUSTOMERS successfully created. <p>Lets edit our data share. </p> <p>Lets uncheck our original table and select our secure view. </p> <p>Click save. </p> <p>Now we can go to our second account and see that now the secure view object has been added and only the one row is now avaliable. </p>"},{"location":"engineering/sharing/marketplace/","title":"Index","text":"<p>In this tutorial we will show how to share your data to the marketplace.</p>"},{"location":"engineering/sharing/reader/","title":"Index","text":"<p>In this tutorial we will show how to create and manage a reader account and then share data with that account.</p>"},{"location":"engineering/transformations/dbt/","title":"Transformations - DBT in Snowflake","text":""},{"location":"engineering/transformations/dbt/#transformations-dbt-in-snowflake","title":"Transformations - DBT in Snowflake","text":"<p>Goal of this tutorial is to show how you can setup development and production dbt in Snowflake.</p>"},{"location":"engineering/transformations/dbt/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"engineering/transformations/dbt/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"engineering/transformations/dbt/#workspace","title":"Workspace","text":"<p>To start using dbt in a share development space we will need a shared workspace to allow others to see our code.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>-- Create a database to store our schemas.\ncreate database if not exists demo;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists demo.code;\ncreate schema if not exists demo.production;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\ncreate warehouse if not exists production \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre>"},{"location":"ingestion/connectors/api/","title":"API","text":""},{"location":"ingestion/connectors/api/#openflow-api","title":"Openflow - API","text":"<p>Goal of this tutorial is to load data from an API into Snowflake table via openflow.</p>"},{"location":"ingestion/connectors/api/#video","title":"Video","text":""},{"location":"ingestion/connectors/api/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> </ul>"},{"location":"ingestion/connectors/api/#download","title":"Download","text":"<ul> <li>Connector (Link)</li> </ul>"},{"location":"ingestion/connectors/api/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a workspace worksheet (1) and creating the nesseray objects for openflow and the connector.</p> If you don't have a database, schema, or warehouse yet.  Database, schema and warehouse <pre><code>-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.api;\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with our API.</p>  Code Result <pre><code>-- Create network rule for the api\ncreate or replace network rule api_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'api.openweathermap.org'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (api_network_rule)\n    enabled = true;\n</code></pre> <p>Integration OPENFLOW_EXTERNAL_ACCESS successfully created.</p> <p>Now we will need a table to store the contents of our API call.</p>  Code Result <pre><code>create or replace table weather (\n    raw variant\n);\n</code></pre> <p>Table WEATHER successfully created.</p>"},{"location":"ingestion/connectors/api/#openflow","title":"Openflow","text":"<p>Next we'll head into openflow to setup our runtime and add the connector. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Click \"Launch openflow\". </p>"},{"location":"ingestion/connectors/api/#add-the-connector","title":"Add the connector","text":"<p>We'll create a new runtime. </p> <p>We'll select our deployment, give the runtime a name, select our snowflake role and if deployed in Snowflake our external access intergration. </p> <p>Now we'll wait 5-10 minutes for our runtime to become usable. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Once the runtime is \"Active\" we can click to go into it. </p> <p>Next we'll drag a process group to the canvas. </p> <p>We'll click \"Browse\" button and upload our connector we downloaded at the start of the tutorial. </p> <p>Click \"Add\". </p>"},{"location":"ingestion/connectors/api/#paramaters","title":"Paramaters","text":"<p>Next we'll want to go into our connectors paramaters. Right click the connector and go into paramaters. </p> <p>Click cancel. </p>"},{"location":"ingestion/connectors/api/#api-credentials","title":"API Credentials","text":"<p>From here we can see the two or three sections we need. We'll start by clicking edit on the api paramaters. </p> <p>Here we can see our only paramater. This is to keep the tutorial simple. I have already add the weather data api and it's key. </p>"},{"location":"ingestion/connectors/api/#snowflake-credentials","title":"Snowflake Credentials","text":"<p>Next we'll go back and edit our Snowflake configurations. </p> <p>Now you only have to do the third paramater if your openflow is hosted on AWS. </p>"},{"location":"ingestion/connectors/api/#optional-snowflake-for-aws","title":"(Optional) Snowflake for AWS","text":"<p>We'll want to enable our control service so that we can connect to Snowflake. </p>"},{"location":"ingestion/connectors/api/#running-the-connector","title":"Running the connector","text":"<p>To look into our connector we can double click it. </p> <p>To see how we plan on hitting our endpoint we can right click and select configure. </p> <p>Here we can see the configuration for how to hit the API and return a response. In this case we keep it simple and just put in the URL. </p> <p>If we head back we can run the connector once to validate it works. </p> <p>Now that it has been sucessfull we can list the queue of responses by right clicking and go into the queue. </p> <p>Now we can view the contents of our api call. </p> <p>We can see the json that has responded, we'll copy this so we can manipulate it in the next step. </p> <p>We'll look into the configuration of the transformation. </p> <p>Here we can see in the transformation the code used to manipulate the JSON so it fits into our single column varient column. </p> <p>Now if we want a better view of the transformation or to play with it we can right click the transform processor and go into advanced. </p> <p>Now we get a nice UI to paste our JSON into and click transform. This will allow us to see how the data will change after this step. </p> <p>Lets head back and run it once. </p> <p>We can right click and look at the queue. </p> <p>View the contents </p> <p>Here we can see we add the RAW header key to the data to make sure it goes into our RAW column in our weather table. </p> <p>Now lets head back and run the final step once to get it into the table. </p> <p>We can see it's been successful so we will go look at our table in Snowflake. </p> <p>Now we can select our table and click the column to view the results in a nice way.</p>  Code <pre><code>select * from weather;\n</code></pre> <p></p>"},{"location":"ingestion/connectors/box/","title":"Box","text":""},{"location":"ingestion/connectors/box/#openflow-box","title":"Openflow - Box","text":"<p>Goal of this tutorial is to load data from Box into Snowflake via openflow. This tutorial will not cover how to setup a deployment in Snowflake or AWS.</p>"},{"location":"ingestion/connectors/box/#video","title":"Video","text":"<p>Still in development</p>"},{"location":"ingestion/connectors/box/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> <li>Box account must be at minimum a busniess account. This is a requirements from Box. </li> </ul>"},{"location":"ingestion/connectors/box/#downloads","title":"Downloads","text":"<ul> <li>Sample data - Link</li> </ul>"},{"location":"ingestion/connectors/box/#box","title":"Box","text":"<p>This tutorial will use a Box Sandbox to prevent production security complications. </p>"},{"location":"ingestion/connectors/box/#sanbox-creation","title":"Sanbox Creation","text":"<p>Lets login to our box account and go to \"Admin Console\". </p> <p>Now we'll go to platform, select sandbox and click \"Create Sandbox\". This will allow us to have full access to an account. </p> <p>We'll give our sandbox a name and select our email as the admin. Click \"Create\". </p> <p>Now that the sandbox is created we will login to it. </p>"},{"location":"ingestion/connectors/box/#box-platform-app","title":"Box Platform App","text":"<p>Once we are in the accoount or sandbox account, we'll go to https://app.box.com/developers/console </p> <p>We'll need to create an app so that our Openflow can connected to the account via the app. Click \"Create Platform App\". </p> <p>We'll give our app a name and select \"Server Auth -JWT\" as app type. </p>"},{"location":"ingestion/connectors/box/#app-configuration","title":"App Configuration","text":"<p>Once created we'll be launched into Configurations were we'll need to make changes. To start lets update \"App Access Level\" to App + Enterprise Access. </p> <p>Make sure your application scope follows Snowflake requirements.  </p> <p>Next we'll generate a Public / Private Key. Click this button will require you to enter your duo auth code or sign up for one. </p> <p>Enter duo auth information. </p> <p>Once duo auth you'll have to click the button again. </p> <p>Once click it will download a .json file. We will use this file in openflow later. </p> <p>Now lets submit our app for authorization so that our box can use it later. </p> <p>Copy your Client ID and submit it to your enterprise, this removes and uneeded steep later. </p> <p>Only needed when using a sandbox</p> <p>Next lets head to general setting and copy our Service Account ID. This is only if you are using a Sandbox as the description explains. </p> <p>Head back to your account. </p> <p>Go to \"Admin Console\". </p> <p>Navigate to integration select \"Server Authentication Apps\" and view your application. </p> <p>Click \"Authorize\". </p> <p>Click \"Authorize\" again. </p> <p>Now your app and account is ready. Lets go add data and share it to our service account. </p>"},{"location":"ingestion/connectors/box/#sample-data-share","title":"Sample Data / Share","text":"<p>Now lets upload our sample PDF files and share it with our service account because were in a sandbox. Create a new folder. </p> <p>Give the folder a name and paste the service account email in the \"Invite Additional People\" box. Click create. </p> <p>Open your folder. </p> <p>Upload the example PDF files. </p> <p>Next copy your \"Folder ID\" from the URL. This will be used in Openflow later. </p>"},{"location":"ingestion/connectors/box/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a workspace sql file (1) and creating the necessary objects for openflow and the connector.</p> If you don't have a database, schema, or warehouse yet.  Database, schema and warehouse <pre><code>-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.box;\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with our SFTP.</p>  Code Result <pre><code>-- Create network rule for box endpoints\ncreate or replace network rule box_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'api.box.com',\n        'boxcdn.net',\n        'boxcloud.com',\n        'dl.boxcloud.com',\n        'public.boxcloud.com'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (box_network_rule)\n    enabled = true;\n</code></pre> <p>Integration OPENFLOW_EXTERNAL_ACCESS successfully created.</p>"},{"location":"ingestion/connectors/box/#openflow","title":"Openflow","text":"<p>Now that we have our objects lets add the postgres connector to our deployment. Navigate to openflow in the navbar. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Launch openflow and login. </p> <p>From there we can switch to the deployment where we can see our deployment and that it's active. If you don't have a deployment use either SPCS or AWS to deploy your Opeflow instance. </p> <p>Next we'll head to runtime and click \" + Create Runtime\". </p> <p>Note</p> <p>External access will only show up if your on a SPCS deployment.</p> <p>We'll then select our runtime, give it a name, select accountadmin as the role and if your on SPCS your external access integration. </p> <p>Once your runtime is active and ready to go. We can head to overview and add the connector. </p> <p>Click install on the \"Box (Cortex Connect)\" version to have Snowflake search and all needed tables setup by the connector. There are other connectors for box that are very similar but used in different way. </p> <p>Select our runtime. </p>"},{"location":"ingestion/connectors/box/#paramaters","title":"Paramaters","text":"<p>Now that the connector is installed we'll want to input our Box and Snowflake paramaters. </p> <p>Now that we are in paramaters we can break it into 3 sections those being destination, Ingestion and Source.</p>"},{"location":"ingestion/connectors/box/#destination","title":"Destination","text":"<p>Lets click the three dots of our destination and click edit. </p> <p>Now we can select either note option below based on our deployment.</p> If your using SPCS deployment <p>As an example we'll click the three dots and click edit. We'll put the database, schema, role and warehouse. </p> <p>One special paramter is the \"Snowflake Authentication Strategy\" with container service you can put in \"SNOWFLAKE_SESSION_TOKEN\" and it will not require a key_pair. </p> <p>This is an example input if you used the configurations given at the beginning of the tutorial. </p> If your using AWS deployemnt <p>These are the paramaters you'll need to be filled out. We will see how to get them below. </p> <p>To get the Snowflake Account Identifier, you'll go to the bottom left of the homepage and click account details. </p> <p>You'll copy your account identifier and paste it in openflow. </p> <p>Next to get your key we'll have to generate a public and private key and apply it to our user. To generate the key run this bash script.</p>  Code Result <pre><code>openssl genrsa 2048 &gt; rsa_key_pkcs1.pem\nopenssl pkcs8 -topk8 -inform PEM -in rsa_key_pkcs1.pem -out rsa_key.p8 -nocrypt\nopenssl rsa -in rsa_key_pkcs1.pem -pubout -out rsa_key.pub\n</code></pre> <pre><code>rsa_key_pkcs1.pem\nrsa_key.p8\nrsa_key.pub\n</code></pre> <p>This will generate three file. We will apply the content of the .pub file to our Snowflake user using the alter user command.</p>  Code Example Result <pre><code>alter user danielwilczak set rsa_public_key='PUBLIC KEY';\n</code></pre> <pre><code>alter user danielwilczak set rsa_public_key='MIIBIjANBgkqhki...6VIhVnTviwDXXcm558uMrJQIDAQAB';\n</code></pre> <p>Statement executed successfully.</p> <p>Now that we have our public key on our user we'll move to uploading the .pem file to openflow. </p> <p>Click the upload button. </p> <p>Click the upload button again and select your .pem file. </p> <p>Once uploaded select the key and click \"ok\". Then fill out the remaing fields and click apply. </p>"},{"location":"ingestion/connectors/box/#source","title":"Source","text":"<p>Next lets head to source. Click edit on the \"Run App Config FIle\". </p> <p>Select \"Reference assists\" and then the upload button. </p> <p>Click upload again. </p> <p>Upload your json public/private key we go from box earlier (1).</p> <p>Once uploaded, we'll click \"Close\". </p> <p>We'll select our key and click \"ok\". </p> <p>Finally click \"apply\". </p>"},{"location":"ingestion/connectors/box/#ingestion","title":"Ingestion","text":"<p>Now we'll move to the final section ingestion. We'll first edit our folder ID, the reason we don't use 0 in this tutorial because Sandbox's don't allow it. Click edit. </p> <p>We'll put in our \"Folder ID\" we got from Box (1).</p> <p></p> <p>Next we'll edit the role that will have access to the search service.  </p> <p>Since we do everthing in this tutorial with AccountAdmin we'll keep using it. </p> <p>Click \"Apply\". </p>"},{"location":"ingestion/connectors/box/#run","title":"Run","text":"<p>Finally we can run our connector. Head back to the process group. </p> <p>Right click the process group again and click \"Enable all controller services\" and click start. </p> <p>Next we'll right click \"Enable\" the connector. </p> <p>Note</p> <p>If you get an error here, I would let the connector run one more time, sometimes the connector has race connedition problems where it doesn't know objects have been created yet and fails. It will fix itself on the next run.</p> <p>Then we'll right click and hit \"Start\". This will kick off the ingestion.  </p>"},{"location":"ingestion/connectors/box/#stage","title":"Stage","text":"<p>Now if you don't get any errors you can go back to Snowflake and find our box schema with everything int it. Letts start with seeing that the files have been replicated. </p>"},{"location":"ingestion/connectors/box/#cortex-search","title":"Cortex Search","text":"<p>Now that we have the files loaded and in Cortex Search automaticly lets use the playground to search through the files. </p> <p>Now we can search in the playground to get relevent chunks of information. </p>"},{"location":"ingestion/connectors/box/#snowflake-inetllegence","title":"Snowflake Inetllegence","text":"<p>Now lets allow Snowflake Inetllegence to use our cortex search service to use it during it's questions answering. Lets start by going to agents. </p> <p>Next put our agents into the same schema and give it a object name and diplay name. Click \"Create Agent\". </p> <p>Next in our agent we'll go tools and a Cortex Search. </p> <p>Select the location of the search service, select the service. For ID column selet web_url and title column will be full_name. Finally we'll give the tool a name and description and click \"Add\". </p> <p>Click \"Save\". </p> <p>Lets navigate to Snowflake Inetllegence. </p> <p>We'll ask a question similar to \"How much of a fee do we charge for International cellular plans in africa?\". </p> <p>We can see that it was able to answer the question with our documents. Also we can click the document to see it in box. </p> <p>We can see that it pulled it from our international plan section. </p>"},{"location":"ingestion/connectors/google_sheets/","title":"Google Sheet","text":""},{"location":"ingestion/connectors/google_sheets/#openflow-google-sheets","title":"Openflow - Google Sheets","text":"<p>Goal of this tutorial is to load data from a google sheet into Snowflake via openflow.</p>"},{"location":"ingestion/connectors/google_sheets/#video","title":"Video","text":"<p>Video still in development</p>"},{"location":"ingestion/connectors/google_sheets/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> <li>Google cloud account, you can setup a free account to get started.</li> </ul>"},{"location":"ingestion/connectors/google_sheets/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a worksheet (1) and creating the nesseray objects for openflow and the connector.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.google;\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with google sheets.</p>  Example Result <pre><code>-- create network rule for google apis\ncreate or replace network rule google_api_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'admin.googleapis.com',\n        'oauth2.googleapis.com',\n        'www.googleapis.com',\n        'sheets.googleapis.com'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (google_api_network_rule)\n    enabled = true;\n</code></pre> <p>Statement executed successfully.</p>"},{"location":"ingestion/connectors/google_sheets/#openflow","title":"Openflow","text":"<p>In this tutorial we assume you already have a deployment. If you have not it only takes a few minutes. Please follow one of the two options Snowflake or AWS.</p>"},{"location":"ingestion/connectors/google_sheets/#runtime","title":"Runtime","text":"<p>Inside openflow click runtimes. Click \"+ Create runtime\". </p> <p>Select your deployment, next give your runtime a name, and select accountadmin as the role. Scroll down to extrnal acess. </p> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>select the external access we create in our worksheet and finally click \"Create\". </p> <p>Now your runtime will start being created. Lets head to the connectors </p>"},{"location":"ingestion/connectors/google_sheets/#install-connector","title":"Install Connector","text":"<p>On the connectors list, install the google sheets connector. </p> <p>Select your runtime to install, you may have to wait for the runtime to finilize. </p> <p>Once selected click \"add\". The connector will be added and you'll be required to login to get into the underliying container. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Click allow. </p> <p>Now you should see the openflow canvas with the google sheets connector block. We will switch to Google sheets here to get some information to setup our connector. </p>"},{"location":"ingestion/connectors/google_sheets/#google-sheets","title":"Google Sheets","text":"<p>Note</p> <p>To setup google make sure you have Super Admin permissions, we will need this to create a service account. </p> <p>Ensure that you are in the project associated with your organization, not the project in your organization. </p>"},{"location":"ingestion/connectors/google_sheets/#orginization-policy","title":"Orginization Policy","text":"<p>Search \"Orginization Policies\", next click in the fliter box and search \"Disable service account key creation\". Select the managed constraint. </p> <p>Click Mange policy. </p> <p>Click \"Override parent policy\", edit rule and select off for enforcement. Next click done and set policy.  </p> <p>Set the policy. </p>"},{"location":"ingestion/connectors/google_sheets/#service-account","title":"Service account","text":"<p>Search \"Service account\" and once on the page click \"+ Create service account\". </p> <p>Give your service account a name and description and click done. </p> <p>Now copy your email, we will use this later. Once copied click the service account. </p> <p>Go to keys, add key and create a new key. </p> <p>We'll want the JSON key. This key will be added to openflow later. Click Create.  </p> <p>It will download a JSON file. </p>"},{"location":"ingestion/connectors/google_sheets/#enable-sheets-api","title":"Enable Sheets API","text":"<p>Next we'll want to enable the API so that Snowflake can talk with the API. Search \"google sheets api\". </p> <p>Once on the API page, click enable. </p> <p>Once enabled you will be able to see usage metrics. </p>"},{"location":"ingestion/connectors/google_sheets/#share-with-service-account","title":"Share with service account","text":"<p>Next we'll share the google sheet with the service account email (1) we copied earlier. Click the share button.</p> <p></p> <p>Insert the email and click send. </p> <p>Click share anyway. </p>"},{"location":"ingestion/connectors/google_sheets/#connector-configuration","title":"Connector Configuration","text":"<p>Lets head back to openflow and right click the google sheet connector and then parameters. </p> <p>Here we will see three sections where we will have to enter in our configiration paramaters into. </p>"},{"location":"ingestion/connectors/google_sheets/#destination-parameters","title":"Destination Parameters","text":"<p>Lets click the three dots on the right side of the destination paramters. </p> If your using SPCS deployment <p>As an example we'll click the three dots and click edit. We'll put the database, schema, role and warehouse. </p> <p>One special paramter is the \"Snowflake Authentication Strategy\" with container service you can put in \"SNOWFLAKE_SESSION_TOKEN\" and it will not require a key_pair. </p> <p>This is an example input if you used the configurations given at the beginning of the tutorial. </p> If your using AWS deployemnt <p>These are the paramaters you'll need to be filled out. We will see how to get them below. </p> <p>To get the Snowflake Account Identifier, you'll go to the bottom left of the homepage and click account details. </p> <p>You'll copy your account identifier and paste it in openflow. </p> <p>Next to get your key we'll have to generate a public and private key and apply it to our user. To generate the key run this bash script.</p>  Code Result <pre><code>openssl genrsa 2048 &gt; rsa_key_pkcs1.pem\nopenssl pkcs8 -topk8 -inform PEM -in rsa_key_pkcs1.pem -out rsa_key.p8 -nocrypt\nopenssl rsa -in rsa_key_pkcs1.pem -pubout -out rsa_key.pub\n</code></pre> <pre><code>rsa_key_pkcs1.pem\nrsa_key.p8\nrsa_key.pub\n</code></pre> <p>This will generate three file. We will apply the content of the .pub file to our Snowflake user using the alter user command.</p>  Code Example Result <pre><code>alter user danielwilczak set rsa_public_key='PUBLIC KEY';\n</code></pre> <pre><code>alter user danielwilczak set rsa_public_key='MIIBIjANBgkqhki...6VIhVnTviwDXXcm558uMrJQIDAQAB';\n</code></pre> <p>Statement executed successfully.</p> <p>Now that we have our public key on our user we'll move to uploading the .pem file to openflow. </p> <p>Click the upload button. </p> <p>Click the upload button again and select your .pem file. </p> <p>Once uploaded select the key and click \"ok\". </p>"},{"location":"ingestion/connectors/google_sheets/#ingestion-parameters","title":"Ingestion Parameters","text":"<p>Next for the \"Ingestion Parameters\" we'll need the sheet name and the sheet range. You'll select the range you want and copy it and the sheet name.  </p> <p>Next we'll want to copy the Spreadsheet ID from the URL. </p> <p>Now we'll enter in what we want the table name to be. The sheet name and range (1) in the format \"Sheet_name!range\" and finally the speadsheet ID (2) we got from the URL. Click Apply.</p> <ol> <li> <p></p> </li> <li> <p></p> </li> </ol> <p></p>"},{"location":"ingestion/connectors/google_sheets/#source-parameters","title":"Source Parameters","text":"<p>Next we'll want to copy the contents of our service account JSON file. I opened my file by dragging it to the browser. </p> <p>We'll enter that in to our service account json form. Once entered it will show as \"Sensitive value set\". </p> <p>Now we're done with the paramaters. We can go back to the process group to get it started. </p>"},{"location":"ingestion/connectors/google_sheets/#run-the-connector","title":"Run the connector","text":"<p>Lets right click the connector, enable \"All controller services\" and then start the connector. Now since the data is small it will load into Snowflake in seconds. </p> <p>And we're done, once loaded you will be able to see it in your database/schema with the table name we set prior. </p>"},{"location":"ingestion/connectors/google_sheets/#scenarios","title":"Scenarios","text":"<p>Here we will cover some what if scenarios with the data.</p>"},{"location":"ingestion/connectors/google_sheets/#multiple-spreadsheets-sheets","title":"Multiple Spreadsheets / Sheets","text":"<p>Most users will want to ingest multiple spreedsheets with potentially multiple sheets in a spreadsheet. To accomplish this you'll want to go back and add anther Google sheet connector to the runtime. </p> <p>You'll want to right click your new connector. Click configure. </p> <p>This is where you can rename both to give them unique names to differentiate them. </p> <p>After we've named them we'll want to go into our new connector and go to paramaters. </p> <p>We'll select \"Inheritance\" in the top menu and select the destination and google source parameters because these are going to be the same for all our google sheets connectors. </p> <p>Next we'll go back to our \"parameters\" and add our next google spreadsheet which has two sheets so we'll add both names with both ranges by seperating them with a comma. </p> <p>Finally we'll start the second connector by right click start and then we'll be able to see the result in our Snowflake table. </p>"},{"location":"ingestion/connectors/google_sheets/#empty-cells","title":"Empty Cells","text":"<p>In this scenario the question is what happen when cells are empty but you include them in your range. </p> <p>If you add the cells and they are blank it will not show up in the google table. </p> <p>Now we'll add values to the cells and run the pipeline again. </p> <p>You'll see the pipeline refreshes the whole table and includes the values that are not included. </p>"},{"location":"ingestion/connectors/google_sheets/#schema-change","title":"Schema Change","text":"<p>In this scenario we'll see what will happen when we have schema changes. </p> <p>When we change the cell range to include the cells the empty C column it will not be included. </p> <p>Here we we fill in the C column but keep the bottom row empty. </p> <p>We see the column get added and the cell will be set to null. </p>"},{"location":"ingestion/connectors/postgres/","title":"Postgres","text":""},{"location":"ingestion/connectors/postgres/#openflow-rds-postgres","title":"Openflow - RDS Postgres","text":"<p>Goal of this tutorial is to load data from a RDS Postgres into Snowflake via openflow. This tutorial will not cover how to setup a deployment in Snowflake or AWS.</p>"},{"location":"ingestion/connectors/postgres/#video","title":"Video","text":""},{"location":"ingestion/connectors/postgres/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> <li>AWS account, you can setup a free account to get started.</li> </ul>"},{"location":"ingestion/connectors/postgres/#aws","title":"AWS","text":"If you don't have a postgres RDS database, follow here. <p>Search for RDS and select. </p> <p>Click \"DB instances\". </p> <p>Click \"Create database\". </p> <p>We'll select postgres for the database, select a version \"I tested with 17\", and free tier if your just learning. </p> <p>Select single instance since this is for learning, give your database a name, username and password. </p> <p>If your using SPCS to deploy your openflow, this will need to be public so that Snowflake can reach across the internet to connect to the database. </p> <p>Click create database. </p> <p>Next we'll select our database to change it's network configuration. </p> <p>Click it's security group. </p> <p>select the security group by clicking the check box and then going to inbound traffic and edit inbound rules. </p> <p>Now we'll allow all traffic to access the postgres instance. We do this because pgadmin will have to connect and if your using SPCS deployment, Snowflake does not have static i.p's YET. </p> <p>Now we'll do the same thing as before but for outbound traffic. </p> <p>Select all traffic and click \"Save Rules\". Your done with your postgres RDS setup. </p> <p>Lets start in AWS where we'll need to copy our RDS data base URL, port and configure the database paramaters to allow for replication. Lets start by searching for RDS. </p> <p>Click \"DB instances\". </p> <p>Select your database. </p> <p>Copy your endpoint and port. </p> <p>Next click paramater group. </p> <p>Click create paramater group. </p> <p>Give your paramater group a name, description, select postgres engine type, select your postgres version, and select DB paramter group. Click Create. </p> <p>Select your paramater group. </p> <p>Click edit. </p> <p>Search \"rds.logical_replication\" and enter the value to be \"1\". Click save changes. </p> <p>Next search \"wal_sender_timeout\" set this value to \"0\". Click save changes. </p> <p>Next we'll head back to our database and click modify. </p> <p>Head down to \"Additional configurations\" and add your parameter group. </p> <p>Click continue. </p> <p>Apply immediately if you do not mind a short amount of downtime.  </p> <p>You will see the database begin to modify the configuration and then go back to active state. </p>"},{"location":"ingestion/connectors/postgres/#pgadmin","title":"PGAdmin","text":"<p>Lets stat by downloading PgAdmin. After that we'll connect add sample data and configure a \"publication\" and a \"replication slot\".</p>"},{"location":"ingestion/connectors/postgres/#example-data","title":"Example Data","text":"<p>Once installed we'll right click servers, hover over \"Register\" and click \"Server\". </p> <p>Next we'll click connection and put in our host, port, username and password. Click Save. </p> <p>Next we'll want to add our sample data, first we'll open a query window. </p> <p>Next add the code to first create the objects and then enter data into them.</p>  Schmea and Tables Insert Data <pre><code>-- 1. Create the schema\nCREATE SCHEMA IF NOT EXISTS replicate;\n\n-- 2. Create the users table\nCREATE TABLE replicate.users (\n    user_id SERIAL PRIMARY KEY,\n    username VARCHAR(50) UNIQUE NOT NULL,\n    email VARCHAR(100) UNIQUE NOT NULL,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n);\n\n-- 3. Create the products table\nCREATE TABLE replicate.products (\n    product_id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    price NUMERIC(10, 2) NOT NULL CHECK (price &gt;= 0),\n    stock_quantity INTEGER NOT NULL CHECK (stock_quantity &gt;= 0)\n);\n\n-- 4. Create the orders table\nCREATE TABLE replicate.orders (\n    order_id SERIAL PRIMARY KEY,\n    user_id INTEGER REFERENCES replicate.users(user_id) NOT NULL,\n    order_date TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n    total_amount NUMERIC(10, 2) NOT NULL CHECK (total_amount &gt;= 0)\n);\n</code></pre> <pre><code>-- 1. Insert data into the users table\nINSERT INTO replicate.users (username, email) VALUES\n('alice_k', 'alice.k@example.com'),\n('bob_s', 'bob.s@example.com'),\n('charlie_m', 'charlie.m@example.com');\n\n\n-- 2. Insert data into the products table\nINSERT INTO replicate.products (name, price, stock_quantity) VALUES\n('Laptop Pro', 1200.00, 15),\n('Wireless Mouse', 25.50, 200),\n('Mechanical Keyboard', 99.99, 50),\n('Webcam HD', 45.00, 120);\n\n\n-- 3. Insert data into the orders table\n-- Assuming user_id 1 is 'alice_k' and user_id 2 is 'bob_s'\nINSERT INTO replicate.orders (user_id, total_amount) VALUES\n(1, 1225.50), -- Order by alice_k\n(1, 99.99),   -- Order by alice_k\n(2, 45.00);   -- Order by bob_s\n</code></pre>"},{"location":"ingestion/connectors/postgres/#pub-replication-slot","title":"Pub / Replication Slot","text":"<p>Next we'll create a publication and replication slot by entering the code below, you can also edit it to be only for certian tables.</p>  Code <pre><code>-- Choose either to include all the tables in the publication or just a few.\ncreate publication openflow_pub for all tables;\n\n-- Check all tables are in the publication.\nselect * from pg_publication_tables;\n\n-- Create the replication group\nselect pg_create_logical_replication_slot('openflow_pgoutput_slot', 'pgoutput');\n</code></pre>"},{"location":"ingestion/connectors/postgres/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a worksheet (1) and creating the nesseray objects for openflow and the connector.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with google sheets.</p>  Code Example Result <pre><code>-- create network rule for google apis\ncreate or replace network rule postgres_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        '&lt;YOUR AMAZON RDS URL HERE&gt;:5432'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (postgres_network_rule)\n    enabled = true;\n</code></pre> <pre><code>-- create network rule for google apis\ncreate or replace network rule postgres_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'danielwilczak.cdqmaq86m7gc.us-west-2.rds.amazonaws.com:5432'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (postgres_network_rule)\n    enabled = true;\n</code></pre> <p>Statement executed successfully.</p> <p>Now that we have our objects lets add the postgres connector to our deployment. Navigate to openflow in the navbar. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Launch openflow and login. </p> <p>From there we can switch to the deployment where we can see our deployment and that it's active. If you don't have a deployment use either SPCS or AWS to deploy your Opeflow instance. </p> <p>Next we'll head to runtime and click \" + Create Runtime\". </p> <p>Note</p> <p>External access will only show up if your on a SPCS deployment.</p> <p>We'll then select our runtime, give it a name, select accountadmin as the role and if your on SPCS your external access integration. </p> <p>Once your runtime is active and ready to go. We can head to overview and add the connector. </p> <p>Lets search postgres and click install. </p> <p>We'll select our runtime and click add. </p> <p>Next head to runtime and select your runtime if not already in it. </p> <p>Once in we'll want to right click the process group and select paramaters. </p>"},{"location":"ingestion/connectors/postgres/#paramaters","title":"Paramaters","text":"<p>Now that we are in paramaters we can break it into 3 sections those being destination, Ingestion and Source.</p>"},{"location":"ingestion/connectors/postgres/#destination","title":"Destination","text":"<p>Lets click the three dots of our destination and click edit. </p> <p>Now we can select either note option below based on our deployment.</p> If your using SPCS deployment <p>As an example we'll click the three dots and click edit. We'll put the database, schema, role and warehouse. </p> <p>One special paramter is the \"Snowflake Authentication Strategy\" with container service you can put in \"SNOWFLAKE_SESSION_TOKEN\" and it will not require a key_pair. </p> <p>This is an example input if you used the configurations given at the beginning of the tutorial. </p> If your using AWS deployemnt <p>These are the paramaters you'll need to be filled out. We will see how to get them below. </p> <p>To get the Snowflake Account Identifier, you'll go to the bottom left of the homepage and click account details. </p> <p>You'll copy your account identifier and paste it in openflow. </p> <p>Next to get your key we'll have to generate a public and private key and apply it to our user. To generate the key run this bash script.</p>  Code Result <pre><code>openssl genrsa 2048 &gt; rsa_key_pkcs1.pem\nopenssl pkcs8 -topk8 -inform PEM -in rsa_key_pkcs1.pem -out rsa_key.p8 -nocrypt\nopenssl rsa -in rsa_key_pkcs1.pem -pubout -out rsa_key.pub\n</code></pre> <pre><code>rsa_key_pkcs1.pem\nrsa_key.p8\nrsa_key.pub\n</code></pre> <p>This will generate three file. We will apply the content of the .pub file to our Snowflake user using the alter user command.</p>  Code Example Result <pre><code>alter user danielwilczak set rsa_public_key='PUBLIC KEY';\n</code></pre> <pre><code>alter user danielwilczak set rsa_public_key='MIIBIjANBgkqhki...6VIhVnTviwDXXcm558uMrJQIDAQAB';\n</code></pre> <p>Statement executed successfully.</p> <p>Now that we have our public key on our user we'll move to uploading the .pem file to openflow. </p> <p>Click the upload button. </p> <p>Click the upload button again and select your .pem file. </p> <p>Once uploaded select the key and click \"ok\". Then fill out the remaing fields and click apply. </p>"},{"location":"ingestion/connectors/postgres/#ingestion","title":"Ingestion","text":"<p>For this section it's all about selecting what data you want and how often you want the warehouse to merge the changes into the final table. In my case I'll use the regex expression <code>replicate\\..*</code> to grab all tables from the replicate schema. It's a comma seperated list if you want multiple tables or multuple schemas. </p>"},{"location":"ingestion/connectors/postgres/#source","title":"Source","text":"<p>Next we'll head to the final paramater source. We'll first edit the connection URL. </p> <p>The format here is as shown below:</p>  Code Example <pre><code>jdbc:postgresql://&lt;DATABASE URL&gt;:5432/&lt;Database Name&gt;\n</code></pre> <pre><code>jdbc:postgresql://danielwilczak.cdqmaq86m7gc.us-west-2.rds.amazonaws.com:5432/daniel\n</code></pre> <p>Next we'll want to download the JDBC Driver for our database. I used the Java 8 version. </p> <p>We'll head back to openflow and upload our driver. </p> <p>Click upload again in the next window. </p> <p>Once uploaded you can select your driver and click \"ok\". </p> <p>Finally you'll put in your postgres password, username, publication name, and replication slot name. Click Apply. </p>"},{"location":"ingestion/connectors/postgres/#run","title":"Run","text":"<p>Finally we can run our connector. Head back to the process group. </p> <p>Right click the process group again and click \"Enable all controller services\" and click start. </p> <p>Now if you don't get any errors you can go back to Snowflake and find your shema with your tables loaded. </p>"},{"location":"ingestion/connectors/sftp/","title":"SFTP","text":""},{"location":"ingestion/connectors/sftp/#openflow-sftp","title":"Openflow - SFTP","text":"<p>Goal of this tutorial is to load data from SFTP into Snowflake stage via openflow.</p>"},{"location":"ingestion/connectors/sftp/#video","title":"Video","text":""},{"location":"ingestion/connectors/sftp/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> </ul>"},{"location":"ingestion/connectors/sftp/#download","title":"Download","text":"<ul> <li>Connector (Link)</li> </ul>"},{"location":"ingestion/connectors/sftp/#snowflake","title":"Snowflake","text":"<p>Lets start the snowflake setup by going into a workspace worksheet (1) and creating the necessary objects for openflow and the connector.</p> If you don't have a database, schema, or warehouse yet.  Database, schema and warehouse <pre><code>-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.sftp;\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with our SFTP.</p>  Code Example Result <pre><code>-- Create network rule for our SFTP.\ncreate or replace network rule sftp_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        '&lt;SFTP URL&gt;:22'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (sftp_network_rule)\n    enabled = true;\n</code></pre> <pre><code>-- create network rule for google apis\ncreate or replace network rule sftp_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'danielwilczaksftp.blob.core.windows.net:22'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (sftp_network_rule)\n    enabled = true;\n</code></pre> <p>Integration OPENFLOW_EXTERNAL_ACCESS successfully created.</p> <p>Now we will need a stage to store the files we pull from our SFTP.</p>  Code Result <pre><code>-- Stage to store files in.\ncreate stage files directory = ( enable = true );\n</code></pre> <p>Stage area FILES successfully created.</p>"},{"location":"ingestion/connectors/sftp/#openflow","title":"Openflow","text":"<p>Next we'll head into openflow to setup our runtime and add the connector. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Click \"Launch openflow\". </p>"},{"location":"ingestion/connectors/sftp/#add-the-connector","title":"Add the connector","text":"<p>We'll create a new runtime. </p> <p>We'll select our deployment, give the runtime a name, select our snowflake role and if deployed in Snowflake our external access intergration. </p> <p>Now we'll wait 5-10 minutes for our runtime to become usable. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Once the runtime is \"Active\" we can click to go into it. </p> <p>Next we'll drag a process group to the canvas. </p> <p>We'll click \"Browse\" button and upload our connector we downloaded at the start of the tutorial. </p> <p>We'll click \"Add\". </p>"},{"location":"ingestion/connectors/sftp/#paramaters","title":"Paramaters","text":"<p>Now we'll see the connector on the canvas, we'll right click and select \"paramaters\". </p> <p>We'll click cancel. </p> <p>Next we'll edit each paramater group. These are just the basics. </p>"},{"location":"ingestion/connectors/sftp/#sftp-credentials","title":"SFTP Credentials","text":"<p>We'll enter in our SFTP URL, username and upload our key. If you want to use a password you can modify this in the process I just haven't added here. I will show this in the video. </p> <p>Now for the key file you'll have to click \"reference assests\" and then the upload button. </p> <p>Click upload again. </p> <p>Once uploaded click cancel. </p> <p>Now select the file and click okay. </p> <p>Once everything has been edited, click apply. </p>"},{"location":"ingestion/connectors/sftp/#snowflake-credentials","title":"Snowflake Credentials","text":"<p>Now we will do the same for our Snowflake credentials. If your hosting Openflow in AWS make sure your authentication strategy is \"KEY_PAIR\". </p>"},{"location":"ingestion/connectors/sftp/#optional-snowflake-for-aws","title":"(Optional) Snowflake for AWS","text":"<p>Only required if your hosting openflow in AWS</p> <p></p>"},{"location":"ingestion/connectors/sftp/#running-the-connector","title":"Running the connector","text":"<p>Next we'll head back to the canvas. </p> <p>Right click the connector and add \"Enable all controller services\". </p> <p>Now lets go into the connector to look at what we can change and also run it one step at a time. Double click the connector. </p> <p>Right click the first step \"ListSFTP\", and click configure. This step will allow us to look into how the files are selected before grabbing them. In our case we are just going to pull all files in all folders. </p> <p>Once we are happy with the configurations we can go back and right click again and click \"Run Once\". This will allow us to list the files to grab in the next step. </p> <p>Now that it has successfully run, we can right click the success box and look at queue of files to grab. </p> <p>We can see all the files to pull in on the next step. </p> <p>Now we can run the next step once again.  </p> <p>This time if we look at the queue it will allow us to download the file if we wanted to validate it. </p> <p>Now we can run the final step that will put it into our stage. </p> <p>Now that we have a successful run of the final step we can go back into Snowflake UI and see our file in the stage once we click refresh. </p>"},{"location":"ingestion/deployments/aws/","title":"AWS","text":""},{"location":"ingestion/deployments/aws/#openflow-hosted-in-aws","title":"Openflow - Hosted in AWS","text":"<p>Goal of this tutorial is to setup openflow deployment in AWS with the VPC configuration manged by Snowflake. This will allow you to start adding runtimes that have connectors in them.</p>"},{"location":"ingestion/deployments/aws/#video","title":"Video","text":""},{"location":"ingestion/deployments/aws/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> </ul>"},{"location":"ingestion/deployments/aws/#opeflow","title":"Opeflow","text":"<p>Navigate to openflow in the navbar. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Launch openflow and login. </p>"},{"location":"ingestion/deployments/aws/#deployment","title":"Deployment","text":"<p>Once logged in lets click \"create deployment\". </p> <p>Click next. </p> <p>Select AWS and click next. </p> <p>This is where we will select Snowflake as the VPC deployment and click \"create deployment\". </p> <p>This will allow us to download the cloudformation template to be used in AWS next. </p>"},{"location":"ingestion/deployments/aws/#aws","title":"AWS","text":"<p>Warning</p> <p>Ensure your Snowflake region and AWS infrastructure are in the same region to avoid egress cost.</p> <p>Lets get our deployment active by getting cloudformation to generate the necessary infrustructure. </p>"},{"location":"ingestion/deployments/aws/#cloudformation","title":"CloudFormation","text":"<p>Search cloudformation and click the icon. </p> <p>Click \"create stack\". </p> <p>Select existing template, select \"upload a template file\" and then upload the template we got from openflow. If you want to view what will be deployed then click \"View in Infrastructure Composer\" (1). Click next.</p> <p></p> <p>Give the stack a name and then scroll all the way down the longest page ever to the next button. </p> <p>All the defaults are good here. Click the acklowledge and then next. </p> <p>Scroll down anther long page and click submit. </p> <p>This will kick off the creation process of all the resources. To see the process click \"Resources\". </p> <p>Here we can watch the process. </p>"},{"location":"ingestion/deployments/aws/#network-policy-optional","title":"Network Policy (Optional)","text":"If you have a blocking network policy in place, please follow these steps <p>Lets copy the IP address so that we can copy it into our external access next. </p> <p>Note</p> <p>Make sure you add the \"/32\" to the ip address.</p> <p>Lets create the network rule and external access in a worksheet(1) so that it will allow openflow/aws/snowflake to talk.</p>  Code Example Result <pre><code>create or replace network rule openflow_network_rule\n    mode = ingress\n    type = IPV4\n    value_list = (\n        '&lt;AWS IP ADDRESS&gt;/32'\n    );\n\n-- Change the existing policy.\nalter network policy &lt;your_network_policy_name&gt; allowed_network_rule_list = openflow_network_rule;\n</code></pre> <pre><code>create or replace network rule openflow_network_rule\n    mode = ingress\n    type = IPV4\n    value_list = (\n        '3.213.253.254/32'\n    );\n\n-- Change the existing policy.\nalter network policy my_network_pollicy allowed_network_rule_list = openflow_network_rule;\n</code></pre> <p>Statement executed succesfully.</p> <p>Your network policy should be updated now to allow snowflake and aws to talk with eachother.</p>"},{"location":"ingestion/deployments/aws/#progress","title":"Progress","text":"If you want to check on the progress <p>To check the deployment status, click on the EC2 instance. </p> <p>Right click the EC2 instance and click connect. </p> <p>Select \"Connect using a private IP\" and click Connect. </p> <p>Here we will enter the bash command below and see if we see the response \"Should now work.\".</p>  Code <pre><code>journalctl -u openflow-apply-infrastructure -f -n 10000 | grep \"should now work\"\n</code></pre> <p>You'll see it should say, should now work but even not it still may take some time for Snowflake to update.  </p>"},{"location":"ingestion/deployments/aws/#waiting","title":"Waiting","text":"<p>Now we can go back to openflow and refresh our window or use the refresh button. </p> <p>Warning</p> <p>It can take longer then the bash command leads to believe. I waited 40 minutes before it worked.</p>"},{"location":"ingestion/deployments/aws/#success","title":"Success","text":"<p>Now that your deployment is active we can move on to a runtime and add a connector. Please select one of the connector tutorials on the left. </p>"},{"location":"ingestion/deployments/snowflake/","title":"Snowflake","text":""},{"location":"ingestion/deployments/snowflake/#openflow-hosted-in-snowflake","title":"Openflow - Hosted in Snowflake","text":"<p>Goal of this tutorial is to setup openflow deployment in Snowflake container services. This will allow you to start adding runtimes that have connectors in them.</p>"},{"location":"ingestion/deployments/snowflake/#video","title":"Video","text":""},{"location":"ingestion/deployments/snowflake/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> </ul>"},{"location":"ingestion/deployments/snowflake/#openflow","title":"Openflow","text":"<p>Navigate to openflow in the navbar. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Launch openflow and login. </p>"},{"location":"ingestion/deployments/snowflake/#deployment","title":"Deployment","text":"<p>Once logged in lets click \"create deployment\". </p> <p>Click next. </p> <p>Now we'll want to select Snowflake as the deployment envirement. Give it a name and click next. </p> <p>Click next. </p> <p>Now your deployment will start creating. It will take between 5-15 minutes. </p> <p>Now that your deployment is active we can move on to a runtime and add a connector. Please select one of the connector tutorials on the left. </p>"},{"location":"ingestion/notebook/api/","title":"API","text":""},{"location":"ingestion/notebook/api/#notebooks-with-external-access","title":"Notebooks with external access","text":"<p>In this tutorial we will show how to connect a notebook to an external api to get data from it.</p>"},{"location":"ingestion/notebook/api/#video","title":"Video","text":""},{"location":"ingestion/notebook/api/#requirement","title":"Requirement","text":"<ul> <li>Must be on a on-demand (Paying) or contracted account. This is due to external access not being allowed on trial accounts. We also assume not complex security requirement.</li> </ul>"},{"location":"ingestion/notebook/api/#downloads","title":"Downloads","text":"<ul> <li>Notebook (Link)</li> </ul>"},{"location":"ingestion/notebook/api/#setup","title":"Setup","text":"<p>Lets start the network setup prcoess in Snowflake. </p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.api;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema api;\nuse warehouse developer;\n</code></pre>"},{"location":"ingestion/notebook/api/#external-access","title":"External Access","text":"<p>First lets start by setting up the network rules in a worksheet to allow our Snowflake Notebook to talk with our external source.</p>  Setup Result <pre><code>create or replace network rule openweathermap_network_rule\n    mode = egress\n    type = host_port\n    value_list = ('api.openweathermap.org');\n\nuse role accountadmin;\n\ncreate or replace external access integration openweathermap_access_integration\n    allowed_network_rules = (openweathermap_network_rule)\n    enabled = true;\n</code></pre> <pre><code>Integration OPENWEATHERMAP_ACCESS_INTEGRATION successfully created.\n</code></pre>"},{"location":"ingestion/notebook/api/#notebook","title":"Notebook","text":"<p>Next lets import the example notebook in our database / schema. </p> <p>Next lets assign it to our database/schema </p> <p>To enable our notebook to talk outside of Snowflake we'll have to enable the notebook to use that external access we created before. We can do this by going to Notebooks settings. </p> <p>Next clicking \"external access\" and then checking our external access. </p> <p>Finally we'll click \"run all\" and see our notebook query an outside source and put the results into a table for us. </p>"},{"location":"ingestion/notebook/sftp/","title":"SFTP","text":""},{"location":"ingestion/notebook/sftp/#sftp-to-snowflake-stage","title":"SFTP to Snowflake Stage","text":"<p>In this tutorial we will show how you can takes files from an sftp and load them into a Snowflake stage to later be loaded into a table.</p>"},{"location":"ingestion/notebook/sftp/#video","title":"Video","text":""},{"location":"ingestion/notebook/sftp/#requirement","title":"Requirement","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"ingestion/notebook/sftp/#download-needed-files","title":"Download needed files:","text":"<ul> <li>Notebook (Link)</li> </ul>"},{"location":"ingestion/notebook/sftp/#setup","title":"Setup","text":"<p>Lets start the network setup prcoess in Snowflake. </p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.sftp;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema sftp;\nuse warehouse development;\n</code></pre> <p>First lets start by setting up the network rules, stage and compute pool in a worksheet to allow our Snowflake Notebook to talk with our external source.</p>  Setup Result <pre><code>-- We use sysadmin because accountadmin can't be used in the containized notebooks.\nuse role sysadmin;\n\n-- We will store our files in the stage.\ncreate stage if not exists files directory = ( enable = true );\n\n-- This is the compute needed for our notebook.\ncreate compute pool sftp\n    min_nodes = 1\n    max_nodes = 1\n    instance_family = cpu_x64_xs;\n\n/*\nThis rule allows your Snowflake to talk to all\nexternal I.P's. Feel free to change it to your\nneeded I.P's. We also assume it's port 22.\n*/\ncreate or replace network rule sftp_network_rule\n    mode = egress\n    type = host_port\n    value_list = ('&lt;UPDATE WITH YOUR URL&gt;:22');\n\n-- We'll need this to download the sftp python package.\ncreate or replace network rule pypi_network_rule\n    mode = egress\n    type = host_port\n    value_list = ('pypi.org', 'pypi.python.org', 'pythonhosted.org',  'files.pythonhosted.org');\n\n\nuse role accountadmin;\n\ncreate or replace external access integration sftp_external_access\n    allowed_network_rules = (sftp_network_rule)\n    enabled = true;\n\ncreate or replace external access integration pypi_access_integration\n    allowed_network_rules = (pypi_network_rule)\n    enabled = true;\n\ngrant usage on integration sftp_external_access to role sysadmin;\ngrant usage on integration pypi_access_integration to role sysadmin;\n</code></pre> <pre><code>Integration PYPI_ACCESS_INTEGRATION successfully created.\n</code></pre>"},{"location":"ingestion/notebook/sftp/#notebook-creation","title":"Notebook Creation","text":"<p>Next lets import the example notebook in our database / schema. </p> <p>Next lets assign it to our database/schema </p>"},{"location":"ingestion/notebook/sftp/#enable-external-access","title":"Enable external access","text":"<p>To enable our notebook to talk outside of Snowflake we'll have to enable the notebook to use that external access we created earlier. We can do this by going to Notebooks settings. </p> <p>Next clicking \"external access\" and then checking our external access. </p> <p>Check your SFTP external access and pypi external access to give the notebook access to talk with the SFTP endpoint and PyPI to download the sftp python package. </p> Secret Management <p>Once the example is working I suggest to keep the password stored as a secret instead of plane text via the secret object. </p>"},{"location":"ingestion/notebook/sftp/#update-sftp-parameters","title":"Update SFTP parameters","text":"<p>We'll want to update our sftp python function input parameters with the hostname,username and password. After that put in your file you want to load into a stage. It can be either a single file or a ZIP which will be unzipped and loaded into a folder in the stage. </p> <p>Finally we'll click \"run all\" and see our notebook start, it may take up to 5 minutes to start the compute pool, we can see the status in the bottom right. </p>"},{"location":"ingestion/notebook/sftp/#result","title":"Result","text":"<p>Once finished we will see that the file we selected is loaded into the stage via the <code>ls</code> command to the stage </p> <p>We can also see that the file is in the stage via the UI. The idea here is that we can schedule the notebook to load the files from the stftp and then use a child task to load the file via a copy into coammand on a schedule. </p>"},{"location":"ingestion/other/github/","title":"Github","text":""},{"location":"ingestion/other/github/#openflow-version-control-github","title":"Openflow Version Control - Github","text":"<p>Goal of this tutorial is to show how you can connect to a github repo and load connectors that can be version controlled.</p>"},{"location":"ingestion/other/github/#video","title":"Video","text":""},{"location":"ingestion/other/github/#requirements","title":"Requirements","text":"<ul> <li>You can NOT be on a trial account. (Link)</li> <li>Snowflake account has to be in an AWS region.(Link)</li> </ul>"},{"location":"ingestion/other/github/#github-access-token","title":"Github Access Token","text":"<p>We will need a persoanl access token to allow Openflow to work with our Git repository. First lets navigate to the token page. </p> <p>Click on settings. </p> <p>Next developer settings. </p> <p>We'll be using a classic token. </p> <p>Next we'll enter in a name, the experation of the token, and select \"repo\" for the scope of the permissions. </p> <p>We'll copy our token, it will be used in a later Snowflake step. </p>"},{"location":"ingestion/other/github/#snowflake","title":"Snowflake","text":"<p>Lets head into snowflake and going into a workspace worksheet (1) and creating the nesseray objects for openflow and the github connection.</p> If you don't have a database, schema, or warehouse yet.  Database, schema and warehouse <pre><code>-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects.\ncreate schema if not exists raw.network;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to query our integration and to load data.\n*/\ncreate warehouse if not exists openflow \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Only required if your hosting openflow in Snowflake (SPCS)</p> <p>Lets create the network rule and external access that will allow openflow/snowflake to talk with our github repo.</p>  Code Result <pre><code>-- Create network rule for our Github account.\ncreate or replace network rule github_network_rule\n    mode = egress\n    type = host_port\n    value_list = (\n        'api.github.com'\n    );\n\n-- Create one external access integration with all network rules.\ncreate or replace external access integration openflow_external_access\n    allowed_network_rules = (github_network_rule)\n    enabled = true;\n</code></pre> <p>Integration OPENFLOW_EXTERNAL_ACCESS successfully created.</p>"},{"location":"ingestion/other/github/#openflow","title":"Openflow","text":"<p>Next we'll head into openflow to setup our runtime and add the connector. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Click \"Launch openflow\". </p>"},{"location":"ingestion/other/github/#add-the-runtime","title":"Add the runtime","text":"<p>We'll create a new runtime. </p> <p>We'll select our deployment, give the runtime a name, select our snowflake role and if deployed in Snowflake our external access intergration. </p> <p>Now we'll wait 5-10 minutes for our runtime to become usable. </p> If you get the error 'Invalid consent request' or 'TOTP Invalid' <p>You will have to change your default role to a role that is not an admin role. Example default would be public. </p> <p>Once the runtime is \"Active\" we can click to go into it. </p>"},{"location":"ingestion/other/github/#connect-to-repository","title":"Connect to Repository","text":"<p>First we'll head to controller settings. </p> <p>The registry clients and we'll add a new one via the plus button. </p> <p>We'll select github and click add. </p> <p>Once added we'll click the three dots on the right and select edit. </p> <p>We'll put in our github repository information as shown. I believe you can also put public repos as well. </p> <p>Next we'll drag \"import from repository\" on to the canvas. </p> <p>Now you'll be able to switch to the github registry and select your branch, bucket \"aka folder to store into\" and if you have a connector in there you'll be able to add it to the canvas. If not thats okay. </p> <p>Now we can modify or add our process groups to our reposity by right clicking and using one of the options avaliable to us from git. </p>"},{"location":"other/code/go/readme/","title":"Readme","text":"<p>temp</p>"},{"location":"other/code/jdbc/jdbc/","title":"Snowflake - JDBC connection","text":""},{"location":"other/code/jdbc/jdbc/#snowflake-jdbc-connection","title":"Snowflake - JDBC connection","text":"<p>This tutorial will show you how to create a connector to load data into snowflake from your java application using our jdbc library.</p>"},{"location":"other/code/jdbc/jdbc/#video","title":"Video","text":"<p>Video still in development</p>"},{"location":"other/code/jdbc/jdbc/#requirements","title":"Requirements","text":"<ul> <li> <p>We will need our account url. This can be found in the url bar when you log into Snowflake. It will end with <code>.snowflakecomputing.com</code>.</p> </li> <li> <p>Add the jdbc library (.jar) file into your java project / application. This will depend on the application and IDE. A list can be found here. I used the most recent (closest to current date).</p> </li> </ul>"},{"location":"other/code/jdbc/jdbc/#example","title":"Example","text":"<p>Get your customer to run this sql in there snowflake account and then have them tell you what they used as variables:</p> <p>This is an example where the user would give you the string values for connection, role, warehouse and database. It creates a schema called products and a table called customers in products schema. I have also inserted some sample data. </p>  Sql Java Result <pre><code>Begin; \n    -- create variables for user / password / role / warehouse / database (needs to be uppercase for objects)\n    set role_name = 'service_jdbc';\n    set user_name = 'service_jdbc';\n    set user_password = 'Password12';\n    set warehouse_name = 'service_jdbc';\n    set database_name = 'jdbc';\n\n    -- change role to securityadmin for user / role steps\n    use role securityadmin;\n\n    -- create role for jdbc\n    create role if not exists identifier($role_name);\n    grant role identifier($role_name) to role SYSADMIN;\n\n    -- create a user for jdbc\n    create user if not exists identifier($user_name)\n    password = $user_password\n    default_role = $role_name\n    default_warehouse = $warehouse_name;\n\n    grant role identifier($role_name) to user identifier($user_name);\n\n    -- change role to sysadmin for warehouse / database steps\n    use role sysadmin;\n\n    -- create a warehouse for jdbc\n    create warehouse if not exists identifier($warehouse_name)\n    warehouse_size = xsmall\n    warehouse_type = standard\n    auto_suspend = 60\n    auto_resume = true\n    initially_suspended = true;\n\n    -- create database for jdbc\n    create database if not exists identifier($database_name);\n\n    -- grant jdbc role access to warehouse\n    grant USAGE\n    on warehouse identifier($warehouse_name)\n    to role identifier($role_name);\n\n    -- grant jdbc access to database\n    use role ACCOUNTADMIN;\n    grant CREATE SCHEMA, MONITOR, USAGE\n    on database identifier($database_name)\n    to role identifier($role_name);\ncommit;\n</code></pre> <pre><code>import java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.sql.SQLException;\nimport java.sql.Statement;\n\npublic class SnowflakeExample {\n\n    public static void main(String[] args) {\n        /* \n            Snowflake login information and object to be used. This would be\n            given to you once the user runs the connector code.\n\n            For documentation on what the account url will be use our snowflake docs:\n            https://docs.snowflake.com/en/user-guide/admin-account-identifier#non-vps-account-locator-formats-by-cloud-platform-and-region\n        */ \n        String account_url = \"GGB82720.snowflakecomputing.com\";\n        String role        = \"service_jdbc\";\n        String warehouse   = \"service_jdbc\";\n        String user        = \"service_jdbc\";\n        String password    = \"Password12\";\n        String database    = \"jdbc\";\n\n        String url = \"jdbc:snowflake://\"+account_url+\"/?warehouse=\"+warehouse+\"&amp;role=\"+role;\n\n        // You can choose this. I'm just using the product as an example.\n        String schema = \"data\";\n        String table  = \"example\";\n\n        // JDBC connection object\n        Connection connection = null;\n\n        try {\n            // Load the Snowflake JDBC driver\n            Class.forName(\"net.snowflake.client.jdbc.SnowflakeDriver\");\n\n            // Establish the connection\n            connection = DriverManager.getConnection(url, user, password);\n\n            // Create a Statement object\n            Statement statement = connection.createStatement();\n\n            // Create the schema.\n            statement.execute(\"CREATE schema IF NOT EXISTS \" + database + \".\" + schema);\n            // Create the table.\n            statement.execute(\"CREATE TABLE IF NOT EXISTS \" + database + \".\" + schema + \".\"+ table +\" (\" +\n                \"id INT, \" +\n                \"name VARCHAR(255), \" +\n                \"customer VARCHAR(255)\" +\n                \")\"\n            );\n            // Insert some data.\n            statement.execute(\"INSERT INTO \" + database + \".\" + schema + \".\"+ table +\" (id, name, customer) VALUES \" +\n                \"(1, 'John Doe', 'Company A'), \" +\n                \"(2, 'Jane Smith', 'Company B'), \" +\n                \"(3, 'Bob Johnson', 'Company C'), \" +\n                \"(4, 'Alice Brown', 'Company A'), \" +\n                \"(5, 'Charlie White', 'Company B')\"\n            );\n\n            System.out.println(\"Created successfully.\");\n\n        } catch (ClassNotFoundException | SQLException e) {\n            e.printStackTrace();\n        } finally {\n            // Close the connection in the finally block to ensure proper cleanup\n            if (connection != null) {\n                try {\n                    connection.close();\n                } catch (SQLException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>Run: </p> <p>The result in snowflake: </p>"},{"location":"other/code/r/r/","title":"Snowflake - R:","text":""},{"location":"other/code/r/r/#snowflake-r","title":"Snowflake - R:","text":"<p>Example already created just add it here.</p>"},{"location":"other/snowconvert/redshift/","title":"S3 URL - s3://danielwilczak-snowconvert/","text":"<p>Need to update with the use of external tables now.</p> <ol> <li>create redshift, snowflake and s3 bucket.</li> <li>Download snowconvert for refshift. </li> <li>Fill in project name, source, folder path and add key  Key = aecbfe93-3615-4aae-bd0f-ea6ae1460b1a (Valid through: 08/06/2025)</li> <li>Three options to login (IAM Provision Cluster, IAM Serverless, Standard) I used standard and it work. Need to figure out other two and give them as options.</li> <li>Setup Snowflake account (Image of where to get account identifier)</li> <li>Redshift will create the tables.</li> <li>Data miration service.</li> <li>User will need to use the redhisft iam role, and give that role to the bucket.</li> </ol>"},{"location":"other/snowconvert/redshift/#s3-url-s3danielwilczak-snowconvert","title":"S3 URL - s3://danielwilczak-snowconvert/","text":""},{"location":"other/snowconvert/redshift/#role-arnawsiam084828565535roleservice-roleamazonredshift-commandsaccessrole-20250506t162039","title":"Role - arn:aws:iam::084828565535:role/service-role/AmazonRedshift-CommandsAccessRole-20250506T162039","text":""},{"location":"other/snowconvert/redshift/#user-akiar5htps2n-must-have-access-to-the-above-role-in-the-trusted-relationship","title":"User - AKIAR....5HTPS2N (Must have access to the above role in the trusted relationship)","text":""},{"location":"other/snowconvert/redshift/#secret-5zpk0ktwx9anithhhoqbybvssq","title":"Secret - 5ZPK0Ktwx9a....NithhHo+QBYBVSs+Q","text":"<ol> <li>Data will get pushed to the bucket.</li> <li>Probably should talk about logs here.</li> </ol>"},{"location":"other/snowconvert/sqlserver/","title":"Index","text":""},{"location":"science/analyst/","title":"Cortex Analyst","text":""},{"location":"science/analyst/#science-cortex-analyst","title":"Science - Cortex Analyst","text":"<p>In this tutorial we will go through an introduction to setting up cortex analyst. We'll show two ways to deploy it via streamlit in Snowflake and Flask.</p>"},{"location":"science/analyst/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"science/analyst/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"science/analyst/#download","title":"Download","text":"<ul> <li>Sales Semantic Layer (Link)</li> </ul>"},{"location":"science/analyst/#setup","title":"Setup","text":"<p>Lets create some example data first the our chatbot (Cortex Analyst) can use later on.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.cortex;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n-- We'll use this stage \"folder\" to store our semantic layer.\ncreate stage files directory = ( ENABLE = true );\n\nuse database raw;\nuse schema cortex;\nuse warehouse development;\n</code></pre>  Tables Result <pre><code>-- Product Table\ncreate or replace table product (\n    product_id string primary key,\n    product_name string,\n    category string,\n    brand string\n);\n\n-- customer table\ncreate or replace table customer (\n    customer_id string primary key,\n    customer_name string,\n    region string\n);\n\n-- sales table (denormalized)\ncreate or replace table sales (\n    sale_id string primary key,\n    sale_date date,\n    day_of_week string,\n    store_name string,\n    city string,\n    region string,\n    product_id string,\n    customer_id string,\n    quantity_sold int,\n    revenue float\n);\n</code></pre> status Table SALES successfully created. <p>Lets now insert some data to those tables.</p>  Insert Result <pre><code>-- insert into product\ninsert into product (product_id, product_name, category, brand) values\n('p001', 'smartphone x', 'electronics', 'branda'),\n('p002', 'laptop pro', 'electronics', 'brandb'),\n('p003', 'wireless earbuds', 'accessories', 'branda');\n\n-- insert into customer\ninsert into customer (customer_id, customer_name, region) values\n('c001', 'alice johnson', 'north'),\n('c002', 'bob smith', 'south'),\n('c003', 'carol lee', 'east');\n\n-- insert into sales\ninsert into sales (sale_id, sale_date, day_of_week, store_name, city, region, product_id, customer_id, quantity_sold, revenue) values\n('s0001', '2023-04-01', 'saturday', 'downtown store', 'new york', 'east', 'p001', 'c001', 2, 1200.00),\n('s0002', '2023-04-02', 'sunday', 'uptown store', 'chicago', 'midwest', 'p002', 'c002', 1, 1500.00),\n('s0003', '2023-04-03', 'monday', 'suburban store', 'los angeles', 'west', 'p003', 'c003', 3, 300.00);\n</code></pre> number of rows inserted 3"},{"location":"science/analyst/#building-cortex-analyst","title":"Building Cortex Analyst","text":"<p>Now that we have our data loaded into our tables lets build the semantic layer and play with it in the Cortex Analyst playground.</p> <p>To start lets upload our example semantic layer to our stage. We can do that by clicking \"+ Files\". </p> <p>Click browse. </p> <p>Select the example semantic layer called sales.  </p> <p>Now that we have our example uploaded we can head to cortex analyst under the AI / ML Studio. </p> <p>We'll select our semantic layer from our stage and click open. </p> <p>Now that it's open we can immediately start playing with the semantic layer / chatbot. </p> <p>We can see that the chatbot gives us example questions we can click and get results from. These examples are in the semantic layer. </p> <p>We can also leave edit mode to show what it can potentially look like when we deploy the applicaiton. </p> <p>You can see that our chatbot not only gives us the data in a table but also as a query. This can be changed in the future to remove the sql and or add a diagram / chart. </p>"},{"location":"science/analyst/#edit-semantic-layer","title":"Edit Semantic Layer","text":"<p>In the playground we can also edit the semantic layer by using the GUI interface or by code. A great note here is that I found it helpful to use chatgpt to write my semantic layer code for me. I just provided it the table defenitions and it built the rest. Took me only 15 mins to get it to my liking with some great example quries. </p>"},{"location":"science/analyst/#monitoring","title":"Monitoring","text":"<p>You can monitor the usage of this semantic layer by going to the monitoring tab, this allows you to see how people are using your chatbot to help correct it without asking for feedback. </p>"},{"location":"science/analyst/#deployment","title":"Deployment","text":"<p>Now that we have our cortex analyst chatbot built we'll want to deploy it. In this section we have two example platforms but you can deploy to many more.</p>"},{"location":"science/analyst/#streamlit-in-snowflake","title":"Streamlit in Snowflake","text":"<p>Lets deploy our semantic layer / chatbot in streamlit in Snowflake. </p>"},{"location":"science/analyst/#downloads","title":"Downloads","text":"<ul> <li>Streamlit Application (Link)</li> </ul>"},{"location":"science/analyst/#streamlit","title":"Streamlit","text":"<p>To start lets create a new streamlit application in Snowflake. </p> <p>Lets give our streamlit app a name, where to put it and finally the warehouse that will be used. </p> <p>Note</p> <p>If you put your semantic file somewhere else you'll just update it the string of the location.</p> <p>Now that we have the app, lets copy in the code from the example application that we downloaded earlier and hit run. </p> <p>Now you can see the chatbot and start using it and sharing it with any team who needs a sales chatbot. </p>"},{"location":"science/analyst/#flask-website","title":"Flask Website","text":"<p>Lets move to a build use case where we want to add our semantic layer into our application. In this example we'll build two additional semantic layers and then deploy them in our flask application. Make sure we download the necessary files for this section.</p>"},{"location":"science/analyst/#downloads_1","title":"Downloads","text":"<ul> <li>Flask Application (Link)</li> <li>All semantic layers (Link)</li> </ul>"},{"location":"science/analyst/#snowflake","title":"Snowflake","text":"<p>Lets adding some new tables for our new semantic layers.</p>  Tables <pre><code>CREATE TABLE raw.cortex.campaign_dim (\n  campaign_id STRING PRIMARY KEY,\n  campaign_name STRING,\n  channel STRING,\n  start_date DATE,\n  end_date DATE\n);\n\nCREATE TABLE raw.cortex.ad_fact (\n  ad_id STRING PRIMARY KEY,\n  campaign_id STRING,\n  date_id DATE,\n  impressions INT,\n  clicks INT,\n  cost FLOAT\n);\n\nCREATE TABLE raw.cortex.agent_dim (\n  agent_id STRING PRIMARY KEY,\n  agent_name STRING,\n  team STRING,\n  region STRING\n);\n\nCREATE TABLE raw.cortex.ticket_fact (\n  ticket_id STRING PRIMARY KEY,\n  agent_id STRING,\n  issue_type STRING,\n  status STRING,\n  created_at TIMESTAMP,\n  resolved_at TIMESTAMP\n);\n</code></pre> <p>Lets add data to those tables.</p>  Insert <pre><code>INSERT INTO raw.cortex.campaign_dim VALUES\n  ('C001', 'Spring Promo', 'email', '2024-03-01', '2024-03-31'),\n  ('C002', 'Summer Social', 'social', '2024-06-01', '2024-06-30'),\n  ('C003', 'Holiday Search Ads', 'search', '2024-11-15', '2024-12-31');\n\nINSERT INTO raw.cortex.ad_fact VALUES\n  ('A001', 'C001', '2024-03-05', 10000, 250, 500.00),\n  ('A002', 'C002', '2024-06-10', 20000, 500, 800.00),\n  ('A003', 'C003', '2024-11-20', 30000, 600, 1500.00);\n\nINSERT INTO raw.cortex.agent_dim VALUES\n  ('AG001', 'Alice Smith', 'Technical', 'North America'),\n  ('AG002', 'Bob Johnson', 'Billing', 'Europe'),\n  ('AG003', 'Cindy Lee', 'Technical', 'Asia');\n\nINSERT INTO raw.cortex.ticket_fact VALUES\n  ('T001', 'AG001', 'Login Issue', 'Resolved', '2024-04-01 08:00:00', '2024-04-01 09:00:00'),\n  ('T002', 'AG002', 'Payment Failure', 'Resolved', '2024-04-02 10:30:00', '2024-04-02 11:15:00'),\n  ('T003', 'AG003', 'Feature Request', 'Open', '2024-04-03 14:00:00', NULL);\n</code></pre> <p>Next lets upload the new semantic layers to our stage. </p>"},{"location":"science/analyst/#flask","title":"Flask","text":"<p>Lets walk throught the process of setting up the example files and starting our flask application. First we'll want to update our Snowflake account information in the <code>app.py</code> file.</p>  Python <pre><code># If your snowflake account has an underscore then you must use the\n# account locator here for host. Otherwise it can we the same as account.\nHOST = \"&lt;Account Identifier&gt;\"\nACCOUNT = \"&lt;Account Identifier&gt;\"\nUSER = \"&lt;Username&gt;\"\nPASSWORD = \"&lt;Password&gt;\"\nROLE = \"&lt;role&gt;\"\n</code></pre>  Example <pre><code># If your snowflake account has an underscore then you must use the\n# account locator here for host. Otherwise use your account URL.\nHOST = \"EASYCONNECT-ACCOUNT.snowflakecomputing.com\"\nACCOUNT = \"EASYCONNECT-ACCOUNT\"\nUSER = \"danielwilczak\"\nPASSWORD = \"...\"\nROLE = \"ACCOUNTADMIN\"\n</code></pre>  Example 2 <pre><code># If your snowflake account has an underscore then you must use the\n# account locator here for host. Otherwise use your account URL.\nHOST = \"CVB15898.snowflakecomputing.com\"\nACCOUNT = \"EASYCONNECT-TUTORIALS_AWS\"\nUSER = \"danielwilczak\"\nPASSWORD = \"...\"\nROLE = \"ACCOUNTADMIN\"\n</code></pre> <p>Next we'll want to start a python virtual environment and install the packages we need. This may be different on windows.</p>  Python <pre><code>python -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Now that we have everything for the application, lets run it.</p>  Python <pre><code>python app.py \n</code></pre> <p>Now we can go to the I.P address provided and see our flask application with multiple semantic models. </p>"},{"location":"science/models/registry/","title":"Registry","text":""},{"location":"science/models/registry/#snowflake-model-registry","title":"Snowflake Model Registry","text":"<p>In this tutorial we will show how you can create a model in a notebook, save that model to the model registry and deploy/use that model via SQL.</p>"},{"location":"science/models/registry/#video","title":"Video","text":"<p>Video still in development</p>"},{"location":"science/models/registry/#requirement","title":"Requirement","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"science/models/registry/#download","title":"Download","text":"<ul> <li>Notebook (Link)</li> </ul>"},{"location":"science/models/registry/#setup","title":"Setup","text":"<p>Lets start by adding some sample data and uploading our notebook. </p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role accountadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.science;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema science;\nuse warehouse development;\n</code></pre>"},{"location":"science/models/registry/#sample-data","title":"Sample data","text":"<p>First lets start by creating some sample data for our model to train from.</p>  Code Result <pre><code>use role accountadmin;\n\n-- Sample Data\ncreate or replace table student_test_scores as\n    select\n        $1 as hours_studied,\n        $2 as test_score\n    from values\n        (1.0, 3.5),\n        (2.0, 9.2),\n        (3.0, 13.1),\n        (4.0, 24.7),\n        (5.0, 28.5),\n        (6.0, 41.0),\n        (7.0, 50.3),\n        (8.0, 63.5),\n        (9.0, 82.1),\n        (10.0, 95.0);\n</code></pre> <pre><code>| status                                          |\n|-------------------------------------------------|\n| Table STUDENT_TEST_SCORES successfully created. |\n</code></pre>"},{"location":"science/models/registry/#upload-and-setup-notebook","title":"Upload and Setup Notebook","text":"<p>Lets upload our notebook we downloaded earlier. Start by navigating to notebooks and clicking <code>import .ipynb file</code>. </p> <p>Next we'll name it and select the location and that we want to run via a warehouse. </p> <p>Our final step before running is to install the needed package <code>snowflake-ml-python</code> to build and save our model. The package comes with <code>sklearn</code> with it. </p>"},{"location":"science/models/registry/#run-the-notebook","title":"Run the notebook","text":"<p>To run the notebook we'll click <code>run all</code>.  </p> <p>Once the model is run we can refresh our schema to see a model object has been added. We'll want to open the details to get our metrics we saved in the notebook.  </p> <p>We can click on the version we just created. </p> <p>We can see the metadata  our model and also the functions we will use in the next section to make predictions via SQL. </p>"},{"location":"science/models/registry/#use-the-model-via-sql","title":"Use the model via SQL","text":"<p>Now that we have our model saved into our model registry, lets use the model via sql to make a prediction. Lets open a new worksheet.</p>  Code Result <pre><code>-- Using the default version.\nselect\n    8 as hours_studied,\n\n    predict_test_score!predict(hours_studied) as result,\n\n    result:output_feature_0::float as prediction;\n</code></pre> <pre><code>| HOURS_STUDIED | RESULT                                          | PREDICTION  |\n|---------------|-------------------------------------------------|-------------|\n| 8             | {   \"output_feature_0\": 6.647030303030303e+01 } | 66.47030303 |\n</code></pre> <p>We can also select specific versions if we have multiple or want to retrieve the latest model version..</p>  Code Result <pre><code>-- Selecting a specific model version.\nwith\n    predict_test_score as model predict_test_score version last\n\nselect\n    3.5 as hours_studied,\n    predict_test_score!predict(hours_studied) as result,\n    result:output_feature_0::float as prediction;\n</code></pre> <pre><code>| HOURS_STUDIED | RESULT                                          | PREDICTION   |\n|---------------|-------------------------------------------------|--------------|\n| 3.5           | {   \"output_feature_0\": 2.586181818181818e+01 } | 25.861818182 |\n</code></pre>"},{"location":"science/models/registry/#change-default-version","title":"Change default version","text":"<p>Updating the version can allow us to make sure downstream users are using the correct model.</p>  Code Example Result <pre><code>alter model\n    predict_test_score\nset\n    default_version = &lt;Model version name&gt;;\n</code></pre> <pre><code>alter model\n   predict_test_score\nset\n    default_version = V20241122_114535;\n</code></pre> <pre><code>| status                           |\n|----------------------------------|\n| Statement executed successfully. |\n</code></pre>"},{"location":"science/models/vectors/","title":"Index","text":"<pre><code>-- Create the 'products' table\nCREATE OR REPLACE TABLE products (\n    product_id INT,\n    product_name VARCHAR(100),\n    description VARCHAR(500),\n    description_vector vector(float, 768)\n);\n\n-- Insert sample data into 'products'\nINSERT INTO products (product_id, product_name, description) VALUES\n    (1, 'Laptop', 'A high-performance laptop with 16GB RAM and 512GB SSD.'),\n    (2, 'Smartphone', 'A smartphone with excellent camera quality and battery life.'),\n    (3, 'Headphones', 'Noise-cancelling over-ear headphones with superior sound quality.');\n\n\nupdate products\n  set description_vector = snowflake.cortex.embed_text_768('snowflake-arctic-embed-m', description);\n\n\nselect * from products;\n</code></pre> <p>``` with     prompt as (         select             'I want to get a new smartphone' as prompt,             snowflake.cortex.embed_text_768('snowflake-arctic-embed-m', prompt) as prompt_vector     )</p> <p>select     * exclude (prompt_vector, description_vector) from     prompt, products order by     vector_L2_distance(prompt_vector, description_vector) limit     2;```</p>"},{"location":"science/notebooks/RAG/pdf/pdf/","title":"Snowflake Notebook RAG with PDF's","text":""},{"location":"science/notebooks/RAG/pdf/pdf/#snowflake-notebook-rag-with-pdfs","title":"Snowflake Notebook RAG with PDF's","text":"<p>In this tutorial we will show you how to perform RAG on a series of PDF's in Snowflake using Snowflake notebooks. To make it usable to users we will also add a streamlit application in our notebook so that it's simple for users to interat with the content.</p>"},{"location":"science/notebooks/RAG/pdf/pdf/#video","title":"Video","text":""},{"location":"science/notebooks/RAG/pdf/pdf/#requirements","title":"Requirements","text":"<p>If you have a Snowflake account, reach out to your account team to have these enabled.  </p> <ul> <li>You will need access to Snowflake Notebooks which is still currently in private preview.  </li> <li>You will need access to our embed/vector LLM functions which is still currently in private preview.  </li> </ul>"},{"location":"science/notebooks/RAG/pdf/pdf/#download","title":"Download","text":"<ul> <li>Notebook</li> <li>PDF files</li> </ul>"},{"location":"science/notebooks/RAG/pdf/pdf/#tutorial","title":"Tutorial","text":"<p>Lets start by setting up some objects in snowflake and follow it up with uploading and using the notebook.</p>"},{"location":"science/notebooks/RAG/pdf/pdf/#setup","title":"Setup","text":"<p>In this section we will do the setup to support our notebook. Lets open a notebook and run the code below.</p> If you don't have a database, schema or warehouse yet. <pre><code>use role accountadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists \n    raw comment='This is only api data from our sources.';\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.pdf;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema pdf;\nuse warehouse developer;\n/*\n    Setup our two stages (aka filder) that the\n    user defined functions and our pdf files will live in.\n*/\n</code></pre> <p>Lets setup our stages to upload the pdf and store some sql udf's.</p>  Setup Result <pre><code>create or replace stage udf;\ncreate or replace stage folder;\n</code></pre> <pre><code>Stage area FOLDER successfully created. \n</code></pre>"},{"location":"science/notebooks/RAG/pdf/pdf/#upload-pdfs","title":"Upload PDF's","text":"<p>Using the UI we will upload our downloaded pdf's into our stage (folder on Snowflake). Lets go to the stage in our snowflake account. </p> <p>Once we click the \"+ Files\" button we will need to drag or select the pdf files that downloaded earlier. </p> <p>Once selected we can click upload. </p> <p>Once upload we will see the files in the stage. Next we can go to notebooks. </p>"},{"location":"science/notebooks/RAG/pdf/pdf/#notebook","title":"Notebook","text":"<p>Let's start with going to the notebooks. </p> <p>Click the upload button so that we can upload our provided notebooks. </p> <p>Input the name of the notebook, select the database, schema and warehouse. </p> <p>Select the packages drop-down. </p> <p>Add these three packages with the version specified:  </p> <ul> <li>langchain  = 0.0.298</li> <li>pypdf2 = 2.10.5</li> <li>snowflake-snowpark-python 1.13.0a1  </li> <li>pandas = 2.14</li> </ul> <p></p> <p>You can either run all the cells or one by one. </p>"},{"location":"science/notebooks/RAG/pdf/pdf/#dataflow","title":"Dataflow","text":"<p>Here we can see the data flow of of our notebook / application. </p>"},{"location":"science/notebooks/RAG/pdf/pdf/#step-by-step","title":"Step by Step","text":"<p>I will add this section later but for now the video will be best. </p>"},{"location":"science/notebooks/containers/","title":"Intro to Containerized","text":""},{"location":"science/notebooks/containers/#containerize-notebooks","title":"Containerize Notebooks","text":"<p>In this tutorial we will show how to use containerized notebooks using the chatgpt 2 model from hugging face.</p>"},{"location":"science/notebooks/containers/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"science/notebooks/containers/#requirement","title":"Requirement","text":"<ul> <li>Be in a container services enabled region.  </li> <li>You CAN NOT be on a trial account.</li> </ul>"},{"location":"science/notebooks/containers/#downloads","title":"Downloads","text":"<ul> <li>Notebook (Link)</li> </ul>"},{"location":"science/notebooks/containers/#setup","title":"Setup","text":"If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.notebook;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema notebook;\nuse warehouse development;\n</code></pre>"},{"location":"science/notebooks/containers/#gpu-compute-pool","title":"GPU Compute Pool","text":"<p>First lets start by creating a gpu compute pool for our notebook via accountadmin role and then grant sysadmin to use it.</p>  Setup Result <pre><code>use role accountadmin;\n\ncreate compute pool gpu_small\n    min_nodes = 1\n    max_nodes = 1\n    instance_family = gpu_nv_s;\n\ngrant usage on compute pool gpu_small to role sysadmin;\n</code></pre> <pre><code>Statement executed successfully.\n</code></pre>"},{"location":"science/notebooks/containers/#external-access","title":"External Access","text":"<p>Lets create the network rules in a worksheet to allow our Snowflake Notebook to talk with our external source.</p>  Setup Result <pre><code>use role sysadmin;\n\ncreate or replace network rule pypi_network_rule\n    mode = egress\n    type = host_port\n    value_list = ('pypi.org', 'pypi.python.org', 'pythonhosted.org',  'files.pythonhosted.org');\n\ncreate or replace network rule hf_network_rule\n        mode = egress\n        type = host_port\n        value_list = ('huggingface.co', 'cdn-lfs.huggingface.co','cdn-lfs.hf.co');\n\nuse role accountadmin;\n\ncreate or replace external access integration pypi_access_integration\n    allowed_network_rules = (pypi_network_rule)\n    enabled = true;\n\ncreate or replace external access integration hf_access_integration\n    allowed_network_rules = (hf_network_rule)\n    enabled = true;\n\ngrant usage on integration pypi_access_integration to role sysadmin;\ngrant usage on integration hf_access_integration to role sysadmin;\n</code></pre> <pre><code>Statement executed successfully.\n</code></pre>"},{"location":"science/notebooks/containers/#notebook","title":"Notebook","text":"You can not use accountadmin. Please switch to sysadmin role. <p>Lets start by setting our role to sysadmin. </p>"},{"location":"science/notebooks/containers/#setup_1","title":"Setup","text":"<p>Upload the example notebook provided. </p> <p>Select Run on container, ML runtime and the gpu compute pool we created earlier.  </p> <p>To enable external connection to pypi and hugging face we will need to go to notebook settings. </p> <p>Select External access. </p> <p>Enable both pypi and huggingface. </p> <p>Click run all, the notebook will provide you a timer. It typically takes 5-7 minutes. </p>"},{"location":"science/notebooks/containers/#result","title":"Result","text":"<p>Once the notebook runs you will see we first make a directory for files to be downloaded to. </p> <p>We then download the chatgpt 2 model and save it to our files folder. </p> <p>We finally use our model and gpu to provide a prompt and get a response back. </p>"},{"location":"science/notebooks/jupyter/","title":"Intro to Local Jupyter","text":""},{"location":"science/notebooks/jupyter/#introduction-to-jupyter-notebook","title":"Introduction to Jupyter Notebook","text":"<p>In this tutorial you will learn how to get data from snowflake into our local jupyter notebook.</p>"},{"location":"science/notebooks/jupyter/#video","title":"Video","text":""},{"location":"science/notebooks/jupyter/#installing-jupyter-notebook","title":"Installing Jupyter Notebook","text":"<p>The fastest way I got jupyter notebooks up and running is by installing VsCode and then creating a new file with the extension \".ipynb\".</p>"},{"location":"science/notebooks/jupyter/#downloads","title":"Downloads","text":"<ul> <li>Notebook (Link)</li> </ul>"},{"location":"science/notebooks/jupyter/#setup","title":"Setup","text":"<p>Lets start with some setup.</p>"},{"location":"science/notebooks/jupyter/#snowflake","title":"Snowflake","text":"If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create the raw database for our data and a science database for our models.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all objects that we will need later.\ncreate schema if not exists raw.jupyter;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n</code></pre> <p>Lets create the table we'll query later in the notebook.</p>  Code Result <pre><code>create table xy (\n    x int,\n    y int\n);\n\n-- Insert 15 rows of fake data\ninsert into xy (x, y)\n    values\n    (1, 10),\n    (2, 15),\n    (3, 23),\n    (4, 25),\n    (5, 32),\n    (6, 35),\n    (7, 40),\n    (8, 43),\n    (9, 50),\n    (10, 52);\n</code></pre> <pre><code>Unumber of rows inserted - 10\n</code></pre>"},{"location":"science/notebooks/jupyter/#jupyter-notebook","title":"Jupyter Notebook","text":"Please enable token caching if you get to many MFA notifications <pre><code>alter account set allow_client_mfa_caching = TRUE;\n</code></pre> <p>To add our development environment we'll want to get our account identifier.  </p> <p>In the notebook we'll have to update our connectection parameters. Once pasted we'll make sure we replace the <code>.</code> with a <code>-</code> otherwise it won't connect to the account.</p>  Setup Example <pre><code>connection_parameters = {\n    \"account\":  \"&lt;ACCOUNT IDENTIFIER&gt;\",\n    \"user\":     \"&lt;MY USERNAME&gt;\",\n    \"password\": \"&lt;YOUR PASSWORD&gt;\"\n}   \n</code></pre> <pre><code>connection_parameters = {\n    \"account\":  \"wzb37388.gh787th\",\n    \"user\":     \"daniel.wilczak@snowflake.com\",\n    \"password\": \"password123\"\n}   \n</code></pre>"},{"location":"science/notebooks/jupyter/#results","title":"Results","text":"<p>We can follow along in the notebook or see the result in the image below. </p>"},{"location":"science/notebooks/jupyter/#troubleshooting","title":"Troubleshooting","text":"<p>Here are some common errors I've seen during the setup process.</p>"},{"location":"science/notebooks/jupyter/#cant-find-module-snowflake","title":"Can't find module - Snowflake","text":"<p>If you encounter an error like \"module not found\" or \"Can't find module,\" it indicates that the Snowflake Python package has not been installed in your Jupyter Notebook environment. </p>"},{"location":"science/notebooks/jupyter/#solution-1","title":"Solution 1","text":"<p>If you run into an issue where you install the dependencies but when you run the snowflake connect code it doesn't work. Try restarting the kernal.</p> <p></p>"},{"location":"science/notebooks/jupyter/#solution-2","title":"Solution 2","text":"<p>If restarting the kernal did not work then you might have to change your envirement to a \"virtual envirement\". Steps shown below.</p> <p></p> <p></p>"},{"location":"security/authentication/pair/","title":"Key Pair","text":""},{"location":"security/authentication/pair/#key-pair-authentication","title":"Key Pair Authentication","text":"<p>In this tutorial we will show how to setup a user with key pair authentication. We will then use a python example to connect to our account and run a simple query.</p> <p>For the official Snowflake documentation this tutorial was based on: https://docs.snowflake.com/en/user-guide/key-pair-auth</p>"},{"location":"security/authentication/pair/#video","title":"Video","text":""},{"location":"security/authentication/pair/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p>"},{"location":"security/authentication/pair/#create-key","title":"Create Key","text":"<p>Lets create the private and public key so that we can apply the public key to our user.</p>  Setup Result <pre><code>openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n</code></pre> <pre><code>Writing RSA key.\n</code></pre> <p>This will create two files in the folder we are currently located. </p>"},{"location":"security/authentication/pair/#apply-key-to-user","title":"Apply key to user","text":"<p>Lets apply the public key to our user in Snowflake. The public key file will end with <code>.pub</code>.</p>  Code Example Result <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user &lt;username&gt;;\n\n-- Apply the public key to the user.\nalter user &lt;username&gt; set rsa_public_key='&lt;Public Key&gt;';\n</code></pre> <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user danielwilczak;\n\n-- Apply the public key to the user.\nalter user danielwilczak set rsa_public_key='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA\nzd7lfIGps+lBXrVCT05l 92rDpYUsXyjtvAu26Q2z0k3/7n7HnZNmKjreIlGQJZl\nBe0Eud4LzqGX9Vbp53G2FoZePQSy46rxXQ9bmCGlF8tGhV7gOgh7D/LGfLHhtVt+\nb4BhPWLgOqOqCDUv+MXlYN+..................bdZJtCalMpjYq0o8aC1qJVv\n+ry9W+8xmfTRUSq6B0de8Y9XBEAhJu/3tJkyDSqs7ZEXR9F02hQ3WlmfQEExaktc\npIm1l+3beupmCoCliFfoNbdcZegiIdFmGcYRmKba+YpQ3yqpqcqAlCErdqwql8rs\ncJTGx0/AnxyaeX5Qtr86c1wIDAQAB';\n</code></pre> status Statement executed successfully. <p>With our key now set on the user, we might want to test it via local python. Here is a tutorial to test it.</p>"},{"location":"security/code/python/","title":"Python","text":""},{"location":"security/code/python/#key-pair-authentication","title":"Key Pair Authentication","text":"<p>In this tutorial we will show how to connect to your Snowflake account via python with an authentication token/key.</p> <p>For the official Snowflake documentation this tutorial was based on: https://docs.snowflake.com/en/user-guide/key-pair-auth</p>"},{"location":"security/code/python/#video","title":"Video","text":""},{"location":"security/code/python/#requirement","title":"Requirement","text":"<ul> <li>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</li> <li>This tutorial assumes you've already have a private key, if not here is a tutorial</li> </ul>"},{"location":"security/code/python/#download-needed-files","title":"Download needed files:","text":"<ul> <li>Files (Link)                </li> </ul>"},{"location":"security/code/python/#install","title":"Install","text":"<p>Lets start by updating our code to use the stream we setup at the beginning. Inside our <code>main.py</code> we'll update:</p> <p>Using the correct account locator.</p> <p>If your Snowflake account is in anther region other then US EAST (Oregon). Please append your locator with the region <code>xy12345.us-east-1</code> read the Account Identifiers documentation to learn how to format your account locator based on your region. </p>  Code Result <p></p><pre><code>user='&lt;User Name&gt;',  #(1)! \naccount='&lt;Account Locator&gt;',  #(2)!\n</code></pre><p></p> <ol> <li> <p>Name we gave to the user in our setup section.</p> </li> <li> <p></p> </li> </ol> <pre><code>user='danielwilczak',\naccount='GGB82720',\n</code></pre>"},{"location":"security/code/python/#run","title":"Run","text":"<p>Here is the code we'll use. Please fill in the two needed areas.</p> <p>Next we'll want to run that code to start generating the data which will be moved to Snowflake.</p>  Code Result <pre><code>python -m venv venv \nsource venv/bin/activate\npip install -r requirements.txt\npython main.py\n</code></pre> <pre><code>('DANIELWILCZAK',)\n</code></pre>"},{"location":"security/networking/account/","title":"Index","text":""},{"location":"security/networking/user/","title":"User","text":""},{"location":"security/networking/user/#network-policy-user-level","title":"Network Policy - User level","text":"<p>In this tutorial we will show how you can create a network policy that is only applied to an individual user.</p>"},{"location":"security/networking/user/#video","title":"Video","text":"<p>Video in development</p>"},{"location":"security/networking/user/#requirement","title":"Requirement","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> </ul>"},{"location":"security/networking/user/#walk-through","title":"Walk Through","text":"<p>We'll need a user to apply the policy to. Typically this is applied to a service user with an Key Pair but you can apply it to who ever you want. You'll also want to update your I.P addresses since these are just for an example.</p>  Code Example Result <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user &lt;username&gt;;\n\n-- Create a network policy and apply it to the user. \ncreate network policy &lt;policy_name&gt;  allowed_ip_list = ('&lt;IP ADDRESS&gt;');\n\n-- Apply the policy on the user.\nalter user &lt;username&gt; set network_policy = &lt;policy_name&gt; ;\n</code></pre> <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user danielwilczak;\n\n-- Create a network policy and apply it to the user. \ncreate network policy my_policy allowed_ip_list = ('34.230.230.9');\n\n-- Apply the policy on the user.\nalter user danielwilczak set network_policy = my_policy;\n</code></pre> status Statement executed successfully. <p>Now the network policy has been applied to your user.</p>"},{"location":"security/scim/azure/","title":"Azure","text":""},{"location":"security/scim/azure/#azure-entra-id-scim-to-snowflake","title":"Azure Entra ID SCIM to Snowflake","text":"<p>In this tutorial, we\u2019ll walk through how to setup Azure EntraID SCIM to provision users and role to our Snowflake account.</p>"},{"location":"security/scim/azure/#video","title":"Video","text":""},{"location":"security/scim/azure/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>An Azure account with P1 Security otherwise you will not be able to add groups.</li> </ul>"},{"location":"security/scim/azure/#snowflake","title":"Snowflake","text":"<p>Lets start with the easy part, lets get Snowflake setup. Our first goal will be to create our azure provisioning role, example engineer role and the security integration and finally grab the needed URL and token that will be used in Azure Entra ID.</p> <p>Note</p> <p>If you plan on using your own role to be provisioned, make sure it's owned (1) by the \"aad_provisioner\" role.</p> <ol> <li><code>grant ownership on role engineer to role aad_provisioner copy current grants;</code></li> </ol> <p>Lets open a worksheet (1) and add in the code below. This will create a engineer and aad_provisioner role. We will give the engineer role to myself in this tutorial via Entra ID SCIM group.</p> <ol> <li></li> </ol>  Code Result <pre><code>use role accountadmin;\n\n-- Create a povisioning role that will be used by azure to create and grant roles.\ncreate role if not exists aad_provisioner;\ngrant role aad_provisioner to role accountadmin;\ngrant create user on account to role aad_provisioner;\ngrant create role on account to role aad_provisioner;\n\n-- Create a new role called engineer and give it's ownership to aad_provisioner.\ncreate role if not exists engineer;\ngrant role engineer to role sysadmin;\ngrant ownership on role engineer to role aad_provisioner copy current grants;\n\n-- Create the integration.\ncreate or replace security integration aad_provisioning\n    type = scim\n    scim_client = 'azure'\n    run_as_role = 'AAD_PROVISIONER';\n</code></pre> status Integration AAD_PROVISIONING successfully created. <p>Now that we have our provisioning role, engineer role and integration we will just need to write down two things. Our Tenant URL and Token. There will both be needed later.</p>  Code Result <pre><code>-- Your tenant URL that needs to be provided to Azure.\nselect concat('https://',CURRENT_ORGANIZATION_NAME(),'-',CURRENT_ACCOUNT_NAME(),'.snowflakecomputing.com/scim/v2/');\n\n-- Your token that needs to be proved to azure.\nselect system$generate_scim_access_token('AAD_PROVISIONING');\n</code></pre> URL https://EASYCONNECT-SECURITY.snowflakecomputing.com/scim/v2/ <p>and</p> Token ver:2-hint:61552672773-did:1049-ETMsDgAAAZgvJl/6ABRBR....../UK7NvDTjYOLgtFb0fyV+YLI/VnZxcDr"},{"location":"security/scim/azure/#azure","title":"Azure","text":"<p>Now for the fun / long part of setting up azure. Lets start in azure by logging into our azure portal and navigate to Microsoft Entra ID. </p> <p>Note</p> <p>If you already have SSO setup you can skip to the provisioning step which is 3 images down.</p>"},{"location":"security/scim/azure/#enterprise-application","title":"Enterprise Application","text":"<p>Next lets click on Enterprise Applications on the left navbar. </p> <p>Click on new application. </p> <p>In the Browse Azure AD Gallery search bar, search for Snowflake, and choose Snowflake for Microsoft Entra ID application. Give your Snowflake application a name, then click the Create button at the bottom. </p> <p>Lets navigate to provisioning on the left side menu. </p>"},{"location":"security/scim/azure/#setup-provisioning","title":"Setup Provisioning","text":"<p>Click Add configuration.  </p> <p>Now enter in your tenant URL and token we got from Snowflake earlier, click test and create.  </p>"},{"location":"security/scim/azure/#groups","title":"Groups","text":"<p>Once created your ready to create a group and assign it.  </p> <p>Next lets go to groups to create the engineer group.  </p> <p>Add a new group.  </p> <p>Warning</p> <p>Make sure to name the group the same as your Snowflake role.</p> <p>Make it a securiy group and name it engineer. </p>"},{"location":"security/scim/azure/#adding-the-group","title":"Adding the group","text":"<p>Now we'll head back to the enterprise application and click on \"Assign users and groups\".  </p> <p>Click add a user/group.  </p> <p>Warning</p> <p>Make sure to add the group, not the user.</p> <p>Lets add the engineering group.   </p> <p>Once the group is added we'll go back to provisioning.  </p> <p>Warning</p> <p>This will only start the scheduled event. You will need to provision ondemand in the follow up step to see results.</p>"},{"location":"security/scim/azure/#provision-group","title":"Provision Group","text":"<p>We'll start the provisioning task.   </p> <p>Validate you want to start the provisioning scheduled task.  </p> <p>To adhoc add the engineering group to Snowflake. We'll click \"provision on demand\", search for the engineer role and add it.  </p> <p>Once selected we'll click the users we want to add and then click provision.  </p> <p>Once completed you'll get check boxes and you can head over to Snowflake to see your new user and it's role. </p>"},{"location":"security/scim/azure/#done-check-user","title":"Done / Check user","text":"<p>Under admin then users you can see my users has been added. We'll want to click on the user to see thier assigned role.  </p> <p>We can also see the role has been assigned to the user.  </p>"},{"location":"security/scim/azure/#removing-groups-users","title":"Removing groups / users","text":"<p>When you remove a group, it will delete the role and remove all users.  </p> <p>If you remove a user it will diable the user.  </p>"},{"location":"security/sso/azure/","title":"Active Directory / Entra ID","text":""},{"location":"security/sso/azure/#azure-sso-to-snowflake","title":"Azure (SSO) to Snowflake","text":"<p>In this tutorial we will show how to setup authentication to Snowflake using SSO with Azure Microsoft Entra ID Identity Provider which used to called Azure Active Directory.</p>"},{"location":"security/sso/azure/#video","title":"Video","text":""},{"location":"security/sso/azure/#azure","title":"Azure","text":"<p>Lets start in azure by setting up the SSO and then adding the users to the approved list.</p>"},{"location":"security/sso/azure/#setup","title":"Setup","text":"<p>Lets start in azure by logging into our azure and navigate to Microsoft Entra ID. </p> <p>Next lets click on Enterprise Applications on the left navbar. </p> <p>Click on new application. </p> <p>In the Browse Azure AD Gallery search bar, search for Snowflake, and choose Snowflake for Microsoft Entra ID application. </p> <p>Give your Snowflake application a name, then click the Create button at the bottom. </p> <p>Once the application is created, on the left side choose Single sign-on, then choose SAML in the middle panel. </p> <p>In the middle pane under the Basic SAML configuration section, click the Edit button. </p> <p>Next we'll want to get our Snowflake account URL which will be entered into the saml configuration. Lets head to Snowflake and click  </p> <p>Click the copy button on \"Account/Server URL\". Save this somewhere we will use it twice. </p> <p>Once in the menu, click \"Add Identifier\". </p> <p>To add and identifier, click in the empty space a form will appear. Got to love microsoft UI. </p> <p>Warning</p> <p>Make sure to add in the <code>https://</code> for both url's and <code>/fed/login</code> for the reply/second URL.</p> <p>Once you have the form, enter in your account URL we got from Snowflake earlier. Make sure to add in the <code>https://</code> for both and <code>/fed/login</code> for the reply URL. Click save once entered. </p> <p>Go back to the application's SAML-based Sign-on page, scroll down to the SAML Certificates section. Download the Federation Metadata XML. We will use this file in our Snowflake steps later. </p>"},{"location":"security/sso/azure/#add-users","title":"Add users","text":"<p>Warning</p> <p>If you don't add the user in the Azure AD group they will not be able to use the SSO login on Snowflake. </p> <p>Lets add users into the azure AD group for the application. First click on \"Users and groups\" on the left side navbar and then \"add user/group\".  </p> <p>Select Users and groups. </p> <p>Select the user or groups you want to add. The search bar can be very helpful when you have alot of users/groups. </p> <p>Finally click assign. </p>"},{"location":"security/sso/azure/#snowflake","title":"Snowflake","text":"<p>Next we will setup Snowflake with the information we got from our <code>federation metadata xml</code> file (1). To make this process easier I suggest formatting your XML file so it's easier to look through. I used VS code and an xml formatter to accomplish this. Once you have the file open in vs code and the xml extension installed, select all the code and right clicl -&gt; \"Format Document\". </p>"},{"location":"security/sso/azure/#setup_1","title":"Setup","text":"<p>Lets open a worksheet (1) in snowflake and enter the code below by entering in the necessary areas from our federation metadata xml file.</p>  Template Example Result <p></p><pre><code>use role accountadmin;\ncreate security integration azureadintegration\n    type = saml2\n    enabled = true\n    saml2_snowflake_acs_url    = 'https://[...].snowflakecomputing.com/fed/login' /* (1)! */\n    saml2_snowflake_issuer_url = 'https://[...].snowflakecomputing.com' /* (2)! */\n    saml2_issuer               = 'https://sts.windows.net/[...]/'  /* (3)! */\n    saml2_sso_url              = 'https://login.microsoftonline.com/[...]/saml2' /* (4)! */\n    saml2_x509_cert            = 'MIIC8DCCAdigAwIBAg.......eaq/4d52DuNwD'  /* (5)! */\n    saml2_provider = 'CUSTOM'\n    saml2_enable_sp_initiated = true\n    saml2_sp_initiated_login_page_label = 'Azure AD SSO';\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> <li> <p><code>&lt;EntityDescriptor ID=\"_8416250f-50fb-...8bcd335e92\" entityID=\"https://sts.windows.net/9a2d78cb-73...fc1ac5ef57a7/\" xmlns=\"urn:oasis:names:tc:SAML:2.0:metadata\"&gt;</code></p> </li> <li> <p><code>&lt;SingleSignOnService Binding=\"urn:oasis:names:tc:SAML:2.0:bindings:HTTP-Redirect\" Location=\"https://login.microsoftonline.com/9a2d78c...-fc1ac5ef57a7/saml2\" /&gt;</code></p> </li> <li> <p><code>&lt;X509Certificate&gt;     MIIC8DCCAdigAwIBAgIQQH4r9rnBiKlPEFVEjpdNhTANBgkqhkiG9w0BAQ     .......     Y9B1uSBpb4OmtWZ/LRNzHBDcDNbR oQ6PiPd2yWhtUfbYClOoNcMFOkk8E     &lt;/X509Certificate&gt;</code></p> </li> </ol> <pre><code>use role accountadmin;\ncreate security integration azureadintegration\ntype = saml2\nenabled = true\nsaml2_snowflake_acs_url    = 'https://EASYCONNECT-DEMO.snowflakecomputing.com/fed/login'\nsaml2_snowflake_issuer_url = 'https://EASYCONNECT-DEMO.snowflakecomputing.com'\nsaml2_issuer               = 'https://sts.windows.net/0fdf63dd-5e4d-4975-b415-5e75f710ae3a/'  \nsaml2_sso_url              = 'https://login.microsoftonline.com/0fdf63dd-5e4d-4975-b415-5e75f710ae3a/saml2' \nsaml2_x509_cert            = 'MIIC8DCCAdigAwIBAg.......eaq/4d52DuNwD'  \nsaml2_provider = 'CUSTOM'\nsaml2_enable_sp_initiated = true\nsaml2_sp_initiated_login_page_label = 'Azure AD SSO';\n</code></pre> status Integration AZUREADINTEGRATION successfully created."},{"location":"security/sso/azure/#user-management","title":"User Management","text":""},{"location":"security/sso/azure/#add-user","title":"Add user","text":"<p>Warning</p> <p>Users must use their email for logging into Snowflake that matches in Azure AD email or it will not work.</p> <p>Lets add the user with thier email. Start by navigation to \"users and roles\". </p> <p>Click \"+ user\". </p> <p>Add the users email. For the username it is required that when an email is entered you put quotes around it. Like \"daniel.wilczak@easyconnectapps.com\". </p>"},{"location":"security/sso/azure/#edit-user","title":"Edit user","text":"<p>Click \"edit\" on the user you want to modify. </p> <p>You will need to change the username, login name and display name. </p>"},{"location":"security/sso/azure/#test-login","title":"Test login","text":"<p>Note</p> <p>If you get an error stating the user doesnt exist, you either forgot to add the user in Azure or Snowflake.</p> <p>Lets make sure your Azure AD is working. Logout of your Snowflake account and you should now see the Azure AD login button. </p>"},{"location":"security/sso/google/","title":"Google","text":""},{"location":"security/sso/google/#google-authenticator-to-snowflake-sso-saml","title":"Google Authenticator to Snowflake (SSO / SAML)","text":"<p>In this tutorial, we will show how to set up authentication to Snowflake using SSO / SAML with Google Authenticator.</p>"},{"location":"security/sso/google/#video","title":"Video","text":""},{"location":"security/sso/google/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Google cloud account, you can setup a free account to get started.</li> </ul>"},{"location":"security/sso/google/#setup","title":"Setup","text":"<p>Lets start with some basic setup.</p>"},{"location":"security/sso/google/#snowflake","title":"Snowflake","text":"<p>Please make sure your username is the email that is in google or else the SSO will not work. To edit your user to be a email please go to your user under admin and update it by adding double quotes and your email in the username block. </p> <p>Now that we have our username setup, we'll copy our account url and add it to a worksheet. We'll need this later. </p>"},{"location":"security/sso/google/#google","title":"Google","text":"<p>Lets start in google by going to our Google administrator account. </p> <p>Click Apps &gt; Web and mobile apps &gt; Add app &gt; Add custom SAML app. </p> <p>Give the integration a name. This name is only used in Google. </p> <p>We'll want to copy the SSO URL, Entity ID and Certificate to a Snowflake worksheet. We will need these in the next step. </p>"},{"location":"security/sso/google/#integration","title":"Integration","text":"<p>Now that we have our Snowflake account URL, SSO URL, Entity ID and Certificate we can put it all together in Snowflake to enable the SSO.</p>"},{"location":"security/sso/google/#snowflake_1","title":"Snowflake","text":"<p>Warning</p> <p>Please remove the <code>-----BEGIN CERTIFICATE-----</code> and <code>-----END CERTIFICATE-----</code> from the certificate, it will give you an eror.</p> <p>Now in our worksheet we'll want to add the code below with the information we've copied so far.</p>  Code Example Result <pre><code>use role accountadmin;\n\ncreate security integration GOOGLE_SSO\n    type = saml2\n    enabled = true\n    saml2_issuer  = '&lt;Entity ID HERE&gt;'\n    saml2_sso_url = '&lt;SSO URL HERE&gt;'\n    saml2_snowflake_acs_url    = '&lt;ACCOUNT URL HERE&gt;/fed/login'\n    saml2_snowflake_issuer_url = '&lt;ACCOUNT URL HERE&gt;'\n    saml2_sp_initiated_login_page_label = 'GOOGLE SSO'\n    saml2_enable_sp_initiated = true\n    saml2_provider  = 'custom'\n    saml2_x509_cert = '&lt;Certificate HERE&gt;';\n\ndesc integration GOOGLE_SSO;\nselect \"property\", \"property_value\" from TABLE(RESULT_SCAN(LAST_QUERY_ID()))\nwhere \"property\" = 'SAML2_SNOWFLAKE_ACS_URL' or \"property\" = 'SAML2_SNOWFLAKE_ISSUER_URL';\n</code></pre> <pre><code>use role accountadmin;\n\ncreate security integration GOOGLE_SSO\n    type = saml2\n    enabled = true\n    saml2_issuer    = 'https://accounts.google.com/o/saml2?idpid=C02n8rltd'\n    saml2_sso_url   = 'https://accounts.google.com/o/saml2/idp?idpid=C02n8rltd'\n    saml2_snowflake_acs_url    = 'https://vpb00288.snowflakecomputing.com/fed/login'\n    saml2_snowflake_issuer_url = 'https://vpb00288.snowflakecomputing.com'\n    saml2_sp_initiated_login_page_label = 'GOOGLE SSO'\n    saml2_enable_sp_initiated = true\n    saml2_provider  = 'custom'\n    saml2_x509_cert = 'MIIDdDCCAlygAwIBAgIGAZE+BMSSMA0GCSqGSIb3DQEBCwUAMHsxFDASBgNVBAoTC0dvb2dsZSBJ\n    bmMuMRYwFAYDVQQHEw1Nb3VudGFpbiBWaWV3MQ8wDQYDVQQDEwZHb29nbGUxGDAWBgNVBAsTD0dv\n    b2dsZSBGb3IgV29yazELMAkGA1UEBhMCVVMxEzARBgNVBAgTCkNhbGlmb3JuaWEwHhcNMjQwODEw\n    MjAzOTQ1WhcNMjkwOD........................Yq/o1B/utIgjtWX6Vru3yGpnIead1vvbzJ\n    fT0SNwzSVsoplblpU5O+nfJ96/fGj5eBk8u8X+BPA+KqTZHTb9CisHB6o8j4SjnZQVTrmU7HJet1\n    a1PvW+pCw+M2WXr8mZyID+/UAtQ/kM/jDgG/5ZDK0HOg';\n\n    desc integration GOOGLE_SSO;\n    select \"property\", \"property_value\" from TABLE(RESULT_SCAN(LAST_QUERY_ID()))\n    where \"property\" = 'SAML2_SNOWFLAKE_ACS_URL' or \"property\" = 'SAML2_SNOWFLAKE_ISSUER_URL';\n</code></pre> property property_value SAML2_SNOWFLAKE_ACS_URL https://vpb00288.snowflakecomputing.com/fed/login SAML2_SNOWFLAKE_ISSUER_URL https://vpb00288.snowflakecomputing.com"},{"location":"security/sso/google/#google_1","title":"Google.","text":"<p>Lets take the output of our Snowflake and finish off in Google. Take the <code>SAML2_SNOWFLAKE_ACS_URL</code> -&gt; <code>ACS URL</code> and <code>SAML2_SNOWFLAKE_ISSUER_URL</code> -&gt; <code>Entity ID</code> </p> <p>Click finished. </p> <p>Next we'll want to enable user access. Click the <code>User access</code> box. </p> <p>Enable for everyone. </p>"},{"location":"security/sso/google/#login","title":"Login","text":"<p>Warning</p> <p>Please make sure your username is the email that is in google or else the SSO will not work. You can edit your user in the admin section as shown in part one of this tutorial.</p> <p>Now you should be able to go to your Snowflake login page and see the new <code>Google SSO</code> version. </p>"},{"location":"tools/dbt/setup/","title":"Cloud","text":""},{"location":"tools/dbt/setup/#dbt-development-and-production","title":"DBT Development and Production:","text":"<p>In this tutorial, we will guide you through setting up dbt Cloud with Snowflake. Along the way, we'll demonstrate how to configure both your development and production environments.</p>"},{"location":"tools/dbt/setup/#video","title":"Video","text":""},{"location":"tools/dbt/setup/#requirement","title":"Requirement","text":"<ul> <li>At least a free Snowflake (trial account) and no complex security needs.</li> <li>At least a free DBT developer account and no complex security needs.</li> <li>A git repository. I suggest github.</li> </ul>"},{"location":"tools/dbt/setup/#snowflake","title":"Snowflake","text":"<p>First lets start in Snowflake by setting up some resources. We'll create one database, two warehouses and one new users using sysadmin role.</p> If your user is using MFA - Please enable token caching before uploading <p>If your planned development user is using MFA. Please enable token caching or else it will ask you for authentication everytime you run anything in dbt.</p> <pre><code>alter account set allow_client_mfa_caching = TRUE;\n</code></pre>  Setup Example Result <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists dbt;\n\n-- We'll remove the default schema to keep things clean.\ndrop schema dbt.public;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\ncreate warehouse if not exists production \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n-- User/login used during our production prcoess.\nuse role accountadmin;\n\ncreate user service_dbt password = '&lt;USER PASSWORD&gt;';\n\n-- Lets keep the RBAC simple and use sysadmin for everything. \ngrant role sysadmin to user service_dbt;\n</code></pre> <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists dbt;\n\n-- We'll remove the default schema to keep things clean.\ndrop schema dbt.public;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\ncreate warehouse if not exists production \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\n-- User/login used during our production prcoess.\nuse role accountadmin;\n\ncreate user service_dbt password = 'Password1234';\n\n-- Lets keep the RBAC simple and use sysadmin for everything. \ngrant role sysadmin to user service_dbt;\n</code></pre> <pre><code>Statement executed successfully.\n</code></pre>"},{"location":"tools/dbt/setup/#development","title":"Development","text":"<p>Lets walk through the development environment setup in dbt. </p>"},{"location":"tools/dbt/setup/#project","title":"Project","text":"<p>As soon on you login you'll be asked to start a project and give it a name. </p> <p>We'll want to add our development environment connection to Snowflake. </p>"},{"location":"tools/dbt/setup/#snowflake_1","title":"Snowflake","text":"<p>To add our development environment we'll want to get our account identifier. Once copied we'll make sure we replace the <code>.</code> with a <code>-</code> otherwise it won't find the account. </p> <p>We'll select snowflake and give the connection a name. For the connection settings you'll want to add your account identifier as the account. Please remember to swap <code>.</code> with a <code>-</code> otherwise it won't find the account. For the database we'll use <code>DBT</code>, warehouse = <code>development</code> and role = <code>SYSADMIN</code>. </p> <p>Once saved, dbt will not redirect you to the setup so please click <code>Credentials</code> and then the project. This will prompt you to click a link that will take you back to the setup page. </p> <p>Select your connection. </p> <p>If your user is using MFA</p> <p>If your planned development user is using MFA. This connection test will ask for MFA authentication.</p> <p>Now we'll want to enter our own development credentials and then select <code>test connection</code> to validate everything. </p> <p>Once tested you will see a <code>complete</code> status. From there we'll click save. </p>"},{"location":"tools/dbt/setup/#git","title":"Git","text":"<p>Next we'll want to add our git reposity. If you have not created one yet, this would be a great time before we connect to it. I named mine <code>dbt</code>. </p> <p>Once you authenticate with your git provider, a list of git repos will show. Select the one you want to use. </p>"},{"location":"tools/dbt/setup/#run-first-models","title":"Run first models","text":"<p>Great our development environment connection is setup. Lets build our first models and see them in Snowflake. </p> <p>Lets initialize the project code. </p> <p>We can see a sample project has been added to our branch. </p> <p>Before we build these models lets remove some code so that everything builds successfully. DBT has setup up this code for learning but we are more intrested in getting everything working. </p> If your user is using MFA - Please enable token caching before uploading <p>If your development user is using MFA. Please enable token caching or else it will ask you for authentication everytime you run each dbt model. In this case will we ask at least 6-7 times.</p> <pre><code>alter account set allow_client_mfa_caching = TRUE;\n</code></pre> <p>Now lets build our code by going to the bottom of the page and entering <code>dbt build</code> and hitting enter. This will build all our models. </p> <p>We can see now that all our current branches models have built successfully. </p> <p>We can view those built models in our Snowflake account. </p> <p>Finally we will commit those changes to our branch. </p>"},{"location":"tools/dbt/setup/#production","title":"Production","text":"<p>Great now that we have our development environment setup we can start adding new users and all they will have to do is add thier development credientials. Lets setup our product environment that will curate our models on a regular bases based on our <code>main</code> branch.</p>"},{"location":"tools/dbt/setup/#environment","title":"Environment","text":"<p>Lets start by committing our changes to the branch. </p> <p>Once commit DBT will give us the option to create a pull request. Lets commit our branch to main branch. </p> <p>Click create pull request. </p> <p>Give the PR a description and click \"create pull request\". </p> <p>After it's created lets merge the code changes. </p> <p>Confirm the merge. </p> <p>After it's merged lets delete the branch to keep our git clean. </p> <p>Next lets go back to DBT to setup the environment. </p> <p>Lets create a new environment. </p> <p>Lets give our environment a name, select deplyment type of <code>production</code> and connection of Snowflake we created earlier. </p> <p>Lets add our connection variables. The image shows what everything should be if you used our setup code. The password comes from that setup code. If your want to use key-pair instead of username/password, check our key-pair tutorial. </p> <p>Once finished head back to the top and click save. </p>"},{"location":"tools/dbt/setup/#job","title":"Job","text":"<p>Jobs run dbt code in our environment on a regular bases. Lets use the left navbar to go to deploy and then jobs. </p> <p>Create a new deploy job. </p> <p>We'll give our job a name. The environment should be production and we want to run the command <code>dbt build</code> similar to when we were in development. Next we'll select to generate docs and run on a schedule. Click save when finished. </p> <p>Now that we created the job, lets run it. </p> <p>We can see the job is now running, lets click in and see what it's doing. </p> <p>You can see the steps dbt will take to run this job. It may take 20 seconds for it to finish. </p>"},{"location":"tools/dbt/setup/#result","title":"Result","text":"<p>Once the job is completed we can go to Snowflake to see our production jobs result in our production schema. </p> <p>Now we can go back to the dashboard to see all the metadata of our job and run history. </p>"},{"location":"tools/dbt/setup/#documentation","title":"Documentation","text":"<p>Now that our production job has run lets see our documentation. This has a little setup to know which job the docs should look at when displaying results.</p> <p>Lets start by going to account settings in the bottom left of the page. </p> <p>We'll go to project, select the project and finally edit the project. </p> <p>We'll want to update the description, and the job we want to associate the documentation to. Finally click save. </p> <p>Great, lets click documentation on the left side navbar. </p> <p>Now we can see our documentation. </p>"},{"location":"tools/documentation/","title":"Github - Docs:","text":""},{"location":"tools/documentation/#github-docs","title":"Github - Docs:","text":"<p>Images to be added soon.</p> <ol> <li> <p>Copy</p> <ul> <li>.workflow folder </li> <li>Docs/ folder</li> <li>Mkdocs.yml</li> </ul> </li> <li> <p>Create a new branch called \"gh-pages\".</p> </li> <li> <p>Go to settings-&gt; pages and set branch to gh-pages / (root) and click save.</p> </li> <li> <p>Update the files with your intended name.</p> </li> <li> <p>Copy the files to the main branch.</p> </li> <li> <p>Click on action in the repo main page and then the successful action. You will get your pages URL.</p> </li> </ol>"},{"location":"tools/git/","title":"Github","text":""},{"location":"tools/git/#snowflake-git-integration","title":"Snowflake Git Integration","text":"<p>In this tutorial we will show you how to integrate Git into your Snowflake account. We will walk through two main subjects. The first is pulling the repo into Snowflake and navigating around. The second is creating a stored procedure, streamlit dashboard, and run a query from files that lives on github.  </p> <p>Credit for the orginial tutorial goes to Mark Boos!  </p>"},{"location":"tools/git/#video","title":"Video","text":""},{"location":"tools/git/#requirements","title":"Requirements","text":"<p>You will need a github repo and the ability to create a personal access token. We will show how to create the token in this tutorial. </p>"},{"location":"tools/git/#download","title":"Download","text":"<ul> <li>Repository files</li> </ul>"},{"location":"tools/git/#setup","title":"Setup","text":"<p>In this section we will upload the example code to a repository and then setup Snowflake:</p>"},{"location":"tools/git/#github-repository","title":"Github Repository","text":"<p>Start by creating a repo. </p> <p>Call it \"tutorial\". It can be public or private.  </p> <p>Please do not upload the repo folder itself. Just the files in it.</p> <p>Upload the example repository. </p> <p>Your good to go. </p>"},{"location":"tools/git/#personal-access-token","title":"Personal Access Token","text":"<p>We will need a persoanl access token to allow Snowflake to work with our Git repository. First lets navigate to the token page. </p> <p>Click on settings. </p> <p>Next developer settings. </p> <p>We'll be using a classic token. </p> <p>Next we'll enter in a name, the experation of the token, and select \"repo\" for the scope of the permissions. </p> <p>We'll copy our token, it will be used in the following Snowflake step. </p>"},{"location":"tools/git/#snowflake","title":"Snowflake","text":"<p>Lets now use snowflake to connect to our repository.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema raw.git;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists developer \n    warehouse_size = xsmall\n    initially_suspended = true;\n\nuse database raw;\nuse schema git;\nuse warehouse developer;\n</code></pre>  Setup Example Result <p></p><pre><code>use role accountadmin;\n\ncreate or replace secret github_secret\n    type = password\n    username = '&lt;Github Username&gt;' /* (1)! */\n    password = '&lt;Personal Access Token&gt;'; /* (2)! */\n\ncreate or replace api integration git_api_integration\n    api_provider = git_https_api\n    api_allowed_prefixes = ('&lt;Base Github URL&gt;') /* (3)! */\n    allowed_authentication_secrets = (github_secret)\n    enabled = true;\n\ncreate or replace git repository tutorial\n    api_integration = git_api_integration\n    git_credentials = github_secret\n    origin = '&lt;REPOSITORY URL&gt;'; /* (4)! */\n</code></pre><p></p> <ol> <li> <p></p> </li> <li> <p></p> </li> <li> <p></p> </li> <li> <p></p> </li> </ol> <pre><code>use role accountadmin;\n\ncreate secret github_secret\n    type = password\n    username = 'sfc-gh-dwilczak'\n    password = 'huifuhf.....f94894h2';\n\ncreate api integration git_api_integration\n    api_provider = git_https_api\n    api_allowed_prefixes = ('https://github.com/sfc-gh-dwilczak')\n    allowed_authentication_secrets = (github_secret)\n    enabled = true;\n\ncreate git repository tutorial\n    api_integration = git_api_integration\n    git_credentials = github_secret\n    origin = 'https://github.com/sfc-gh-dwilczak/tutorial';\n</code></pre> status Git Repository TUTORIAL was successfully created."},{"location":"tools/git/#examples","title":"Examples","text":"<p>Now that we have our Snowflake and Git repository setup lets go through a few examples.</p>"},{"location":"tools/git/#navigation","title":"Navigation","text":"<p>Lets navigate and show parts of our repo in Snowflake.</p>  Navigation <pre><code>-- Show repos added to snowflake.\nshow git repositories;\n\n-- Show branches in the repo.\nshow git branches in git repository tutorial;\n\n-- List files.\nls @tutorial/branches/main;\n\n-- Show code in file.\nselect $1 from @tutorial/branches/main/examples/app.py;\n\n-- Fetch git repository updates.\nalter git repository tutorial fetch; \n</code></pre>"},{"location":"tools/git/#run-a-file","title":"Run a File","text":"<p>Lets execute a file on Snowflake that lives in our repositroy.</p>  Example Result <pre><code>-- Run the files in Snowflake.\nexecute immediate from @tutorial/branches/main/examples/hello.sql;\n</code></pre> GREETING Hello World from a SQL query"},{"location":"tools/git/#stored-procedure","title":"Stored Procedure","text":"<p>Lets create a snowflake stored procedure from a file that lives in the repository.</p>  Example Result <pre><code>-- Create snowpark procedure\ncreate or replace procedure hello()\n    returns string\n    language python \n    runtime_version= '3.8'\n    packages=('snowflake-snowpark-python')\n    imports=('@tutorial/branches/main/examples/hello.py')\n    handler='hello.main';\n\ncall hello();\n</code></pre> Hello Hello World!"},{"location":"tools/git/#streamlit","title":"Streamlit","text":"<p>Lets create a Streamlit application in Snowflake using a file in our repository.</p>  Example Result <pre><code>-- Create streamlit application from file.\ncreate or replace streamlit streamlit_application\n    root_location = @raw.git.tutorial/branches/main/examples\n    main_file = '/app.py'\n    query_warehouse = 'developer';\n</code></pre> status Streamlit STREAMLIT_APPLICATION successfully created. <p>Lets navigate to the streamlit in snowflake tab and select our dashboard that we just created. </p> <p>Look at our simple streamlit dashboard that was create from our file in our Git repository! </p>"},{"location":"tools/okta/","title":"Okta - Snowflake.","text":""},{"location":"tools/okta/#okta-snowflake","title":"Okta - Snowflake.","text":"<p>Images to come soon. </p><pre><code>Signup for okta\n    - Create a user with your email in snowflake. IT HAS TO BE IN QUOTES. Example - create user 'daniel.wilczak@snowflake.com';\n    - Go to admin page on top right navbar.\n    - Go to application on left sidebar.\n    - Click browse app catalog.\n    - Find snowflake and add it.\n    - Add your subdoin https:&lt;THIS NAME&gt;snowflakecomputing.com\n    - Get your metadata URL from sign on tab.\n    - Get: \n        \u25cb entityID=\n            \u00a7 This is your SAML2_ISSUER\n        \u25cb SingleSignOnService / Location\n            \u00a7 This is your SAML2_SSO_URL\n        \u25cb SAML2_X509_CERT\n            \u00a7 This is your SAML2_X509_CERT\n        \u25cb Additional inputs for entering into snowflake\n            \u00a7 SAML2_SP_INITIATED_LOGIN_PAGE_LABEL = OKTA SSO\n            \u00a7 SAML2_ENABLE_SP_INITIATED = TRUE;\n            \u00a7 SAML2_PROVIDER = OKTA\n            \u00a7 TYPE = SAML2\n            \u00a7 ENABLED = TRUE \n            \u00a7 saml2_snowflake_acs_url = ' https://&lt;Account&gt;.snowflakecomputing.com/fed/login';\n            \u00a7 saml2_snowflake_issuer_url = ' https://&lt;account&gt;.snowflakecomputing.com';\n\n\n    - Create the security integration in snowflake with the information above:\n\n\n\n\ncreate security integration &lt;name&gt;\n  TYPE = saml2\n  ENABLED = true\n  SAML2_ISSUER = 'entityID'\n  SAML2_SSO_URL = 'smal2_sso_url'\n  SAML2_PROVIDER = OKTA\n  SAML2_X509_CERT = 'LONGER ASS CERTIFICATE'\n  saml2_snowflake_acs_url = ' https://&lt;Account&gt;.snowflakecomputing.com/fed/login';\n  saml2_snowflake_issuer_url = ' https://&lt;account&gt;.snowflakecomputing.com';\n</code></pre><p></p>"},{"location":"tools/tableau/key_pair/","title":"Key Pair","text":""},{"location":"tools/tableau/key_pair/#tableau-key-pair-authentication","title":"Tableau - Key Pair Authentication","text":"<p>In this tutorial we will show how to connect to snowflake via tableau with key-pair as the authenitcation method.</p>"},{"location":"tools/tableau/key_pair/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"tools/tableau/key_pair/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p> <p>Tableau requires these versions to use key-pair:</p> <ul> <li>Snowflake ODBC driver 3.4 or later</li> <li>Tableau Desktop 2024.3 or later</li> </ul>"},{"location":"tools/tableau/key_pair/#generate-key","title":"Generate Key","text":"<p>Lets create the private and public key so that we can apply the public key to our user.</p>  Setup Result <pre><code>openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 -nocrypt\nopenssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub\n</code></pre> <pre><code>Writing RSA key.\n</code></pre> <p>This will create two files in the folder we are currently located. </p>"},{"location":"tools/tableau/key_pair/#snowflake","title":"Snowflake","text":"<p>Lets start with applying our key to our user and then getting the account url.</p>"},{"location":"tools/tableau/key_pair/#apply-key-to-user","title":"Apply key to user","text":"<p>Lets create the user, assign the user a role and finally apply the public key to our user in a worksheet.</p>  Code Example Result <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user &lt;username&gt;;\n\n-- Give the user a role.\ngrant role &lt;role_name&gt; to user &lt;username&gt;;\n\n-- Apply the public key to the user.\nalter user &lt;username&gt; set rsa_public_key='&lt;Public Key&gt;';\n\n/* (OPTIONAL) Create a network policy and apply it to the user. \ncreate network policy &lt;policy_name&gt;  allowed_ip_list = ('&lt;IP ADDRESS&gt;');\n\nalter user &lt;username&gt; set network_policy = &lt;policy_name&gt; ; /* \n</code></pre> <pre><code>use role accountadmin;\n\n-- Create the user. Optional add type = 'service' for service accounts.\ncreate user danielwilczak;\n\n-- Give the user a role.\ngrant role sysadmin to user danielwilczak;\n\n-- Apply the public key to the user.\nalter user danielwilczak set \n    rsa_public_key='MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA\nzd7lfIGps+lBXrVCT05l 92rDpYUsXyjtvAu26Q2z0k3/7n7HnZNmKjreIlGQJZl\nBe0Eud4LzqGX9Vbp53G2FoZePQSy46rxXQ9bmCGlF8tGhV7gOgh7D/LGfLHhtVt+\nb4BhPWLgOqOqCDUv+MXlYN+..................bdZJtCalMpjYq0o8aC1qJVv\n+ry9W+8xmfTRUSq6B0de8Y9XBEAhJu/3tJkyDSqs7ZEXR9F02hQ3WlmfQEExaktc\npIm1l+3beupmCoCliFfoNbdcZegiIdFmGcYRmKba+YpQ3yqpqcqAlCErdqwql8rs\ncJTGx0/AnxyaeX5Qtr86c1wIDAQAB';\n\n/* (OPTIONAL) Create a network policy and apply it to the user. \ncreate network policy my_policy allowed_ip_list = ('34.230.230.9');\n\nalter user danielwilczak set network_policy = my_policy; */\n</code></pre> status Statement executed successfully."},{"location":"tools/tableau/key_pair/#account-url","title":"Account URL","text":"<p>Before we leave Snowflake we'll want to copy our account url to later add to tableau. </p> <p>Next you'll see the url </p>"},{"location":"tools/tableau/key_pair/#tableau-desktop","title":"Tableau Desktop","text":"<p>Lets Open tableau Desktop and add a new source that is Snowflake. </p> <p>Past in url, add role name, warehouse and username. Select key-pair as authentication and browse/add your private key. Finally click sign in. </p> <p>Success your login via key-pair authentication. </p>"},{"location":"tools/tableau/key_pair/#tableau-cloud","title":"Tableau Cloud","text":"<p>Go to \"My account settings\" on the top right. </p> <p>Find Snowflake as a source, click add. </p> <p>Click the drop down and select \"key-pair authentication method\". </p> <p>Click add. </p> <p>Fill in the server with the account url, role username, add your private key we generated at the start, and click add. </p> <p>Once added we can test our connection to make sure it works. </p>"},{"location":"tools/tableau/oauth/","title":"OAuth","text":""},{"location":"tools/tableau/oauth/#tableau-snowflake-oauth-authentication","title":"Tableau - Snowflake OAuth Authentication","text":"<p>In this tutorial we will show how to connect to snowflake via tableau with OAuth as the authenitcation method. Snowflake official documentation can be found here: </p>"},{"location":"tools/tableau/oauth/#video","title":"Video","text":"<p>Video is still in development.</p>"},{"location":"tools/tableau/oauth/#requirement","title":"Requirement","text":"<p>This tutorial assumes you have nothing in your Snowflake account (Trial) and no complex security needs.</p> <p>Warning</p> <p>You can not use the ACCOUNTADMIN or SECURITYADMIN role by default. Documentation on this block.</p> <p>Note</p> <p>If you do want to use ACCOUNTADMIN or SECURITYADMIN roles please submit a support ticket allowing it with the integration name created below.</p>"},{"location":"tools/tableau/oauth/#snowflake","title":"Snowflake","text":"<p>Lets start in Snowflake first.</p>"},{"location":"tools/tableau/oauth/#intergration","title":"Intergration","text":"<p>Start a worksheet and add either or both security integrations below.</p>  Code <pre><code>use role accountadmin;\n\n-- For tableau desktop\ncreate security integration tableau_desktop_oauth\n    type = oauth\n    enabled = true\n    oauth_client = tableau_desktop;\n\n-- For tableau cloud\ncreate security integration tableau_cloud_oauth\n    type = oauth\n    enabled = true\n    oauth_client = tableau_server;\n</code></pre>"},{"location":"tools/tableau/oauth/#account-url","title":"Account URL","text":"<p>Before we leave Snowflake we'll want to copy our account url to later add to tableau. </p> <p>Next you'll see the url </p>"},{"location":"tools/tableau/oauth/#tableau-desktop","title":"Tableau Desktop","text":"<p>Lets start by adding Snowflake as a source. Search for Snowflake in \"Connect to Server\". </p> <p>Warning</p> <p>You can not use the ACCOUNTADMIN or SECURITYADMIN role by default. Documentation on this block.</p> <p>Note</p> <p>If you do want to use ACCOUNTADMIN or SECURITYADMIN roles please submit a support ticket allowing it with the integration name created below.</p> <p>Once Snowflake is selected you'll want to enter your Snowflake account URL, role, warehouse and select sign in using OAuth. </p> <p>Once you click \"Sign in\" a browser will appear for login/approval. </p> <p>Login with your user. </p> <p>Click allow for tableau to connect to your Snowflake user. </p> <p>Success your OAuth is setup. </p>"},{"location":"tools/tableau/oauth/#tableau-cloud","title":"Tableau Cloud","text":"<p>Go to \"My account settings\" on the top right. </p> <p>Select Snowflake as a source. </p> <p>Select \"OAuth Credential\" as authentication and click Add. </p> <p>Warning</p> <p>You can not use the ACCOUNTADMIN or SECURITYADMIN role by default. Documentation on this block.</p> <p>Note</p> <p>If you do want to use ACCOUNTADMIN or SECURITYADMIN roles please submit a support ticket allowing it with the integration name created below.</p> <p>Enter your account URL we copied earlier and the role to be used once authenticated </p> <p>Login to your user. </p> <p>Allow tableau to use your user/role. </p> <p>Click test to validate the source works, and we're finished! </p>"},{"location":"tools/zapier/","title":"Zapier","text":""},{"location":"tools/zapier/#zapier-setup","title":"Zapier - Setup","text":"<p>Goal of this tutorial is to setup the OAuth connection between Zapier and Snowflake. We'll follow up with how to reference Snowflake tables in Zapier.</p>"},{"location":"tools/zapier/#video","title":"Video","text":"<p>Video still in development.</p>"},{"location":"tools/zapier/#requirements","title":"Requirements","text":"<ul> <li>Snowflake account, you can use a free trial. We also assume no complex security needs.</li> <li>Zapier account, you can setup a free account to get started.</li> </ul>"},{"location":"tools/zapier/#setup","title":"Setup","text":"<p>Lets start by setting up a Snowflake connection to Google Cloud Storage. After that we'll create and load data into some Iceberg tables.</p>"},{"location":"tools/zapier/#zapier","title":"Zapier","text":"<p>Sign into your Zapier account and head to App Connections. </p> <p>We'll add a new connection. </p> <p>Search Snowflake and add the Snowflake option. </p> <p>From there a window will open where we'll need to copy the redirect URI so that we can enter it into Snowflake to get both our client ID and client secret from Snowflake. </p>"},{"location":"tools/zapier/#snowflake","title":"Snowflake","text":"<p>Inside of Snowflake we'll open a new sql worksheet and add the code below with our redirect URI.</p> If you don't have a database, schema or warehouse yet.  Database, schema and warehouse <pre><code>use role sysadmin;\n\n-- Create a database to store our schemas.\ncreate database if not exists raw;\n\n-- Create the schema. The schema stores all our objectss.\ncreate schema if not exists raw.zapier;\n\n/*\n    Warehouses are synonymous with the idea of compute\n    resources in other systems. We will use this\n    warehouse to call our user defined function.\n*/\ncreate warehouse if not exists development \n    warehouse_size = xsmall\n    auto_suspend = 30\n    initially_suspended = true;\n\nuse database raw;\nuse schema zapier;\nuse warehouse development;\n</code></pre>  Template Example Result <p></p><pre><code>use role accountadmin;\n\ncreate or replace security integration oauth_zapier\n  type = oauth\n  enabled = true\n  oauth_client = custom\n  oauth_client_type = 'CONFIDENTIAL'\n  oauth_redirect_uri = '&lt;REDIRECT URI&gt;' /* (1)! */\n  oauth_issue_refresh_tokens = true\n  oauth_refresh_token_validity = 7776000;\n\nwith integration_secrets as (\n  select parse_json(system$show_oauth_client_secrets('OAUTH_ZAPIER')) as secrets\n)\n\nselect\n  secrets:\"OAUTH_CLIENT_ID\"::string     as client_id,\n  secrets:\"OAUTH_CLIENT_SECRET\"::string as client_secret\nfrom\n  integration_secrets;\n</code></pre><p></p> <pre><code>use role accountadmin;\n\ncreate or replace security integration oauth_zapier\n  type = oauth\n  enabled = true\n  oauth_client = custom\n  oauth_client_type = 'CONFIDENTIAL'\n  oauth_redirect_uri = 'https://zapier.com/dashboard/auth/oauth/return/SnowflakeCLIAPI/'\n  oauth_issue_refresh_tokens = true\n  oauth_refresh_token_validity = 7776000;\n\nwith integration_secrets as (\n  select parse_json(system$show_oauth_client_secrets('OAUTH_ZAPIER')) as secrets\n)\n\nselect\n  secrets:\"OAUTH_CLIENT_ID\"::string     as client_id,\n  secrets:\"OAUTH_CLIENT_SECRET\"::string as client_secret\nfrom\n  integration_secrets;\n</code></pre> CLIENT_ID CLIENT_SECRET E77FnK68beS......mKYD0CX1ZU= iw+0Q8SKcZVkExnr77y....jgAT5HoCDWg8nyXZsDFo= <p>After we have generated our client ID and client secret we'll want to add them to our zapier pop up window. Next we'll get our account name by going to our account setting page in Snowflake. </p> <p>From there we will copy our Account Identifier to be added as our account in Zapier. </p>"},{"location":"tools/zapier/#testing-connection","title":"Testing Connection","text":"<p>Warning</p> <p>You can NOT use a user with accountadmin privligies to login. Please create a new user if you only have one user by:</p> <pre><code>create user service_zapier PASSWORD='Password123456';\ngrant role &lt;role name&gt; to user service_zapier;\n</code></pre> <p>Now we have finished adding in all the required information we might want to also preset the role, database and schema this can make our lives easier later on when we are referencing tables that we already know where they exist. </p> <p>Warning</p> <p>You can NOT use a user with accountadmin privligies to login. Please create a new user if you only have one user by:</p> <pre><code>create user service_zapier PASSWORD='Password123456';\ngrant role &lt;role name&gt; to user service_zapier;\n</code></pre> <p>Now login via your user that's not using account admin. </p> <p>Once you login you'll be brought back into app connections where we can test the connection. </p> <p>Click on the three dots and then test connection. </p> <p>If the connection is successful your ready to build using Snowflake connection. </p>"},{"location":"tools/zapier/#build","title":"Build","text":"<p>Lets build using our connection, one first select your connection in the configuration settings. </p> <p>Next we'll want to add our table, if you want to use the full path of the table you can select custom and then use the path via: ..<p></p> </p>"}]}